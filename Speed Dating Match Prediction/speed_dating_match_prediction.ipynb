{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Formulation:"
      ],
      "metadata": {
        "id": "ujRqQG5S5qQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These is a classification problem** to predict the outcome of a specific speed dating session based on the profile of two people, so we can implement a recommendation system to better match people in speed dating events. </br>\n",
        "**The input** for our problem is the various features that descripe all information abou people profile.</br>\n",
        "**The output** of the problem is a probability that indicates whether two people are likely to be a match or not.</br>\n",
        "**The goal** is to improve the matching of participants in speed dating events.\n"
      ],
      "metadata": {
        "id": "z7OymGVE6GKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The required function is to **classify** if the people is matching or not"
      ],
      "metadata": {
        "id": "fpygvR2F6LFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The challenges** is the dealing with noisy data, missing values, and irrelevant or unnecessary features.</br>\n",
        "Also, the features included in the dataset may not be sufficient to accurately predict the product rating, which may require additional data sources or feature engineering\n"
      ],
      "metadata": {
        "id": "zH3nEjxq6Nwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The benefit of of a speed dating session** Improve matching by predicting the outcome of a speed dating session, we can identify which pairs of people are more likely to be a good match. This can help organizers of speed dating events to better match people and increase the likelihood of successful matches."
      ],
      "metadata": {
        "id": "2dOA5BQ36T1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "uTmWw7Vm126K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, plot_confusion_matrix, accuracy_score,recall_score,precision_score, roc_curve, precision_recall_curve, roc_auc_score,f1_score\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn import preprocessing\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.070429Z",
          "iopub.execute_input": "2023-03-29T18:55:50.071498Z",
          "iopub.status.idle": "2023-03-29T18:55:50.079489Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.071453Z",
          "shell.execute_reply": "2023-03-29T18:55:50.077870Z"
        },
        "trusted": true,
        "id": "dM1-ohgs126N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scikit-plot"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.081818Z",
          "iopub.execute_input": "2023-03-29T18:55:50.082220Z",
          "iopub.status.idle": "2023-03-29T18:55:50.090364Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.082145Z",
          "shell.execute_reply": "2023-03-29T18:55:50.089045Z"
        },
        "trusted": true,
        "id": "yrM-47NK126O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from sklearn import decomposition\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "import scikitplot as skplt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.092275Z",
          "iopub.execute_input": "2023-03-29T18:55:50.092713Z",
          "iopub.status.idle": "2023-03-29T18:55:50.101399Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.092656Z",
          "shell.execute_reply": "2023-03-29T18:55:50.099547Z"
        },
        "trusted": true,
        "id": "04KqoTJm126O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "pd.set_option('display.max_rows', None)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.103284Z",
          "iopub.execute_input": "2023-03-29T18:55:50.103750Z",
          "iopub.status.idle": "2023-03-29T18:55:50.114521Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.103701Z",
          "shell.execute_reply": "2023-03-29T18:55:50.113494Z"
        },
        "trusted": true,
        "id": "WW5ADLTj126O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check version number\n",
        "import imblearn\n",
        "print(imblearn.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.117849Z",
          "iopub.execute_input": "2023-03-29T18:55:50.118838Z",
          "iopub.status.idle": "2023-03-29T18:55:50.128962Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.118785Z",
          "shell.execute_reply": "2023-03-29T18:55:50.127577Z"
        },
        "trusted": true,
        "id": "sjQvT8kB126O",
        "outputId": "53025845-dc3a-451d-f99b-285f3a855a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "0.10.1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data"
      ],
      "metadata": {
        "id": "3R__kbzS126P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"/kaggle/input/cisc-873-dm-w23-a2/train.csv\")\n",
        "test_data = pd.read_csv(\"/kaggle/input/cisc-873-dm-w23-a2/test.csv\")\n",
        "\n",
        "train_data.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.130302Z",
          "iopub.execute_input": "2023-03-29T18:55:50.130652Z",
          "iopub.status.idle": "2023-03-29T18:55:50.335829Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.130607Z",
          "shell.execute_reply": "2023-03-29T18:55:50.334427Z"
        },
        "trusted": true,
        "id": "_FbjVA6s126P",
        "outputId": "54c9f959-d0ac-40a6-eb2f-2f91b6706c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 54,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n0       0    3       2    14     18         2       2.0     14       12   \n1       1   14       1     3     10         2       NaN      8        8   \n2       1   14       1    13     10         8       8.0     10       10   \n3       1   38       2     9     20        18      13.0      6        7   \n4       1   24       2    14     20         6       6.0     20       17   \n\n     pid  ...  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  intel5_3  \\\n0  372.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n1   63.0  ...      8.0       8.0     7.0     8.0      NaN      NaN       NaN   \n2  331.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n3  200.0  ...      9.0       8.0     8.0     6.0      NaN      NaN       NaN   \n4  357.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n\n   fun5_3  amb5_3    id  \n0     NaN     NaN  2583  \n1     NaN     NaN  6830  \n2     NaN     NaN  4840  \n3     NaN     NaN  5508  \n4     NaN     NaN  4828  \n\n[5 rows x 192 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>idg</th>\n      <th>condtn</th>\n      <th>wave</th>\n      <th>round</th>\n      <th>position</th>\n      <th>positin1</th>\n      <th>order</th>\n      <th>partner</th>\n      <th>pid</th>\n      <th>...</th>\n      <th>sinc3_3</th>\n      <th>intel3_3</th>\n      <th>fun3_3</th>\n      <th>amb3_3</th>\n      <th>attr5_3</th>\n      <th>sinc5_3</th>\n      <th>intel5_3</th>\n      <th>fun5_3</th>\n      <th>amb5_3</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>14</td>\n      <td>18</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>14</td>\n      <td>12</td>\n      <td>372.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2583</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>14</td>\n      <td>1</td>\n      <td>3</td>\n      <td>10</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>8</td>\n      <td>63.0</td>\n      <td>...</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6830</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>14</td>\n      <td>1</td>\n      <td>13</td>\n      <td>10</td>\n      <td>8</td>\n      <td>8.0</td>\n      <td>10</td>\n      <td>10</td>\n      <td>331.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4840</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>38</td>\n      <td>2</td>\n      <td>9</td>\n      <td>20</td>\n      <td>18</td>\n      <td>13.0</td>\n      <td>6</td>\n      <td>7</td>\n      <td>200.0</td>\n      <td>...</td>\n      <td>9.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5508</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>24</td>\n      <td>2</td>\n      <td>14</td>\n      <td>20</td>\n      <td>6</td>\n      <td>6.0</td>\n      <td>20</td>\n      <td>17</td>\n      <td>357.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4828</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 192 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploar Data"
      ],
      "metadata": {
        "id": "5AOimypH126Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.338359Z",
          "iopub.execute_input": "2023-03-29T18:55:50.338759Z",
          "iopub.status.idle": "2023-03-29T18:55:50.362016Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.338720Z",
          "shell.execute_reply": "2023-03-29T18:55:50.360571Z"
        },
        "trusted": true,
        "id": "LE5nHGiM126Q",
        "outputId": "ecb7c4e1-b5b3-4896-8f91-e97e290f41e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5909 entries, 0 to 5908\nColumns: 192 entries, gender to id\ndtypes: float64(173), int64(11), object(8)\nmemory usage: 8.7+ MB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we extract numeric features and categorical features names\n",
        "# for later use\n",
        "\n",
        "# numeric features can be selected by: (based on the train_data.info() output )\n",
        "features_numeric = list(train_data.select_dtypes(include=['float64', 'int64']))\n",
        "\n",
        "# categorical features can be selected by: (based on the train_data.info() output )\n",
        "features_categorical = list(train_data.select_dtypes(include=['object']))\n",
        "\n",
        "print('categorical features:', features_categorical)\n",
        "# print('numeric features:', features_numeric)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.363475Z",
          "iopub.execute_input": "2023-03-29T18:55:50.364764Z",
          "iopub.status.idle": "2023-03-29T18:55:50.378685Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.364724Z",
          "shell.execute_reply": "2023-03-29T18:55:50.377309Z"
        },
        "trusted": true,
        "id": "jFIlnqtw126Q",
        "outputId": "0ea1acc4-2d75-4cdd-e6d7-76c9bccbc6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "categorical features: ['field', 'undergra', 'mn_sat', 'tuition', 'from', 'zipcode', 'income', 'career']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[['field', 'undergra', 'mn_sat', 'tuition', 'from', 'zipcode', 'income', 'career']].head() "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.380473Z",
          "iopub.execute_input": "2023-03-29T18:55:50.380935Z",
          "iopub.status.idle": "2023-03-29T18:55:50.407541Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.380888Z",
          "shell.execute_reply": "2023-03-29T18:55:50.405856Z"
        },
        "trusted": true,
        "id": "v8nGQWis126Q",
        "outputId": "d045e74b-5b47-427e-d760-4d2235ec3a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 57,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                    field  \\\n0  Ed.D. in higher education policy at TC   \n1                             Engineering   \n2                          Urban Planning   \n3                   International Affairs   \n4                                Business   \n\n                                           undergra    mn_sat    tuition  \\\n0                  University of Michigan-Ann Arbor  1,290.00  21,645.00   \n1                                               NaN       NaN        NaN   \n2  Rizvi College of Architecture, Bombay University       NaN        NaN   \n3                                               NaN       NaN        NaN   \n4                                   Harvard College  1,400.00  26,019.00   \n\n             from zipcode     income                             career  \n0   Palo Alto, CA     NaN        NaN               University President  \n1      Boston, MA   2,021        NaN  Engineer or iBanker or consultant  \n2   Bombay, India     NaN        NaN             Real Estate Consulting  \n3  Washington, DC  10,471  45,300.00                     public service  \n4     Midwest USA  66,208  46,138.00                          undecided  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>field</th>\n      <th>undergra</th>\n      <th>mn_sat</th>\n      <th>tuition</th>\n      <th>from</th>\n      <th>zipcode</th>\n      <th>income</th>\n      <th>career</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ed.D. in higher education policy at TC</td>\n      <td>University of Michigan-Ann Arbor</td>\n      <td>1,290.00</td>\n      <td>21,645.00</td>\n      <td>Palo Alto, CA</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>University President</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Engineering</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Boston, MA</td>\n      <td>2,021</td>\n      <td>NaN</td>\n      <td>Engineer or iBanker or consultant</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Urban Planning</td>\n      <td>Rizvi College of Architecture, Bombay University</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Bombay, India</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Real Estate Consulting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>International Affairs</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Washington, DC</td>\n      <td>10,471</td>\n      <td>45,300.00</td>\n      <td>public service</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Business</td>\n      <td>Harvard College</td>\n      <td>1,400.00</td>\n      <td>26,019.00</td>\n      <td>Midwest USA</td>\n      <td>66,208</td>\n      <td>46,138.00</td>\n      <td>undecided</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above we can see that most of columns are Float/Int but there are 8 columns have datatype Object,So we will need to convert datatype for some  columns like (mn_sat, tuition,zipcode, income) to float."
      ],
      "metadata": {
        "id": "CnNfCV7M126R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describe Data**</br>\n",
        "We need to view more information about our dataset using statistics to get clear view about it. "
      ],
      "metadata": {
        "id": "VYAxYOy5126R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.409591Z",
          "iopub.execute_input": "2023-03-29T18:55:50.410078Z",
          "iopub.status.idle": "2023-03-29T18:55:50.880672Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.410026Z",
          "shell.execute_reply": "2023-03-29T18:55:50.879717Z"
        },
        "trusted": true,
        "id": "U_rJ-bC1126R",
        "outputId": "671dd9a3-c85e-4af2-a41e-44678a8e0892"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 58,
          "output_type": "execute_result",
          "data": {
            "text/plain": "            gender          idg       condtn         wave        round  \\\ncount  5909.000000  5909.000000  5909.000000  5909.000000  5909.000000   \nmean      0.505331    17.360298     1.824843    11.347436    16.850228   \nstd       0.500014    10.947542     0.380133     6.011495     4.389246   \nmin       0.000000     1.000000     1.000000     1.000000     5.000000   \n25%       0.000000     8.000000     2.000000     7.000000    14.000000   \n50%       1.000000    16.000000     2.000000    11.000000    18.000000   \n75%       1.000000    26.000000     2.000000    15.000000    20.000000   \nmax       1.000000    44.000000     2.000000    21.000000    22.000000   \n\n          position     positin1       order      partner          pid  ...  \\\ncount  5909.000000  4591.000000  5909.00000  5909.000000  5901.000000  ...   \nmean      9.001523     9.254846     8.91166     8.962938   283.733266  ...   \nstd       5.482368     5.611803     5.45710     5.500706   158.993002  ...   \nmin       1.000000     1.000000     1.00000     1.000000     1.000000  ...   \n25%       4.000000     4.000000     4.00000     4.000000   153.000000  ...   \n50%       8.000000     9.000000     8.00000     8.000000   280.000000  ...   \n75%      13.000000    14.000000    13.00000    13.000000   409.000000  ...   \nmax      22.000000    22.000000    22.00000    22.000000   552.000000  ...   \n\n           sinc3_3     intel3_3       fun3_3       amb3_3      attr5_3  \\\ncount  2804.000000  2804.000000  2804.000000  2804.000000  1413.000000   \nmean      8.105563     8.377318     7.644437     7.398716     6.799717   \nstd       1.601011     1.459013     1.757559     1.956924     1.535768   \nmin       2.000000     3.000000     2.000000     1.000000     2.000000   \n25%       7.000000     8.000000     7.000000     6.000000     6.000000   \n50%       8.000000     8.000000     8.000000     8.000000     7.000000   \n75%       9.000000     9.000000     9.000000     9.000000     8.000000   \nmax      12.000000    12.000000    12.000000    12.000000    10.000000   \n\n           sinc5_3     intel5_3       fun5_3       amb5_3           id  \ncount  1413.000000  1413.000000  1413.000000  1413.000000  5909.000000  \nmean      7.631989     7.944798     7.162774     7.092711  4191.314943  \nstd       1.498024     1.320919     1.687431     1.713729  2408.009173  \nmin       2.000000     4.000000     1.000000     1.000000     0.000000  \n25%       7.000000     7.000000     6.000000     6.000000  2124.000000  \n50%       8.000000     8.000000     7.000000     7.000000  4210.000000  \n75%       9.000000     9.000000     8.000000     8.000000  6266.000000  \nmax      10.000000    10.000000    10.000000    10.000000  8372.000000  \n\n[8 rows x 184 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>idg</th>\n      <th>condtn</th>\n      <th>wave</th>\n      <th>round</th>\n      <th>position</th>\n      <th>positin1</th>\n      <th>order</th>\n      <th>partner</th>\n      <th>pid</th>\n      <th>...</th>\n      <th>sinc3_3</th>\n      <th>intel3_3</th>\n      <th>fun3_3</th>\n      <th>amb3_3</th>\n      <th>attr5_3</th>\n      <th>sinc5_3</th>\n      <th>intel5_3</th>\n      <th>fun5_3</th>\n      <th>amb5_3</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5909.000000</td>\n      <td>5909.000000</td>\n      <td>5909.000000</td>\n      <td>5909.000000</td>\n      <td>5909.000000</td>\n      <td>5909.000000</td>\n      <td>4591.000000</td>\n      <td>5909.00000</td>\n      <td>5909.000000</td>\n      <td>5901.000000</td>\n      <td>...</td>\n      <td>2804.000000</td>\n      <td>2804.000000</td>\n      <td>2804.000000</td>\n      <td>2804.000000</td>\n      <td>1413.000000</td>\n      <td>1413.000000</td>\n      <td>1413.000000</td>\n      <td>1413.000000</td>\n      <td>1413.000000</td>\n      <td>5909.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.505331</td>\n      <td>17.360298</td>\n      <td>1.824843</td>\n      <td>11.347436</td>\n      <td>16.850228</td>\n      <td>9.001523</td>\n      <td>9.254846</td>\n      <td>8.91166</td>\n      <td>8.962938</td>\n      <td>283.733266</td>\n      <td>...</td>\n      <td>8.105563</td>\n      <td>8.377318</td>\n      <td>7.644437</td>\n      <td>7.398716</td>\n      <td>6.799717</td>\n      <td>7.631989</td>\n      <td>7.944798</td>\n      <td>7.162774</td>\n      <td>7.092711</td>\n      <td>4191.314943</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.500014</td>\n      <td>10.947542</td>\n      <td>0.380133</td>\n      <td>6.011495</td>\n      <td>4.389246</td>\n      <td>5.482368</td>\n      <td>5.611803</td>\n      <td>5.45710</td>\n      <td>5.500706</td>\n      <td>158.993002</td>\n      <td>...</td>\n      <td>1.601011</td>\n      <td>1.459013</td>\n      <td>1.757559</td>\n      <td>1.956924</td>\n      <td>1.535768</td>\n      <td>1.498024</td>\n      <td>1.320919</td>\n      <td>1.687431</td>\n      <td>1.713729</td>\n      <td>2408.009173</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>2.000000</td>\n      <td>7.000000</td>\n      <td>14.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>4.00000</td>\n      <td>4.000000</td>\n      <td>153.000000</td>\n      <td>...</td>\n      <td>7.000000</td>\n      <td>8.000000</td>\n      <td>7.000000</td>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>7.000000</td>\n      <td>7.000000</td>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>2124.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>16.000000</td>\n      <td>2.000000</td>\n      <td>11.000000</td>\n      <td>18.000000</td>\n      <td>8.000000</td>\n      <td>9.000000</td>\n      <td>8.00000</td>\n      <td>8.000000</td>\n      <td>280.000000</td>\n      <td>...</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>7.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>7.000000</td>\n      <td>7.000000</td>\n      <td>4210.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>26.000000</td>\n      <td>2.000000</td>\n      <td>15.000000</td>\n      <td>20.000000</td>\n      <td>13.000000</td>\n      <td>14.000000</td>\n      <td>13.00000</td>\n      <td>13.000000</td>\n      <td>409.000000</td>\n      <td>...</td>\n      <td>9.000000</td>\n      <td>9.000000</td>\n      <td>9.000000</td>\n      <td>9.000000</td>\n      <td>8.000000</td>\n      <td>9.000000</td>\n      <td>9.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>6266.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>44.000000</td>\n      <td>2.000000</td>\n      <td>21.000000</td>\n      <td>22.000000</td>\n      <td>22.000000</td>\n      <td>22.000000</td>\n      <td>22.00000</td>\n      <td>22.000000</td>\n      <td>552.000000</td>\n      <td>...</td>\n      <td>12.000000</td>\n      <td>12.000000</td>\n      <td>12.000000</td>\n      <td>12.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>8372.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 184 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Nulls "
      ],
      "metadata": {
        "id": "J2U3afMX126R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isna().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.882430Z",
          "iopub.execute_input": "2023-03-29T18:55:50.882786Z",
          "iopub.status.idle": "2023-03-29T18:55:50.901647Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.882753Z",
          "shell.execute_reply": "2023-03-29T18:55:50.900181Z"
        },
        "trusted": true,
        "id": "wZ91OOCt126R",
        "outputId": "035eb28e-2e7c-4502-db7a-0ccdb9049d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 59,
          "output_type": "execute_result",
          "data": {
            "text/plain": "num_in_3    5449\nnumdat_3    4849\nexpnum      4627\namb7_2      4519\nsinc7_2     4519\nshar7_2     4505\nfun7_2      4498\nintel7_2    4498\nattr7_2     4498\nattr7_3     4496\nsinc7_3     4496\nintel7_3    4496\nfun7_3      4496\namb7_3      4496\nshar7_3     4496\nshar2_3     4496\nattr5_3     4496\nsinc5_3     4496\nintel5_3    4496\nfun5_3      4496\namb5_3      4496\nshar4_3     3838\nfun4_3      3838\nintel4_3    3838\nsinc4_3     3838\nattr4_3     3838\nattr2_3     3838\nsinc2_3     3838\nintel2_3    3838\nfun2_3      3838\namb2_3      3838\namb4_3      3838\nmn_sat      3674\ntuition     3365\nattr1_3     3105\nintel1_3    3105\namb3_3      3105\nsinc3_3     3105\nintel3_3    3105\nshar1_3     3105\namb1_3      3105\nfun1_3      3105\nfun3_3      3105\nattr3_3     3105\nsinc1_3     3105\ndate_3      3105\nyou_call    3105\nthem_cal    3105\namb3_s      3069\nsinc3_s     3069\nintel3_s    3069\nfun3_s      3069\nattr3_s     3069\nshar1_s     2994\namb1_s      2994\nfun1_s      2994\nintel1_s    2994\nsinc1_s     2994\nattr1_s     2994\nincome      2862\namb5_2      2821\nsinc5_2     2821\nfun5_2      2821\nattr5_2     2821\nintel5_2    2821\nattr5_1     2452\namb5_1      2452\nfun5_1      2452\nintel5_1    2452\nsinc5_1     2452\nundergra    2442\namb2_2      1846\nsinc4_2     1846\nintel2_2    1846\nsinc2_2     1846\nattr2_2     1846\nshar4_2     1846\namb4_2      1846\nfun2_2      1846\nintel4_2    1846\nfun4_2      1846\nattr4_2     1846\nshar2_2     1846\nshar4_1     1363\namb4_1      1346\nfun4_1      1346\nintel4_1    1346\nsinc4_1     1346\nattr4_1     1346\npositin1    1318\nmatch_es     826\nshar_o       787\nshar         762\nzipcode      720\nnumdat_2     670\nattr1_2      658\namb3_2       647\nlength       647\nsatis_2      647\nintel1_2     647\nfun1_2       647\namb1_2       647\nshar1_2      647\nattr3_2      647\nsinc1_2      647\nfun3_2       647\nintel3_2     647\nsinc3_2      647\namb_o        512\namb          504\nmet_o        275\nfun_o        265\nfun          256\nmet          252\nprob_o       235\nprob         227\nintel_o      220\nsinc_o       209\nintel        205\nsinc         200\nlike_o       183\nlike         175\nattr_o       153\nattr         143\nint_corr     109\ncareer_c     100\nshar1_1       88\npf_o_sha      83\nfun3_1        81\nintel3_1      81\namb3_1        81\nsinc3_1       81\nattr3_1       81\npf_o_amb      73\ndate          72\namb1_1        71\nexphappy      70\npf_o_fun      66\namb2_1        65\nshar2_1       65\nage_o         65\nfun1_1        65\ncareer        64\nage           63\npf_o_att      59\nfield_cd      59\npf_o_sin      59\npf_o_int      59\nfun2_1        58\nsinc2_1       58\nintel2_1      58\nattr2_1       58\nimprace       58\ndining        58\nimprelig      58\nconcerts      58\ntvsports      58\nsports        58\nmuseums       58\nart           58\nhiking        58\ngaming        58\nclubbing      58\nreading       58\ntv            58\ntheater       58\nmovies        58\nmusic         58\nshopping      58\nyoga          58\nattr1_1       58\nsinc1_1       58\nintel1_1      58\ngo_out        58\nexercise      58\nfrom          58\ngoal          58\nrace_o        48\nrace          45\nfield         45\npid            8\ngender         0\nidg            0\nsamerace       0\nmatch          0\npartner        0\norder          0\nposition       0\nround          0\nwave           0\ncondtn         0\nid             0\ndtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the observations, it is evident that the dataset contains a considerable amount of missing values. To address this issue, we can drop certain columns that have a high number of null values. Additionally, we can attempt to fill the missing values in other columns to salvage as much data as possible."
      ],
      "metadata": {
        "id": "Y6jjdpl-126S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Duplication"
      ],
      "metadata": {
        "id": "d-x7aMdv126S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Duplicated data:\", train_data.duplicated().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.905481Z",
          "iopub.execute_input": "2023-03-29T18:55:50.906630Z",
          "iopub.status.idle": "2023-03-29T18:55:50.960495Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.906588Z",
          "shell.execute_reply": "2023-03-29T18:55:50.959065Z"
        },
        "trusted": true,
        "id": "7LRzSD5c126S",
        "outputId": "445255a8-48d1-433a-a3bc-b844576758dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Duplicated data: 0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset does not contain any duplicate values across all the variables."
      ],
      "metadata": {
        "id": "OkoYdYk1126S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check if data imbalanced "
      ],
      "metadata": {
        "id": "fQ-iEFpm126S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize class distribution\n",
        "myexplode = [0.2, 0]\n",
        "counter = Counter(train_data['match'])\n",
        "Labels = ['Not Matched','Matched']\n",
        "plt.pie(counter.values(),labels = Labels , explode = myexplode, shadow = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:50.962156Z",
          "iopub.execute_input": "2023-03-29T18:55:50.962664Z",
          "iopub.status.idle": "2023-03-29T18:55:51.086251Z",
          "shell.execute_reply.started": "2023-03-29T18:55:50.962622Z",
          "shell.execute_reply": "2023-03-29T18:55:51.084794Z"
        },
        "trusted": true,
        "id": "6IDc0eTT126S",
        "outputId": "5258f476-eb1e-4255-fd02-f2b10bd0c619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is evident that the dataset is imbalanced, which can negatively impact the performance of our machine learning model. To address this issue, we can oversample the examples in the minority class using various techniques. We will experiment with different methods to find the most effective approach."
      ],
      "metadata": {
        "id": "Z_BKmvI2126T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Distribution Of Data "
      ],
      "metadata": {
        "id": "GGN3Kz_y126T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sns.pairplot(data=train_data,hue='match')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.088263Z",
          "iopub.execute_input": "2023-03-29T18:55:51.089099Z",
          "iopub.status.idle": "2023-03-29T18:55:51.095163Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.089050Z",
          "shell.execute_reply": "2023-03-29T18:55:51.093759Z"
        },
        "trusted": true,
        "id": "IX4VWgCJ126T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Corrolation Between Features And Target Column"
      ],
      "metadata": {
        "id": "5_SqW9O4126T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # calculate the correlation matrix\n",
        "# corr = train_data.corr()\n",
        "# fig, ax = plt.subplots(figsize=(20,20)) \n",
        "# # plot the heatmap\n",
        "# sns.heatmap(corr, \n",
        "#         xticklabels=corr.columns,\n",
        "#         yticklabels=corr.columns\n",
        "#            , annot=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.096994Z",
          "iopub.execute_input": "2023-03-29T18:55:51.097861Z",
          "iopub.status.idle": "2023-03-29T18:55:51.105770Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.097811Z",
          "shell.execute_reply": "2023-03-29T18:55:51.104470Z"
        },
        "trusted": true,
        "id": "ORFlMNpU126T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrM = train_data.corr()\n",
        "corrM"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.107722Z",
          "iopub.execute_input": "2023-03-29T18:55:51.108495Z",
          "iopub.status.idle": "2023-03-29T18:55:51.797722Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.108447Z",
          "shell.execute_reply": "2023-03-29T18:55:51.796208Z"
        },
        "trusted": true,
        "id": "yPUO1J-2126T",
        "outputId": "25aaa25b-2b7d-4dd8-cb70-4428339c1fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 64,
          "output_type": "execute_result",
          "data": {
            "text/plain": "            gender       idg    condtn      wave     round  position  \\\ngender    1.000000  0.032132 -0.000875 -0.004192  0.017755 -0.004047   \nidg       0.032132  1.000000  0.330587  0.093823  0.391918  0.164705   \ncondtn   -0.000875  0.330587  1.000000  0.219735  0.820898  0.331013   \nwave     -0.004192  0.093823  0.219735  1.000000  0.228917  0.079820   \nround     0.017755  0.391918  0.820898  0.228917  1.000000  0.380632   \nposition -0.004047  0.164705  0.331013  0.079820  0.380632  1.000000   \npositin1  0.000410  0.174651  0.306722  0.061166  0.368352  0.725619   \norder     0.009850  0.161976  0.331402  0.093478  0.397952  0.156510   \npartner   0.010318  0.139034  0.322467  0.087904  0.390320  0.164090   \npid      -0.056275  0.088372  0.218009  0.996714  0.218901  0.076666   \nmatch     0.004293  0.006630 -0.042888 -0.025673 -0.031313 -0.010135   \nint_corr  0.011520 -0.019557  0.043470  0.043207  0.017517 -0.014928   \nsamerace -0.017636 -0.014186  0.066645 -0.017883  0.035692  0.026598   \nage_o    -0.088138  0.001640  0.103362  0.101982  0.013366 -0.003581   \nrace_o    0.027244 -0.026071 -0.091912  0.067121 -0.062549 -0.092831   \npf_o_att -0.358661 -0.022933 -0.059716  0.064234 -0.022082  0.021470   \npf_o_sin  0.137767  0.018207  0.056617  0.000619  0.034419 -0.046014   \npf_o_int  0.119492  0.021344  0.039571 -0.059921  0.033372  0.075163   \npf_o_fun -0.054472  0.001556 -0.010274  0.020695 -0.016803 -0.058370   \npf_o_amb  0.349116 -0.019420 -0.040077 -0.121859 -0.059631 -0.006482   \npf_o_sha  0.131327  0.005954  0.012578 -0.003644  0.005248 -0.017178   \nattr_o   -0.131695  0.025992  0.000433 -0.027403 -0.048432 -0.011114   \nsinc_o   -0.035660 -0.014766 -0.039144 -0.049451 -0.087160 -0.033502   \nintel_o   0.056205 -0.025672 -0.019193 -0.042894 -0.073468 -0.030418   \nfun_o    -0.058486  0.015192 -0.031480 -0.008809 -0.073642 -0.003423   \namb_o     0.094500 -0.003001 -0.022295 -0.030943 -0.074191 -0.010865   \nshar_o   -0.027496 -0.001070 -0.028479 -0.051531 -0.055836 -0.025627   \nlike_o   -0.072048  0.000713 -0.035615 -0.046025 -0.072608 -0.027920   \nprob_o    0.004045 -0.001639 -0.037464  0.017731 -0.073096 -0.047533   \nmet_o     0.008274  0.031788  0.020551  0.049918  0.022617  0.037015   \nage       0.069427  0.027894  0.091264  0.090236  0.004127  0.001442   \nfield_cd -0.069071  0.069341  0.118383  0.123313  0.077153  0.010745   \nrace     -0.011617 -0.038091 -0.105011  0.077669 -0.083882 -0.086422   \nimprace  -0.113948  0.016161  0.044369 -0.030351  0.078953  0.066652   \nimprelig -0.208852  0.056349  0.083515 -0.054266  0.066776  0.023342   \ngoal      0.028397 -0.043024 -0.065811 -0.053324 -0.076306 -0.030210   \ndate     -0.099738 -0.025829  0.059261 -0.000005  0.070118  0.037104   \ngo_out    0.023241 -0.086469  0.044050  0.024167  0.073584  0.055182   \ncareer_c -0.007684  0.165261  0.015837  0.144097  0.030779  0.048137   \nsports    0.228759  0.077981  0.036334 -0.002509  0.023234 -0.036944   \ntvsports  0.145324  0.075633  0.013386 -0.023255  0.033805 -0.028955   \nexercise -0.077061 -0.005694 -0.037484 -0.015931 -0.071055 -0.002636   \ndining   -0.208608  0.021910  0.007183  0.105460 -0.018757 -0.006099   \nmuseums  -0.206215 -0.079038  0.029040  0.064333  0.017779  0.008001   \nart      -0.218061 -0.042987  0.039190  0.110946  0.037574 -0.010808   \nhiking   -0.065324  0.050733  0.008914  0.012606 -0.003676 -0.068490   \ngaming    0.217357  0.083485  0.005409  0.001732  0.075653  0.000536   \nclubbing -0.054334 -0.027315 -0.054789 -0.038468 -0.010795  0.024627   \nreading  -0.105863  0.008332  0.006538 -0.018368  0.035405 -0.020462   \ntv       -0.166900 -0.010950 -0.007640  0.018124 -0.013004 -0.019809   \ntheater  -0.305193  0.008516  0.026644  0.032895  0.000845 -0.003645   \nmovies   -0.160059 -0.039631  0.031231 -0.009459  0.012650  0.002444   \nconcerts -0.132015 -0.005953 -0.016695  0.051820 -0.019012 -0.023554   \nmusic    -0.077399  0.005671 -0.026277  0.049195 -0.033941 -0.008580   \nshopping -0.327006 -0.034792  0.029079  0.071717  0.000919  0.019994   \nyoga     -0.222094 -0.047489 -0.036536  0.026499 -0.071173 -0.066239   \nexphappy  0.212598 -0.010431  0.025067  0.059054  0.049766 -0.015050   \nexpnum    0.008436 -0.061917 -0.219622  0.010543 -0.206112 -0.087988   \nattr1_1   0.358708  0.064929 -0.058400  0.066159 -0.022033  0.029496   \nsinc1_1  -0.128036 -0.054237  0.073861 -0.004444  0.054769 -0.034389   \nintel1_1 -0.116111  0.001187  0.022598 -0.068378  0.018986  0.068122   \nfun1_1    0.038880 -0.005397 -0.011343  0.018212 -0.002919 -0.058613   \namb1_1   -0.353727 -0.091886 -0.037925 -0.106278 -0.084007 -0.010809   \nshar1_1  -0.138141  0.013375  0.021193  0.005778  0.023091 -0.027411   \nattr4_1   0.109446  0.039394  0.032700  0.493314  0.034487  0.072010   \nsinc4_1  -0.051003 -0.015084 -0.014103  0.189814  0.026545 -0.042490   \nintel4_1 -0.082631  0.004993  0.043723  0.272818  0.008698  0.016263   \nfun4_1    0.081553 -0.014381  0.009761  0.402315  0.002562 -0.033385   \namb4_1   -0.306273  0.063270  0.010402  0.141852 -0.004121 -0.031143   \nshar4_1  -0.030401 -0.044442  0.026313  0.235715  0.019805 -0.047731   \nattr2_1  -0.321041 -0.018334  0.000013  0.113596  0.000602  0.080404   \nsinc2_1   0.272461  0.015002  0.006631 -0.079352  0.047320 -0.056115   \nintel2_1  0.291767 -0.046757 -0.014672 -0.145004 -0.026564 -0.077221   \nfun2_1   -0.046865 -0.007714 -0.062844 -0.022612 -0.056252 -0.062787   \namb2_1    0.360655  0.005122  0.037463 -0.030698  0.019585  0.029416   \nshar2_1  -0.129111  0.068580  0.025546 -0.004710  0.010532 -0.021853   \nattr3_1  -0.097964 -0.003279 -0.021499  0.031454 -0.008915 -0.000843   \nsinc3_1  -0.112585  0.022840  0.039220  0.080159  0.041560 -0.037172   \nfun3_1   -0.124766  0.011894  0.006433  0.041330  0.004073 -0.056843   \nintel3_1  0.066972 -0.025905  0.036632 -0.020034  0.054837  0.027916   \namb3_1   -0.027999  0.013006 -0.026106  0.066982  0.009213  0.047543   \nattr5_1  -0.054282  0.021721 -0.001471  0.040485  0.015923  0.001034   \nsinc5_1  -0.120055 -0.002354 -0.024573  0.052784 -0.029875 -0.090004   \nintel5_1  0.033399  0.059064  0.037056  0.004592  0.062049 -0.015384   \nfun5_1   -0.046997  0.007296  0.071791  0.105681  0.026465 -0.054398   \namb5_1   -0.068505 -0.008596 -0.001089 -0.066055  0.001293 -0.011485   \nattr      0.150591 -0.029040  0.001257 -0.021996 -0.030136 -0.017458   \nsinc      0.052213 -0.042309 -0.035046 -0.045152 -0.065544 -0.037133   \nintel    -0.056286 -0.055453 -0.013140 -0.042712 -0.049731 -0.026030   \nfun       0.067724 -0.055276 -0.029161 -0.000692 -0.062719 -0.007740   \namb      -0.100807 -0.056884 -0.019165 -0.029314 -0.051643 -0.016013   \nshar      0.045450 -0.033752 -0.027448 -0.047732 -0.046708 -0.021186   \nlike      0.087993 -0.048893 -0.017051 -0.033305 -0.053121 -0.028364   \nprob     -0.002889 -0.002267 -0.013728  0.026464 -0.034377 -0.037648   \nmet      -0.008150 -0.073981 -0.149189 -0.534940 -0.165709 -0.062723   \nmatch_es  0.072425 -0.009075  0.160734  0.066307  0.176401  0.030625   \nattr1_s   0.227378  0.144504  0.125272 -0.036866  0.139061  0.099658   \nsinc1_s  -0.148228  0.037631  0.004089 -0.370831  0.113735 -0.007399   \nintel1_s -0.071305  0.071612  0.024626 -0.449692  0.216136  0.102865   \nfun1_s    0.045059 -0.007659  0.055672 -0.478134  0.190455  0.060828   \namb1_s   -0.225662  0.033745 -0.012921 -0.499079  0.152895  0.107787   \nshar1_s  -0.163630  0.052607  0.057951 -0.392774  0.241270  0.090933   \nattr3_s  -0.170337 -0.042979 -0.002199 -0.048040  0.000260 -0.070755   \nsinc3_s  -0.083114 -0.076855 -0.033920 -0.022823 -0.057385 -0.085861   \nintel3_s  0.079544 -0.110518  0.014676 -0.117192  0.052595 -0.010670   \nfun3_s   -0.151719  0.004248  0.006073  0.042108 -0.044370 -0.110437   \namb3_s    0.000678  0.049354  0.018129 -0.035084  0.021250  0.075103   \nsatis_2   0.233018 -0.068285  0.089599  0.025666  0.033352 -0.015543   \nlength   -0.119169 -0.042588 -0.074486 -0.031396 -0.087297 -0.010583   \nnumdat_2  0.009902 -0.014616  0.125513  0.052464  0.046609  0.029570   \nattr7_2   0.240079  0.101113  0.094626  0.140518  0.161243  0.230463   \nsinc7_2  -0.229532 -0.079521 -0.133667 -0.203478 -0.116902 -0.226966   \nintel7_2 -0.035797 -0.125661 -0.157890 -0.049091 -0.172290 -0.115714   \nfun7_2    0.050539  0.116986  0.164666  0.040259  0.095115  0.032051   \namb7_2   -0.118053 -0.098058  0.068928 -0.304713 -0.121248 -0.021613   \nshar7_2  -0.187272  0.038767  0.017435  0.015800  0.017017 -0.121056   \nattr1_2   0.286076  0.050717  0.025172  0.017277  0.030184  0.054764   \nsinc1_2  -0.119539 -0.081276  0.051594  0.005384  0.009784 -0.048538   \nintel1_2 -0.181103  0.027341 -0.032168  0.044726 -0.046451 -0.013350   \nfun1_2    0.039954 -0.024334 -0.009615 -0.117616 -0.011172 -0.056444   \namb1_2   -0.253489 -0.032963 -0.038700  0.005138 -0.022467  0.002718   \nshar1_2  -0.180378  0.008411 -0.004014  0.019403  0.024939 -0.002842   \nattr4_2   0.134797  0.043706  0.037410  0.506276  0.030875  0.059540   \nsinc4_2  -0.134006 -0.039498  0.036923  0.317918  0.077659  0.015477   \nintel4_2 -0.159543  0.010525  0.015670  0.300415  0.042230  0.053747   \nfun4_2    0.073694  0.020063  0.039591  0.317603  0.021948 -0.049850   \namb4_2   -0.213561 -0.007788  0.000135  0.070735 -0.057689 -0.050490   \nshar4_2  -0.062882  0.028333  0.004393  0.229321  0.035801 -0.012619   \nattr2_2  -0.249641  0.032125  0.021666  0.364277  0.081487  0.109483   \nsinc2_2   0.282042 -0.036260  0.001673 -0.117161 -0.068704 -0.079203   \nintel2_2  0.262282 -0.014203 -0.023484 -0.272287 -0.079662 -0.046209   \nfun2_2   -0.125038  0.004210 -0.016848  0.023925 -0.004139 -0.105188   \namb2_2    0.363901 -0.039150 -0.035504 -0.310607 -0.070813  0.009670   \nshar2_2  -0.169447  0.016351  0.032716 -0.231347  0.035671 -0.039712   \nattr3_2  -0.083599 -0.013440  0.011033  0.008712  0.036352 -0.001342   \nsinc3_2  -0.140321  0.009154  0.043116  0.030243  0.012534 -0.028218   \nintel3_2  0.095417 -0.005050  0.049328 -0.005593  0.057628  0.057750   \nfun3_2   -0.116650  0.008124  0.008403  0.070527  0.003904 -0.050530   \namb3_2   -0.007924  0.039601  0.040339  0.091290  0.054940  0.077231   \nattr5_2  -0.001287 -0.021981 -0.000160 -0.048536  0.017414 -0.039660   \nsinc5_2  -0.178150 -0.061233 -0.042388 -0.072139 -0.069566 -0.063697   \nintel5_2  0.054945 -0.130879 -0.001262 -0.044116  0.025495  0.014985   \nfun5_2   -0.009537  0.025262  0.048333  0.005875  0.023722 -0.056221   \namb5_2    0.064466 -0.057714  0.017924 -0.000992  0.013475  0.044112   \nyou_call  0.321151 -0.028005  0.028137  0.069609  0.029863  0.021048   \nthem_cal -0.267411  0.090237  0.166501  0.095790  0.163257  0.014472   \ndate_3    0.007755  0.057249  0.120132  0.114185  0.093302  0.034538   \nnumdat_3  0.003680  0.196831  0.061342 -0.183618  0.135128 -0.006652   \nnum_in_3  0.251143  0.016424  0.257876  0.173421  0.245137 -0.076935   \nattr1_3   0.291946  0.052652 -0.003305  0.091458 -0.005465  0.021833   \nsinc1_3  -0.095523 -0.068116 -0.030393  0.033867 -0.000397 -0.067908   \nintel1_3 -0.043765  0.062598 -0.014441 -0.133639 -0.040050  0.035103   \nfun1_3    0.073421 -0.029183  0.061891 -0.054681  0.067762  0.058265   \namb1_3   -0.279596 -0.036053  0.003579 -0.121360  0.010185 -0.024862   \nshar1_3  -0.240175  0.011663  0.025050  0.021159  0.024254 -0.016526   \nattr7_3   0.265643  0.050812  0.044714  0.118919  0.008609  0.084423   \nsinc7_3  -0.299939 -0.114605 -0.101834 -0.087408 -0.087175 -0.144410   \nintel7_3  0.065740  0.062704 -0.042053 -0.035181 -0.042486  0.022261   \nfun7_3    0.163306  0.089223  0.036073 -0.030592  0.166911  0.033813   \namb7_3   -0.274540 -0.031118 -0.061534 -0.181025 -0.038347 -0.004403   \nshar7_3  -0.186516 -0.072238  0.048955 -0.044943  0.018906 -0.064925   \nattr4_3   0.094793  0.174953  0.072068  0.584029  0.073596  0.031114   \nsinc4_3  -0.239556 -0.085686 -0.048438  0.199146 -0.056157 -0.142264   \nintel4_3  0.039176 -0.002741 -0.037550  0.307739 -0.039640 -0.060468   \nfun4_3    0.078254 -0.078506  0.020939  0.372885  0.082495  0.055664   \namb4_3   -0.213160 -0.000157 -0.009982  0.116946 -0.101453 -0.004656   \nshar4_3  -0.056307 -0.056162  0.093551  0.271893  0.102718  0.015256   \nattr2_3  -0.181189  0.122896  0.057393  0.576042  0.038028 -0.007567   \nsinc2_3   0.154823 -0.030499 -0.060318  0.246075 -0.049210 -0.048717   \nintel2_3  0.237649  0.059922 -0.010001  0.324646  0.016105 -0.002853   \nfun2_3   -0.149604 -0.092149  0.010686  0.396995  0.022080 -0.012413   \namb2_3    0.362593 -0.084710  0.017754 -0.003061  0.017948  0.040691   \nshar2_3  -0.252469  0.017551  0.055328 -0.095389  0.027282  0.009081   \nattr3_3  -0.150992  0.011633  0.087388 -0.001678  0.094818  0.060514   \nsinc3_3  -0.169387 -0.050350  0.044158  0.036948  0.036600  0.041865   \nintel3_3  0.011476 -0.060674  0.061796 -0.104562  0.022696  0.066377   \nfun3_3   -0.153701 -0.041080  0.069298  0.053971  0.059530  0.043142   \namb3_3   -0.066626 -0.005502  0.027447  0.000520  0.013107  0.054227   \nattr5_3  -0.133302  0.001764  0.077571  0.092556  0.034647  0.066300   \nsinc5_3  -0.277085 -0.018915 -0.087502 -0.001751 -0.035242  0.111991   \nintel5_3  0.080227 -0.093206  0.051449  0.014611 -0.012896  0.089283   \nfun5_3   -0.065562 -0.061079  0.106486  0.088716  0.104829  0.135484   \namb5_3    0.069091 -0.145645  0.123314  0.047841  0.092951  0.058825   \nid        0.006340 -0.036693 -0.044675 -0.001863 -0.029700 -0.026828   \n\n          positin1     order   partner       pid  ...   sinc3_3  intel3_3  \\\ngender    0.000410  0.009850  0.010318 -0.056275  ... -0.169387  0.011476   \nidg       0.174651  0.161976  0.139034  0.088372  ... -0.050350 -0.060674   \ncondtn    0.306722  0.331402  0.322467  0.218009  ...  0.044158  0.061796   \nwave      0.061166  0.093478  0.087904  0.996714  ...  0.036948 -0.104562   \nround     0.368352  0.397952  0.390320  0.218901  ...  0.036600  0.022696   \nposition  0.725619  0.156510  0.164090  0.076666  ...  0.041865  0.066377   \npositin1  1.000000  0.154697  0.152826  0.057726  ... -0.031083  0.040163   \norder     0.154697  1.000000  0.158508  0.089130  ...  0.000877  0.002350   \npartner   0.152826  0.158508  1.000000  0.112771  ...  0.016141  0.012612   \npid       0.057726  0.089130  0.112771  1.000000  ...  0.040356 -0.113803   \nmatch    -0.012012 -0.046686  0.009451 -0.024160  ... -0.013565  0.019756   \nint_corr -0.035027  0.015889 -0.018501  0.040282  ... -0.059689 -0.028056   \nsamerace  0.044748  0.004305 -0.010329 -0.013668  ...  0.014779  0.080131   \nage_o    -0.080785 -0.001549  0.025385  0.108678  ...  0.061237  0.039288   \nrace_o   -0.069159 -0.017122 -0.039588  0.060003  ... -0.000589 -0.019296   \npf_o_att  0.028941 -0.001589  0.051090  0.093871  ...  0.006564 -0.123352   \npf_o_sin -0.075246  0.008352 -0.064742 -0.012650  ... -0.024739  0.024120   \npf_o_int  0.051622  0.001846  0.015007 -0.064292  ... -0.033627 -0.019020   \npf_o_fun  0.012950 -0.003176 -0.009921  0.022132  ...  0.020859 -0.010636   \npf_o_amb -0.026111 -0.020790 -0.067648 -0.149614  ...  0.001719  0.147931   \npf_o_sha -0.018036  0.002012  0.008362 -0.016944  ...  0.029942  0.111504   \nattr_o   -0.027070 -0.022957 -0.043246 -0.019339  ... -0.042072  0.028249   \nsinc_o   -0.026049 -0.124237 -0.037962 -0.043689  ...  0.005042 -0.034780   \nintel_o  -0.048187 -0.091123 -0.047603 -0.045093  ...  0.003046  0.038570   \nfun_o    -0.032715 -0.058559 -0.050419 -0.004145  ... -0.021349  0.047319   \namb_o    -0.026521 -0.079342 -0.051124 -0.034429  ...  0.005720  0.080934   \nshar_o   -0.045861 -0.001224 -0.025884 -0.048017  ... -0.027548  0.065353   \nlike_o   -0.060944 -0.061194 -0.044188 -0.040668  ... -0.029362  0.027297   \nprob_o   -0.032497 -0.096395 -0.004352  0.020179  ... -0.007677  0.030686   \nmet_o     0.018081 -0.001150  0.002497  0.050574  ...  0.005969 -0.021632   \nage      -0.034133  0.000881  0.000008  0.085023  ... -0.023295 -0.001104   \nfield_cd -0.004896  0.035063  0.041827  0.121785  ... -0.060940 -0.067377   \nrace     -0.111482 -0.042934 -0.032099  0.075196  ...  0.015723 -0.118720   \nimprace   0.130577  0.027145  0.022614 -0.027255  ...  0.064441  0.135849   \nimprelig  0.053534  0.017375  0.019059 -0.040871  ...  0.141049  0.184416   \ngoal      0.003221 -0.023085 -0.040481 -0.049881  ...  0.069918  0.027805   \ndate      0.003945  0.024128  0.028116  0.003028  ...  0.018388 -0.121439   \ngo_out    0.077921  0.024387  0.019726  0.019214  ... -0.001070 -0.161524   \ncareer_c  0.080300  0.006180  0.002895  0.146303  ... -0.034842 -0.124857   \nsports   -0.078193  0.008790  0.001878 -0.014312  ...  0.059039  0.064729   \ntvsports -0.054733  0.005201  0.002481 -0.030770  ...  0.108768  0.053405   \nexercise -0.089496 -0.022582 -0.028422 -0.010085  ...  0.039707  0.033784   \ndining   -0.073877 -0.007313 -0.006899  0.117794  ...  0.110105  0.081312   \nmuseums  -0.017217  0.002759  0.009895  0.071494  ...  0.000583 -0.036061   \nart      -0.047413  0.012456  0.020741  0.120467  ...  0.025086 -0.051568   \nhiking   -0.065454 -0.012845 -0.000274  0.016052  ...  0.028004 -0.076248   \ngaming   -0.019060  0.020638  0.015907 -0.012166  ...  0.065780  0.004322   \nclubbing  0.047144 -0.004245 -0.007468 -0.040095  ...  0.082506 -0.030430   \nreading   0.011974  0.014931 -0.001645 -0.017688  ...  0.096574  0.132744   \ntv       -0.067140 -0.008448 -0.010343  0.026783  ...  0.042670  0.048799   \ntheater  -0.045004  0.003173  0.002555  0.047319  ...  0.038191 -0.121900   \nmovies   -0.034219 -0.002535  0.010053 -0.000266  ...  0.045916 -0.076999   \nconcerts -0.036230 -0.009631 -0.017752  0.059339  ...  0.020933 -0.151309   \nmusic    -0.046754 -0.018522 -0.017026  0.052336  ...  0.067068 -0.027028   \nshopping -0.009181  0.007340 -0.002754  0.089531  ...  0.052722  0.067411   \nyoga     -0.133878 -0.035965 -0.037529  0.037607  ...  0.021349 -0.054702   \nexphappy -0.051197  0.006015  0.027450  0.047259  ...  0.002897  0.047521   \nexpnum         NaN -0.067169 -0.077468  0.004025  ...  0.238088  0.310448   \nattr1_1   0.039966 -0.000206 -0.014164  0.058514  ... -0.165132 -0.053793   \nsinc1_1  -0.108037  0.008055  0.035212 -0.000547  ...  0.276773 -0.098424   \nintel1_1  0.113905  0.010694  0.011788 -0.061844  ... -0.139009  0.052604   \nfun1_1   -0.015776  0.008166  0.005819  0.012844  ... -0.121795 -0.003887   \namb1_1   -0.034438 -0.044883 -0.029271 -0.094237  ...  0.176973  0.119080   \nshar1_1  -0.018629  0.002939 -0.007603  0.005116  ...  0.073131  0.068290   \nattr4_1   0.065447  0.008602  0.007331  0.499765  ... -0.113153 -0.211445   \nsinc4_1  -0.069286  0.008469  0.006681  0.201632  ...  0.064290 -0.211495   \nintel4_1  0.041593 -0.007428 -0.005573  0.294825  ... -0.073756 -0.183882   \nfun4_1    0.021970 -0.000185 -0.000733  0.408668  ... -0.083362 -0.192402   \namb4_1   -0.032264  0.001953 -0.013533  0.176704  ...  0.092950 -0.165902   \nshar4_1  -0.055558  0.008589  0.003407  0.247361  ... -0.091868 -0.183753   \nattr2_1   0.087930 -0.001284 -0.003943  0.142768  ... -0.002899 -0.135342   \nsinc2_1  -0.066518  0.012382  0.016386 -0.101223  ... -0.002310  0.008459   \nintel2_1 -0.086568 -0.022672 -0.012907 -0.163611  ... -0.084576  0.043136   \nfun2_1    0.005288 -0.017481 -0.014848 -0.023950  ...  0.061218  0.084568   \namb2_1    0.005615  0.021711  0.010997 -0.056469  ...  0.007858  0.067564   \nshar2_1  -0.057630  0.004621  0.008613 -0.006804  ...  0.044612  0.105042   \nattr3_1  -0.056831 -0.001284 -0.006513  0.033409  ...  0.241868  0.418011   \nsinc3_1  -0.124761  0.004317  0.013982  0.080224  ...  0.595560  0.101004   \nfun3_1   -0.125989 -0.012746 -0.008545  0.047207  ...  0.243325  0.243808   \nintel3_1 -0.046548  0.024283  0.021033 -0.028808  ...  0.275452  0.648242   \namb3_1   -0.004873  0.006092  0.001432  0.068137  ...  0.181898  0.236303   \nattr5_1  -0.000589  0.003969  0.005443  0.043895  ...  0.144479  0.354587   \nsinc5_1  -0.113604 -0.012479 -0.015566  0.058008  ...  0.467349  0.123232   \nintel5_1 -0.044831  0.023665  0.024851 -0.004256  ...  0.219992  0.463971   \nfun5_1   -0.084207 -0.002723  0.013201  0.116088  ...  0.077312  0.124537   \namb5_1   -0.056936 -0.007957  0.008483 -0.057144  ...  0.170869  0.160568   \nattr     -0.041890 -0.015684  0.044669 -0.026312  ...  0.050352  0.054133   \nsinc     -0.081904 -0.109045 -0.011619 -0.044979  ...  0.117877  0.143153   \nintel    -0.094042 -0.070997 -0.030061 -0.039822  ...  0.106134  0.120312   \nfun      -0.055955 -0.049840  0.031848 -0.001095  ...  0.067943  0.066830   \namb      -0.079649 -0.062281 -0.009791 -0.022139  ...  0.104383  0.084675   \nshar     -0.053804  0.004834 -0.002430 -0.047593  ...  0.053673  0.099172   \nlike     -0.061637 -0.045581  0.009716 -0.035488  ...  0.063154  0.067423   \nprob     -0.045606 -0.070418  0.003500  0.028654  ...  0.122711  0.225129   \nmet      -0.042405 -0.066052 -0.063532 -0.516313  ... -0.035228  0.095836   \nmatch_es  0.062017  0.065416  0.066444  0.063662  ...  0.021927  0.238484   \nattr1_s   0.029689  0.056094  0.041694 -0.016376  ... -0.125570 -0.114423   \nsinc1_s  -0.008551  0.034659  0.037056 -0.338712  ...  0.141123 -0.072959   \nintel1_s  0.103964  0.096128  0.099265 -0.420033  ...  0.008616  0.128174   \nfun1_s    0.057004  0.079385  0.079734 -0.459403  ... -0.004265  0.222721   \namb1_s    0.127354  0.064455  0.068826 -0.478852  ...  0.081957  0.213335   \nshar1_s   0.135785  0.102709  0.102039 -0.366181  ...  0.095394  0.107575   \nattr3_s  -0.177439 -0.004447 -0.009027 -0.033833  ...  0.183761  0.364096   \nsinc3_s  -0.126159 -0.037781 -0.035741 -0.014486  ...  0.505139  0.256875   \nintel3_s -0.077018  0.012341  0.019171 -0.122699  ...  0.206168  0.721019   \nfun3_s   -0.141682 -0.035326 -0.031097  0.054940  ...  0.264958  0.379005   \namb3_s   -0.004171  0.002221  0.007901 -0.029416  ...  0.168138  0.414846   \nsatis_2  -0.073251  0.004668  0.025536  0.018324  ...  0.070152  0.069002   \nlength   -0.007105 -0.029560 -0.032727 -0.025512  ...  0.053620  0.083446   \nnumdat_2  0.081855  0.015751  0.012016  0.054740  ...  0.075689  0.143969   \nattr7_2   0.196614  0.056029  0.068460  0.105500  ... -0.423815  0.030720   \nsinc7_2  -0.270938 -0.040826 -0.059975 -0.158737  ...  0.442285 -0.125988   \nintel7_2 -0.011696 -0.046442 -0.084981 -0.053771  ...  0.119599  0.114853   \nfun7_2    0.016590  0.019963  0.061224  0.037684  ...  0.017275  0.088979   \namb7_2   -0.052506 -0.033037 -0.065445 -0.294113  ...  0.173409 -0.064826   \nshar7_2  -0.045231  0.004111 -0.003744  0.047792  ...  0.187403 -0.075326   \nattr1_2   0.017208  0.021672  0.013804  0.013149  ... -0.179189 -0.099979   \nsinc1_2  -0.121358 -0.010012  0.009305  0.007889  ...  0.305937 -0.035080   \nintel1_2  0.070433 -0.017717 -0.027963  0.051520  ... -0.056578  0.075657   \nfun1_2   -0.025703 -0.007437  0.002146 -0.119528  ... -0.089612 -0.047710   \namb1_2    0.022552 -0.014914 -0.013337  0.006846  ...  0.178237  0.131004   \nshar1_2   0.031258  0.012869  0.006547  0.022413  ...  0.026500  0.088735   \nattr4_2   0.022563  0.014388  0.001388  0.509596  ... -0.179959 -0.242490   \nsinc4_2   0.049633  0.018436  0.016394  0.336628  ...  0.108105 -0.253635   \nintel4_2  0.073411  0.011362  0.003559  0.325448  ... -0.011493 -0.285922   \nfun4_2   -0.016466  0.016636  0.008282  0.329761  ...  0.023175 -0.094106   \namb4_2   -0.062375 -0.039256 -0.029746  0.104471  ...  0.077059 -0.071351   \nshar4_2   0.054523  0.012502  0.003183  0.240849  ...  0.010682 -0.074188   \nattr2_2   0.057990  0.026639  0.033129  0.390561  ...  0.014207 -0.264648   \nsinc2_2  -0.036794 -0.026157 -0.032839 -0.144230  ... -0.059786  0.057889   \nintel2_2 -0.057347 -0.034406 -0.028891 -0.294044  ... -0.068325  0.243352   \nfun2_2   -0.056649 -0.002939  0.009590  0.034163  ...  0.073385  0.038466   \namb2_2    0.038334 -0.008177 -0.028579 -0.343063  ...  0.012500  0.131631   \nshar2_2  -0.017535  0.009599  0.003964 -0.222360  ...  0.018925  0.144862   \nattr3_2   0.002049  0.018473  0.008956  0.009011  ...  0.283241  0.449818   \nsinc3_2  -0.134389 -0.014251  0.000096  0.037178  ...  0.701755  0.245663   \nintel3_2  0.026342  0.020453  0.023230 -0.014879  ...  0.345728  0.721426   \nfun3_2   -0.076781 -0.022651 -0.001706  0.077710  ...  0.227269  0.270928   \namb3_2   -0.001756  0.016422  0.018675  0.090609  ...  0.308090  0.329965   \nattr5_2  -0.037168  0.011537 -0.005290 -0.052292  ...  0.144200  0.319702   \nsinc5_2  -0.138315 -0.043010 -0.030502 -0.052526  ...  0.517034  0.275302   \nintel5_2 -0.049397  0.000735  0.013367 -0.055936  ...  0.217510  0.455876   \nfun5_2   -0.064562 -0.008307  0.006360  0.009758  ...  0.159925  0.224323   \namb5_2   -0.058697 -0.003514  0.004763 -0.012435  ...  0.179030  0.211334   \nyou_call  0.004019  0.007800  0.031776  0.053962  ...  0.028763  0.026624   \nthem_cal  0.037554  0.061944  0.065983  0.114239  ...  0.010946 -0.021943   \ndate_3   -0.003920  0.045446  0.044604  0.114708  ... -0.051228 -0.080365   \nnumdat_3  0.004385  0.033122  0.069489 -0.181726  ... -0.005622  0.048642   \nnum_in_3  0.033308  0.075358  0.091934  0.157377  ...  0.010967  0.113989   \nattr1_3   0.025084  0.000844 -0.006415  0.088425  ... -0.194855 -0.115881   \nsinc1_3  -0.095138 -0.013001  0.006763  0.036280  ...  0.266495 -0.027765   \nintel1_3  0.019810 -0.008996 -0.023158 -0.129687  ... -0.083965  0.060835   \nfun1_3    0.178941  0.036429  0.044068 -0.064903  ...  0.005032  0.094166   \namb1_3   -0.106862  0.002551  0.001750 -0.118295  ...  0.136103  0.139973   \nshar1_3   0.008345  0.006491 -0.001156  0.024263  ...  0.051439  0.002686   \nattr7_3   0.062109 -0.004360  0.008913  0.106994  ... -0.269991 -0.037334   \nsinc7_3  -0.142299 -0.040043 -0.037115 -0.064473  ...  0.376003  0.028562   \nintel7_3 -0.066124  0.006698 -0.021499 -0.046402  ...  0.005997  0.054931   \nfun7_3    0.126571  0.069327  0.070966 -0.059019  ...  0.002913  0.105665   \namb7_3   -0.015219 -0.005659 -0.019524 -0.158031  ...  0.056967 -0.131288   \nshar7_3  -0.015383 -0.002368 -0.000267 -0.033759  ...  0.044720 -0.063683   \nattr4_3   0.014891  0.009074  0.015907  0.590255  ... -0.110277 -0.237046   \nsinc4_3  -0.101457 -0.048693 -0.034430  0.229317  ...  0.070964 -0.166425   \nintel4_3 -0.004987 -0.011341 -0.032447  0.318122  ... -0.102617 -0.160289   \nfun4_3    0.125276  0.028365  0.025944  0.378940  ...  0.011046 -0.125946   \namb4_3    0.025779 -0.057178 -0.041156  0.148365  ...  0.021287 -0.046057   \nshar4_3   0.094471  0.038042  0.039546  0.280248  ... -0.059340 -0.134786   \nattr2_3  -0.010926 -0.015732  0.005954  0.607683  ... -0.016681 -0.257133   \nsinc2_3  -0.026804 -0.035766 -0.036746  0.241499  ... -0.064288 -0.111796   \nintel2_3  0.030367  0.011661 -0.008189  0.309253  ... -0.153843 -0.125591   \nfun2_3    0.058210 -0.000422  0.008930  0.427383  ... -0.013369 -0.172522   \namb2_3    0.045706  0.022774  0.013225 -0.021860  ... -0.047040 -0.047872   \nshar2_3  -0.000387  0.016610 -0.009307 -0.075240  ...  0.033459 -0.071692   \nattr3_3   0.104672  0.039429  0.028673 -0.001053  ...  0.400349  0.516139   \nsinc3_3  -0.031083  0.000877  0.016141  0.040356  ...  1.000000  0.407683   \nintel3_3  0.040163  0.002350  0.012612 -0.113803  ...  0.407683  1.000000   \nfun3_3    0.073800  0.006350  0.021345  0.056667  ...  0.398775  0.440359   \namb3_3   -0.024003  0.002645 -0.000157 -0.001845  ...  0.363356  0.527568   \nattr5_3   0.201598  0.018610  0.002868  0.108400  ...  0.174329  0.421117   \nsinc5_3   0.125594 -0.035897 -0.002601  0.021372  ...  0.608213  0.315857   \nintel5_3  0.106791 -0.004632  0.003345  0.009585  ...  0.288062  0.683871   \nfun5_3    0.232412  0.016419  0.031795  0.096788  ...  0.160204  0.295997   \namb5_3    0.030543  0.029174  0.050511  0.043115  ...  0.135499  0.353093   \nid       -0.011816 -0.016321  0.006595 -0.002027  ...  0.020346  0.004193   \n\n            fun3_3    amb3_3   attr5_3   sinc5_3  intel5_3    fun5_3  \\\ngender   -0.153701 -0.066626 -0.133302 -0.277085  0.080227 -0.065562   \nidg      -0.041080 -0.005502  0.001764 -0.018915 -0.093206 -0.061079   \ncondtn    0.069298  0.027447  0.077571 -0.087502  0.051449  0.106486   \nwave      0.053971  0.000520  0.092556 -0.001751  0.014611  0.088716   \nround     0.059530  0.013107  0.034647 -0.035242 -0.012896  0.104829   \nposition  0.043142  0.054227  0.066300  0.111991  0.089283  0.135484   \npositin1  0.073800 -0.024003  0.201598  0.125594  0.106791  0.232412   \norder     0.006350  0.002645  0.018610 -0.035897 -0.004632  0.016419   \npartner   0.021345 -0.000157  0.002868 -0.002601  0.003345  0.031795   \npid       0.056667 -0.001845  0.108400  0.021372  0.009585  0.096788   \nmatch     0.024759  0.014811  0.042603 -0.040531 -0.011494  0.015977   \nint_corr -0.052016 -0.031000 -0.014248 -0.130741  0.058994 -0.082505   \nsamerace  0.033090 -0.013831  0.071866  0.004318  0.065095  0.032052   \nage_o     0.012037  0.020727  0.007459  0.041266  0.020879 -0.038257   \nrace_o    0.009734  0.019909 -0.022354  0.025867 -0.001924  0.025706   \npf_o_att -0.005803 -0.073788  0.060730  0.121022 -0.044838  0.034232   \npf_o_sin  0.007306  0.021869 -0.034533 -0.078758  0.010946 -0.003366   \npf_o_int -0.039907 -0.023281 -0.007365 -0.030232  0.027977  0.001947   \npf_o_fun  0.010691  0.011182 -0.011298  0.039741 -0.014393  0.005304   \npf_o_amb  0.002992  0.060453 -0.083804 -0.120149  0.047995 -0.061783   \npf_o_sha  0.039563  0.085489  0.002692 -0.060377  0.026938 -0.024538   \nattr_o    0.127471  0.072266  0.229829 -0.044499 -0.024843  0.131459   \nsinc_o   -0.009639 -0.050024 -0.056311 -0.006763 -0.082100 -0.033609   \nintel_o  -0.028981 -0.025375 -0.029575 -0.029827  0.013227 -0.058197   \nfun_o     0.156654  0.060780  0.114706  0.000202 -0.022603  0.153961   \namb_o     0.039626  0.059628  0.034623 -0.052729  0.022583  0.019996   \nshar_o    0.074508  0.042933  0.128149 -0.042126  0.006964  0.075896   \nlike_o    0.092382  0.027875  0.113651 -0.027713 -0.037313  0.087366   \nprob_o    0.043200  0.014001  0.013743 -0.017628 -0.032689  0.028500   \nmet_o     0.001133 -0.034778  0.028479  0.058930 -0.006701  0.063289   \nage      -0.091368 -0.134249 -0.096014 -0.020274  0.087549 -0.076022   \nfield_cd -0.022162 -0.016643 -0.089767 -0.184357 -0.007130 -0.232345   \nrace     -0.060218 -0.022075 -0.180510  0.036745 -0.108570 -0.190634   \nimprace   0.168694  0.199266  0.198056  0.221680  0.122016  0.273409   \nimprelig  0.231129  0.126834  0.220660  0.314702  0.307723  0.259477   \ngoal     -0.102437 -0.091611  0.046964  0.156584  0.146097 -0.020968   \ndate     -0.272808 -0.188325 -0.264036  0.033016 -0.138150 -0.202900   \ngo_out   -0.336673 -0.225101 -0.375717 -0.002031 -0.149558 -0.265636   \ncareer_c -0.036115  0.034519 -0.014018 -0.020888  0.105055 -0.071122   \nsports    0.139470  0.179588  0.045596 -0.068817  0.049949  0.114218   \ntvsports  0.097166 -0.014492 -0.025214  0.093377 -0.036258  0.088232   \nexercise  0.180618  0.116326  0.135917  0.048008 -0.101250  0.003234   \ndining    0.177058  0.253417  0.194289 -0.032040  0.098567 -0.015668   \nmuseums  -0.002971 -0.034990  0.045490 -0.103755 -0.039821 -0.082370   \nart       0.069272 -0.020453 -0.028968 -0.077500 -0.024287 -0.017419   \nhiking    0.030309 -0.024546 -0.093525 -0.075179 -0.094685 -0.106205   \ngaming   -0.016737  0.070400 -0.153726 -0.010430 -0.136067 -0.103754   \nclubbing  0.150512  0.140721 -0.072702 -0.205654 -0.186982 -0.064888   \nreading  -0.084584  0.000408  0.019147 -0.101042  0.069386 -0.173555   \ntv        0.046928  0.088496  0.064928  0.086154 -0.067903  0.091350   \ntheater   0.041026 -0.055303 -0.114084  0.022848 -0.088945 -0.043596   \nmovies   -0.023180 -0.039844 -0.055256  0.088386  0.029035 -0.075798   \nconcerts -0.013663 -0.075999  0.004560  0.028306 -0.003903 -0.037652   \nmusic     0.095359  0.024996  0.012711  0.001373 -0.009794 -0.042046   \nshopping  0.293958  0.224608  0.263795 -0.001206 -0.117283  0.127488   \nyoga      0.100792  0.030477  0.077607 -0.055472 -0.067602 -0.041259   \nexphappy  0.170751  0.055124  0.062260 -0.113926  0.084459  0.163823   \nexpnum    0.403202  0.364171       NaN       NaN       NaN       NaN   \nattr1_1   0.016634  0.017151  0.141781 -0.126329  0.028733  0.053737   \nsinc1_1  -0.135019 -0.221646 -0.218225  0.266792 -0.037643 -0.030022   \nintel1_1 -0.106786 -0.055398  0.145876 -0.002869  0.182763 -0.025429   \nfun1_1    0.208838 -0.022123  0.041759 -0.195344 -0.060989  0.080179   \namb1_1    0.143643  0.338457  0.019402 -0.065959 -0.218550 -0.110028   \nshar1_1  -0.052838  0.010282 -0.156318  0.176820  0.060130  0.026773   \nattr4_1  -0.026292 -0.105653  0.072470 -0.058278  0.080330  0.060939   \nsinc4_1  -0.135032 -0.147860 -0.153126  0.072240 -0.049013 -0.002854   \nintel4_1 -0.032640 -0.114885  0.015993 -0.081432 -0.047351 -0.088294   \nfun4_1   -0.023672 -0.100802  0.056719 -0.021706 -0.012436  0.105909   \namb4_1   -0.072509 -0.126621  0.040663  0.139908 -0.079058 -0.018185   \nshar4_1  -0.229451 -0.122882 -0.045712  0.073514  0.048611 -0.123826   \nattr2_1   0.010052 -0.049852  0.006362  0.192893 -0.100061  0.091315   \nsinc2_1  -0.046643 -0.032854 -0.072877 -0.163560 -0.056843 -0.043213   \nintel2_1 -0.004259  0.037097 -0.001693 -0.178927  0.058929 -0.022984   \nfun2_1    0.030122  0.069966  0.251204  0.089477  0.053217  0.053786   \namb2_1    0.067664  0.061583 -0.186672 -0.180578 -0.056502  0.005160   \nshar2_1  -0.078481 -0.019268  0.046571  0.061499  0.223123 -0.204678   \nattr3_1   0.453654  0.328365  0.728599  0.062970  0.253562  0.200068   \nsinc3_1   0.151177  0.121634 -0.008482  0.346617  0.118511 -0.092052   \nfun3_1    0.780468  0.395295  0.238455  0.083827  0.063552  0.567068   \nintel3_1  0.248223  0.374548  0.168583  0.126810  0.573897  0.043509   \namb3_1    0.313178  0.717328  0.038822 -0.042573  0.113309  0.012905   \nattr5_1   0.453205  0.239123  0.783188  0.061684  0.266386  0.366830   \nsinc5_1   0.024818 -0.112742  0.003268  0.418335  0.232349 -0.018190   \nintel5_1  0.151436  0.166192  0.194008  0.231658  0.600613  0.025814   \nfun5_1    0.670564  0.228003  0.295534  0.096232  0.064198  0.587761   \namb5_1    0.366440  0.475958  0.166571  0.133860  0.253957  0.181020   \nattr     -0.003241 -0.023687 -0.094509 -0.089235  0.047135 -0.064726   \nsinc      0.096242  0.051357 -0.010054  0.002014  0.126807  0.029569   \nintel     0.099107  0.063536 -0.036674 -0.074296  0.042609 -0.040037   \nfun       0.056486  0.018673 -0.062324 -0.046059  0.055066 -0.048143   \namb       0.111915  0.080133 -0.011721 -0.043762  0.030984 -0.031694   \nshar      0.055434  0.052385 -0.053168  0.002786  0.072060 -0.071557   \nlike      0.039788  0.020897 -0.038667 -0.072835  0.023475 -0.021692   \nprob      0.198384  0.151261  0.193262  0.052151  0.115475  0.102856   \nmet       0.014692  0.065309  0.126795 -0.049179  0.008415  0.086748   \nmatch_es  0.220876  0.090001  0.162734 -0.038218  0.083571  0.192865   \nattr1_s  -0.033959 -0.106962 -0.004976 -0.026713 -0.169411  0.140237   \nsinc1_s  -0.030216 -0.124861 -0.050045  0.192493 -0.031094 -0.043958   \nintel1_s -0.026696  0.153747  0.252735  0.048295 -0.016186  0.023570   \nfun1_s    0.380015  0.201385  0.001344 -0.002813  0.018332  0.284007   \namb1_s    0.233355  0.422025  0.113646  0.011880  0.009424  0.139955   \nshar1_s  -0.059192  0.028756 -0.074977  0.197201  0.223003  0.005782   \nattr3_s   0.411011  0.296898  0.645850  0.097216  0.207239  0.245253   \nsinc3_s   0.107078  0.202500 -0.008320  0.536494  0.240621  0.223292   \nintel3_s  0.324110  0.366250  0.107097  0.023433  0.633459  0.052304   \nfun3_s    0.743430  0.362475  0.443814  0.178383  0.205753  0.618454   \namb3_s    0.339094  0.752821  0.066160 -0.094443  0.171887  0.046869   \nsatis_2   0.001383 -0.059468 -0.131076  0.060918  0.138220 -0.021568   \nlength    0.087704  0.016259  0.005104 -0.040089  0.011840 -0.015701   \nnumdat_2  0.088561  0.084523  0.066042  0.013466  0.176579  0.117053   \nattr7_2   0.086469 -0.023553  0.240206 -0.259468  0.039075  0.078242   \nsinc7_2  -0.133264 -0.271622 -0.222895  0.358466 -0.127123 -0.035996   \nintel7_2 -0.371799  0.120080 -0.065080  0.085637  0.052095 -0.051027   \nfun7_2    0.441174 -0.024716  0.083959 -0.039435 -0.054284  0.070718   \namb7_2   -0.352185  0.163962 -0.274135 -0.024126 -0.083176 -0.240096   \nshar7_2   0.105244  0.123534 -0.104483  0.282007  0.131060  0.152609   \nattr1_2  -0.017683  0.005369  0.134024 -0.202915 -0.045205  0.038521   \nsinc1_2  -0.120756 -0.172083 -0.115117  0.250008 -0.023996 -0.110787   \nintel1_2 -0.025681  0.001921  0.197443  0.045393  0.175110 -0.004640   \nfun1_2    0.156329 -0.099554 -0.077956  0.007568 -0.120197  0.181430   \namb1_2    0.094893  0.322076 -0.158519 -0.039641 -0.119619 -0.142052   \nshar1_2  -0.025856 -0.025786 -0.102186  0.117239  0.134727  0.060368   \nattr4_2  -0.028967 -0.054571 -0.055860 -0.178872  0.039273 -0.086996   \nsinc4_2  -0.050362 -0.132106  0.058395  0.113777 -0.090325  0.071470   \nintel4_2 -0.062223 -0.143255  0.062297  0.023841 -0.128432  0.051341   \nfun4_2   -0.031029 -0.120842  0.030319  0.195694  0.129897  0.116196   \namb4_2   -0.121340 -0.095546 -0.046012 -0.047679 -0.020109 -0.062717   \nshar4_2  -0.104147 -0.070030  0.003800  0.096599  0.039976  0.030900   \nattr2_2  -0.018316 -0.129391  0.003390  0.199671 -0.050254  0.076758   \nsinc2_2   0.042891 -0.031762 -0.037131 -0.263691 -0.011343 -0.081519   \nintel2_2  0.045944  0.269488  0.041522 -0.173493  0.144448 -0.092637   \nfun2_2   -0.028343 -0.077306  0.175831  0.146663 -0.077671  0.001042   \namb2_2   -0.000921  0.088803 -0.231054 -0.161803 -0.017971  0.006171   \nshar2_2  -0.001040  0.079423  0.100580  0.048894  0.112721  0.019140   \nattr3_2   0.488424  0.397723  0.778450  0.176612  0.267179  0.360296   \nsinc3_2   0.180129  0.191822  0.125471  0.615330  0.272623  0.189890   \nintel3_2  0.386880  0.422864  0.271990  0.212728  0.673059  0.189852   \nfun3_2    0.806554  0.391272  0.308667  0.175207  0.134801  0.605386   \namb3_2    0.406224  0.810285  0.146067  0.077369  0.208075  0.081199   \nattr5_2   0.366612  0.155311  0.832708  0.124502  0.231306  0.376704   \nsinc5_2   0.188608 -0.038665  0.127302  0.637660  0.339848  0.187536   \nintel5_2  0.225983  0.159733  0.274736  0.408256  0.650015  0.280329   \nfun5_2    0.678271  0.323849  0.264236  0.217260  0.144337  0.656616   \namb5_2    0.407815  0.567315  0.139705  0.207738  0.263271  0.279301   \nyou_call  0.038245  0.076199 -0.100722  0.070509  0.006323 -0.033755   \nthem_cal  0.108908  0.014056  0.176633 -0.097311 -0.177227  0.102825   \ndate_3   -0.063376 -0.041458  0.038475 -0.034999 -0.072381  0.018261   \nnumdat_3 -0.033306  0.090593  0.018597 -0.058040 -0.080342  0.076046   \nnum_in_3  0.158014 -0.002839  0.083457 -0.006671 -0.039418 -0.021460   \nattr1_3  -0.013441 -0.020330  0.151984 -0.183062 -0.002282  0.071728   \nsinc1_3  -0.109881 -0.151111 -0.143629  0.172087 -0.072747 -0.043329   \nintel1_3 -0.179574 -0.117099  0.063085 -0.001316  0.181427 -0.185431   \nfun1_3    0.332448  0.097415 -0.011183  0.004676  0.010281  0.222417   \namb1_3    0.106014  0.361924 -0.163332 -0.094961 -0.144967 -0.193320   \nshar1_3  -0.008022 -0.063536 -0.051587  0.128463  0.033021  0.019191   \nattr7_3   0.091908  0.001703  0.120429 -0.101053  0.049591  0.073153   \nsinc7_3  -0.049370 -0.133591 -0.070949  0.194890 -0.081343  0.010862   \nintel7_3 -0.149483  0.105161 -0.059777 -0.081319  0.054450 -0.260065   \nfun7_3    0.190674 -0.055382  0.093876  0.054805 -0.055669  0.255162   \namb7_3   -0.200213  0.104107 -0.184484 -0.060530 -0.020241 -0.202434   \nshar7_3  -0.049209 -0.060220 -0.059918  0.005215 -0.030004 -0.023135   \nattr4_3  -0.042730 -0.093987  0.124561 -0.026355  0.043986  0.088047   \nsinc4_3  -0.094704 -0.116336 -0.112648  0.009514 -0.078527 -0.158964   \nintel4_3 -0.061133 -0.106454  0.017918 -0.255931 -0.029460 -0.165309   \nfun4_3   -0.028086 -0.143835 -0.073608  0.215811  0.062933  0.167107   \namb4_3   -0.062640 -0.017990 -0.020759  0.032481 -0.002663 -0.009322   \nshar4_3  -0.035968 -0.101789 -0.026038  0.044714 -0.023516  0.005492   \nattr2_3  -0.035511 -0.139484  0.161679  0.147940 -0.058050  0.139143   \nsinc2_3  -0.060138 -0.116035 -0.116175 -0.152146 -0.013163 -0.056954   \nintel2_3  0.014203 -0.027926  0.017131 -0.257159  0.035891 -0.108908   \nfun2_3   -0.139070 -0.127763  0.076953  0.138838  0.031338 -0.019293   \namb2_3   -0.103486 -0.009560 -0.272513 -0.023040  0.063912 -0.049271   \nshar2_3  -0.077305 -0.032421 -0.027785  0.005579 -0.033531 -0.044611   \nattr3_3   0.577210  0.438404  0.858673  0.224796  0.335831  0.394979   \nsinc3_3   0.398775  0.363356  0.174329  0.608213  0.288062  0.160204   \nintel3_3  0.440359  0.527568  0.421117  0.315857  0.683871  0.295997   \nfun3_3    1.000000  0.533005  0.410789  0.180240  0.240322  0.771447   \namb3_3    0.533005  1.000000  0.235485  0.122856  0.268646  0.331533   \nattr5_3   0.410789  0.235485  1.000000  0.190198  0.355876  0.412403   \nsinc5_3   0.180240  0.122856  0.190198  1.000000  0.498313  0.335823   \nintel5_3  0.240322  0.268646  0.355876  0.498313  1.000000  0.262987   \nfun5_3    0.771447  0.331533  0.412403  0.335823  0.262987  1.000000   \namb5_3    0.429332  0.619087  0.194078  0.261307  0.422743  0.379768   \nid        0.000966 -0.003579 -0.017013  0.051102  0.003512  0.003349   \n\n            amb5_3        id  \ngender    0.069091  0.006340  \nidg      -0.145645 -0.036693  \ncondtn    0.123314 -0.044675  \nwave      0.047841 -0.001863  \nround     0.092951 -0.029700  \nposition  0.058825 -0.026828  \npositin1  0.030543 -0.011816  \norder     0.029174 -0.016321  \npartner   0.050511  0.006595  \npid       0.043115 -0.002027  \nmatch    -0.019651 -0.024600  \nint_corr  0.091037 -0.004452  \nsamerace -0.032939 -0.008852  \nage_o    -0.008256 -0.013694  \nrace_o    0.000540  0.016192  \npf_o_att -0.057774  0.008319  \npf_o_sin  0.019432 -0.014310  \npf_o_int  0.020827 -0.001463  \npf_o_fun  0.030916 -0.003647  \npf_o_amb  0.010931 -0.008735  \npf_o_sha  0.037396  0.009074  \nattr_o   -0.046281 -0.005112  \nsinc_o   -0.081850  0.004987  \nintel_o  -0.018104  0.006735  \nfun_o     0.008543  0.009500  \namb_o     0.030233 -0.000981  \nshar_o    0.012119  0.001961  \nlike_o   -0.022044 -0.001085  \nprob_o   -0.035624  0.001815  \nmet_o    -0.026634 -0.024330  \nage       0.051164 -0.012097  \nfield_cd -0.060974  0.003461  \nrace      0.014533 -0.002041  \nimprace   0.216074  0.018592  \nimprelig  0.164523 -0.012569  \ngoal     -0.064361 -0.004055  \ndate     -0.004260  0.005109  \ngo_out   -0.086735 -0.008325  \ncareer_c  0.022420  0.001839  \nsports    0.048606 -0.013800  \ntvsports -0.101547  0.009894  \nexercise  0.036933  0.008432  \ndining    0.053462 -0.005914  \nmuseums   0.108616 -0.024631  \nart       0.136384 -0.021340  \nhiking   -0.106782  0.005695  \ngaming   -0.090284  0.016828  \nclubbing -0.019706 -0.016390  \nreading  -0.015754 -0.014140  \ntv        0.005325  0.006957  \ntheater   0.143413 -0.014190  \nmovies    0.105382 -0.005111  \nconcerts -0.074486 -0.012210  \nmusic    -0.085466 -0.024923  \nshopping  0.098574 -0.011323  \nyoga     -0.009995  0.000725  \nexphappy  0.135095 -0.002183  \nexpnum         NaN  0.026029  \nattr1_1   0.083864  0.004569  \nsinc1_1  -0.167470  0.003760  \nintel1_1  0.134709 -0.005910  \nfun1_1   -0.031505  0.011890  \namb1_1    0.032544 -0.002390  \nshar1_1  -0.065422 -0.009196  \nattr4_1   0.051275 -0.005220  \nsinc4_1  -0.009661  0.020575  \nintel4_1  0.033842  0.012566  \nfun4_1    0.088294  0.002328  \namb4_1   -0.130667 -0.018565  \nshar4_1  -0.052342  0.018717  \nattr2_1   0.008574 -0.007076  \nsinc2_1   0.022905  0.002436  \nintel2_1  0.128769 -0.000717  \nfun2_1   -0.022542  0.017907  \namb2_1   -0.057602 -0.000935  \nshar2_1  -0.086441  0.000652  \nattr3_1   0.065573  0.001370  \nsinc3_1   0.043192 -0.021861  \nfun3_1    0.206823 -0.012435  \nintel3_1  0.157084 -0.020278  \namb3_1    0.465309 -0.005674  \nattr5_1   0.154660 -0.012665  \nsinc5_1   0.046480 -0.010633  \nintel5_1  0.234111 -0.026115  \nfun5_1    0.172046 -0.018259  \namb5_1    0.537121 -0.002704  \nattr     -0.030551  0.004724  \nsinc      0.012667 -0.005235  \nintel    -0.027882 -0.021023  \nfun      -0.018573  0.006732  \namb      -0.000232  0.006436  \nshar      0.044264 -0.002772  \nlike     -0.028307 -0.005198  \nprob     -0.069938 -0.007546  \nmet       0.055107  0.003800  \nmatch_es  0.080877 -0.010435  \nattr1_s  -0.017623  0.005297  \nsinc1_s  -0.136601 -0.006537  \nintel1_s  0.102055 -0.006084  \nfun1_s    0.096638 -0.040421  \namb1_s    0.091539 -0.005805  \nshar1_s  -0.036773  0.001530  \nattr3_s   0.166821 -0.026020  \nsinc3_s   0.264584 -0.030234  \nintel3_s  0.351795 -0.033225  \nfun3_s    0.176674 -0.043875  \namb3_s    0.397169 -0.017615  \nsatis_2   0.058399 -0.010571  \nlength    0.016839  0.024098  \nnumdat_2  0.178603 -0.003891  \nattr7_2  -0.001049 -0.031765  \nsinc7_2  -0.113881  0.016765  \nintel7_2  0.186837 -0.013544  \nfun7_2   -0.036297  0.001578  \namb7_2    0.188590 -0.003489  \nshar7_2  -0.071867  0.049266  \nattr1_2   0.050290 -0.000348  \nsinc1_2  -0.165929  0.000659  \nintel1_2  0.132209  0.005489  \nfun1_2    0.122049 -0.004338  \namb1_2    0.097966 -0.003638  \nshar1_2  -0.172710  0.006281  \nattr4_2   0.113210 -0.008282  \nsinc4_2  -0.096285  0.027833  \nintel4_2 -0.025929  0.025330  \nfun4_2   -0.057922  0.014238  \namb4_2   -0.074417 -0.005088  \nshar4_2  -0.016677  0.000286  \nattr2_2  -0.012195  0.008762  \nsinc2_2  -0.036465 -0.003652  \nintel2_2  0.230910 -0.022810  \nfun2_2   -0.109821  0.006237  \namb2_2    0.032362 -0.007276  \nshar2_2  -0.061706  0.003793  \nattr3_2   0.145390 -0.005457  \nsinc3_2   0.150855 -0.015575  \nintel3_2  0.249855  0.006043  \nfun3_2    0.235961 -0.016433  \namb3_2    0.413743 -0.004165  \nattr5_2   0.068421 -0.018410  \nsinc5_2   0.058967  0.004089  \nintel5_2  0.252123  0.002299  \nfun5_2    0.242940 -0.012951  \namb5_2    0.567598  0.003009  \nyou_call  0.177991  0.004126  \nthem_cal -0.089334 -0.013923  \ndate_3    0.034839 -0.045999  \nnumdat_3 -0.059301  0.002815  \nnum_in_3 -0.133550 -0.000119  \nattr1_3   0.070902  0.001324  \nsinc1_3  -0.132296 -0.013299  \nintel1_3 -0.057076  0.011455  \nfun1_3    0.087981  0.000125  \namb1_3    0.074328  0.004359  \nshar1_3  -0.083669 -0.007135  \nattr7_3   0.115004 -0.023678  \nsinc7_3  -0.161255 -0.014658  \nintel7_3  0.027736 -0.006156  \nfun7_3   -0.059321  0.035256  \namb7_3   -0.012873  0.011289  \nshar7_3  -0.083565  0.008386  \nattr4_3   0.081528 -0.020716  \nsinc4_3  -0.024962 -0.005087  \nintel4_3  0.011712 -0.002979  \nfun4_3    0.006093  0.033108  \namb4_3   -0.077136 -0.021670  \nshar4_3  -0.057441  0.003276  \nattr2_3   0.021694 -0.014799  \nsinc2_3  -0.121687  0.004638  \nintel2_3  0.131485 -0.011456  \nfun2_3   -0.002714  0.006694  \namb2_3    0.080480 -0.007382  \nshar2_3  -0.136852  0.022419  \nattr3_3   0.194495 -0.005782  \nsinc3_3   0.135499  0.020346  \nintel3_3  0.353093  0.004193  \nfun3_3    0.429332  0.000966  \namb3_3    0.619087 -0.003579  \nattr5_3   0.194078 -0.017013  \nsinc5_3   0.261307  0.051102  \nintel5_3  0.422743  0.003512  \nfun5_3    0.379768  0.003349  \namb5_3    1.000000 -0.007695  \nid       -0.007695  1.000000  \n\n[184 rows x 184 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>idg</th>\n      <th>condtn</th>\n      <th>wave</th>\n      <th>round</th>\n      <th>position</th>\n      <th>positin1</th>\n      <th>order</th>\n      <th>partner</th>\n      <th>pid</th>\n      <th>...</th>\n      <th>sinc3_3</th>\n      <th>intel3_3</th>\n      <th>fun3_3</th>\n      <th>amb3_3</th>\n      <th>attr5_3</th>\n      <th>sinc5_3</th>\n      <th>intel5_3</th>\n      <th>fun5_3</th>\n      <th>amb5_3</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>gender</th>\n      <td>1.000000</td>\n      <td>0.032132</td>\n      <td>-0.000875</td>\n      <td>-0.004192</td>\n      <td>0.017755</td>\n      <td>-0.004047</td>\n      <td>0.000410</td>\n      <td>0.009850</td>\n      <td>0.010318</td>\n      <td>-0.056275</td>\n      <td>...</td>\n      <td>-0.169387</td>\n      <td>0.011476</td>\n      <td>-0.153701</td>\n      <td>-0.066626</td>\n      <td>-0.133302</td>\n      <td>-0.277085</td>\n      <td>0.080227</td>\n      <td>-0.065562</td>\n      <td>0.069091</td>\n      <td>0.006340</td>\n    </tr>\n    <tr>\n      <th>idg</th>\n      <td>0.032132</td>\n      <td>1.000000</td>\n      <td>0.330587</td>\n      <td>0.093823</td>\n      <td>0.391918</td>\n      <td>0.164705</td>\n      <td>0.174651</td>\n      <td>0.161976</td>\n      <td>0.139034</td>\n      <td>0.088372</td>\n      <td>...</td>\n      <td>-0.050350</td>\n      <td>-0.060674</td>\n      <td>-0.041080</td>\n      <td>-0.005502</td>\n      <td>0.001764</td>\n      <td>-0.018915</td>\n      <td>-0.093206</td>\n      <td>-0.061079</td>\n      <td>-0.145645</td>\n      <td>-0.036693</td>\n    </tr>\n    <tr>\n      <th>condtn</th>\n      <td>-0.000875</td>\n      <td>0.330587</td>\n      <td>1.000000</td>\n      <td>0.219735</td>\n      <td>0.820898</td>\n      <td>0.331013</td>\n      <td>0.306722</td>\n      <td>0.331402</td>\n      <td>0.322467</td>\n      <td>0.218009</td>\n      <td>...</td>\n      <td>0.044158</td>\n      <td>0.061796</td>\n      <td>0.069298</td>\n      <td>0.027447</td>\n      <td>0.077571</td>\n      <td>-0.087502</td>\n      <td>0.051449</td>\n      <td>0.106486</td>\n      <td>0.123314</td>\n      <td>-0.044675</td>\n    </tr>\n    <tr>\n      <th>wave</th>\n      <td>-0.004192</td>\n      <td>0.093823</td>\n      <td>0.219735</td>\n      <td>1.000000</td>\n      <td>0.228917</td>\n      <td>0.079820</td>\n      <td>0.061166</td>\n      <td>0.093478</td>\n      <td>0.087904</td>\n      <td>0.996714</td>\n      <td>...</td>\n      <td>0.036948</td>\n      <td>-0.104562</td>\n      <td>0.053971</td>\n      <td>0.000520</td>\n      <td>0.092556</td>\n      <td>-0.001751</td>\n      <td>0.014611</td>\n      <td>0.088716</td>\n      <td>0.047841</td>\n      <td>-0.001863</td>\n    </tr>\n    <tr>\n      <th>round</th>\n      <td>0.017755</td>\n      <td>0.391918</td>\n      <td>0.820898</td>\n      <td>0.228917</td>\n      <td>1.000000</td>\n      <td>0.380632</td>\n      <td>0.368352</td>\n      <td>0.397952</td>\n      <td>0.390320</td>\n      <td>0.218901</td>\n      <td>...</td>\n      <td>0.036600</td>\n      <td>0.022696</td>\n      <td>0.059530</td>\n      <td>0.013107</td>\n      <td>0.034647</td>\n      <td>-0.035242</td>\n      <td>-0.012896</td>\n      <td>0.104829</td>\n      <td>0.092951</td>\n      <td>-0.029700</td>\n    </tr>\n    <tr>\n      <th>position</th>\n      <td>-0.004047</td>\n      <td>0.164705</td>\n      <td>0.331013</td>\n      <td>0.079820</td>\n      <td>0.380632</td>\n      <td>1.000000</td>\n      <td>0.725619</td>\n      <td>0.156510</td>\n      <td>0.164090</td>\n      <td>0.076666</td>\n      <td>...</td>\n      <td>0.041865</td>\n      <td>0.066377</td>\n      <td>0.043142</td>\n      <td>0.054227</td>\n      <td>0.066300</td>\n      <td>0.111991</td>\n      <td>0.089283</td>\n      <td>0.135484</td>\n      <td>0.058825</td>\n      <td>-0.026828</td>\n    </tr>\n    <tr>\n      <th>positin1</th>\n      <td>0.000410</td>\n      <td>0.174651</td>\n      <td>0.306722</td>\n      <td>0.061166</td>\n      <td>0.368352</td>\n      <td>0.725619</td>\n      <td>1.000000</td>\n      <td>0.154697</td>\n      <td>0.152826</td>\n      <td>0.057726</td>\n      <td>...</td>\n      <td>-0.031083</td>\n      <td>0.040163</td>\n      <td>0.073800</td>\n      <td>-0.024003</td>\n      <td>0.201598</td>\n      <td>0.125594</td>\n      <td>0.106791</td>\n      <td>0.232412</td>\n      <td>0.030543</td>\n      <td>-0.011816</td>\n    </tr>\n    <tr>\n      <th>order</th>\n      <td>0.009850</td>\n      <td>0.161976</td>\n      <td>0.331402</td>\n      <td>0.093478</td>\n      <td>0.397952</td>\n      <td>0.156510</td>\n      <td>0.154697</td>\n      <td>1.000000</td>\n      <td>0.158508</td>\n      <td>0.089130</td>\n      <td>...</td>\n      <td>0.000877</td>\n      <td>0.002350</td>\n      <td>0.006350</td>\n      <td>0.002645</td>\n      <td>0.018610</td>\n      <td>-0.035897</td>\n      <td>-0.004632</td>\n      <td>0.016419</td>\n      <td>0.029174</td>\n      <td>-0.016321</td>\n    </tr>\n    <tr>\n      <th>partner</th>\n      <td>0.010318</td>\n      <td>0.139034</td>\n      <td>0.322467</td>\n      <td>0.087904</td>\n      <td>0.390320</td>\n      <td>0.164090</td>\n      <td>0.152826</td>\n      <td>0.158508</td>\n      <td>1.000000</td>\n      <td>0.112771</td>\n      <td>...</td>\n      <td>0.016141</td>\n      <td>0.012612</td>\n      <td>0.021345</td>\n      <td>-0.000157</td>\n      <td>0.002868</td>\n      <td>-0.002601</td>\n      <td>0.003345</td>\n      <td>0.031795</td>\n      <td>0.050511</td>\n      <td>0.006595</td>\n    </tr>\n    <tr>\n      <th>pid</th>\n      <td>-0.056275</td>\n      <td>0.088372</td>\n      <td>0.218009</td>\n      <td>0.996714</td>\n      <td>0.218901</td>\n      <td>0.076666</td>\n      <td>0.057726</td>\n      <td>0.089130</td>\n      <td>0.112771</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.040356</td>\n      <td>-0.113803</td>\n      <td>0.056667</td>\n      <td>-0.001845</td>\n      <td>0.108400</td>\n      <td>0.021372</td>\n      <td>0.009585</td>\n      <td>0.096788</td>\n      <td>0.043115</td>\n      <td>-0.002027</td>\n    </tr>\n    <tr>\n      <th>match</th>\n      <td>0.004293</td>\n      <td>0.006630</td>\n      <td>-0.042888</td>\n      <td>-0.025673</td>\n      <td>-0.031313</td>\n      <td>-0.010135</td>\n      <td>-0.012012</td>\n      <td>-0.046686</td>\n      <td>0.009451</td>\n      <td>-0.024160</td>\n      <td>...</td>\n      <td>-0.013565</td>\n      <td>0.019756</td>\n      <td>0.024759</td>\n      <td>0.014811</td>\n      <td>0.042603</td>\n      <td>-0.040531</td>\n      <td>-0.011494</td>\n      <td>0.015977</td>\n      <td>-0.019651</td>\n      <td>-0.024600</td>\n    </tr>\n    <tr>\n      <th>int_corr</th>\n      <td>0.011520</td>\n      <td>-0.019557</td>\n      <td>0.043470</td>\n      <td>0.043207</td>\n      <td>0.017517</td>\n      <td>-0.014928</td>\n      <td>-0.035027</td>\n      <td>0.015889</td>\n      <td>-0.018501</td>\n      <td>0.040282</td>\n      <td>...</td>\n      <td>-0.059689</td>\n      <td>-0.028056</td>\n      <td>-0.052016</td>\n      <td>-0.031000</td>\n      <td>-0.014248</td>\n      <td>-0.130741</td>\n      <td>0.058994</td>\n      <td>-0.082505</td>\n      <td>0.091037</td>\n      <td>-0.004452</td>\n    </tr>\n    <tr>\n      <th>samerace</th>\n      <td>-0.017636</td>\n      <td>-0.014186</td>\n      <td>0.066645</td>\n      <td>-0.017883</td>\n      <td>0.035692</td>\n      <td>0.026598</td>\n      <td>0.044748</td>\n      <td>0.004305</td>\n      <td>-0.010329</td>\n      <td>-0.013668</td>\n      <td>...</td>\n      <td>0.014779</td>\n      <td>0.080131</td>\n      <td>0.033090</td>\n      <td>-0.013831</td>\n      <td>0.071866</td>\n      <td>0.004318</td>\n      <td>0.065095</td>\n      <td>0.032052</td>\n      <td>-0.032939</td>\n      <td>-0.008852</td>\n    </tr>\n    <tr>\n      <th>age_o</th>\n      <td>-0.088138</td>\n      <td>0.001640</td>\n      <td>0.103362</td>\n      <td>0.101982</td>\n      <td>0.013366</td>\n      <td>-0.003581</td>\n      <td>-0.080785</td>\n      <td>-0.001549</td>\n      <td>0.025385</td>\n      <td>0.108678</td>\n      <td>...</td>\n      <td>0.061237</td>\n      <td>0.039288</td>\n      <td>0.012037</td>\n      <td>0.020727</td>\n      <td>0.007459</td>\n      <td>0.041266</td>\n      <td>0.020879</td>\n      <td>-0.038257</td>\n      <td>-0.008256</td>\n      <td>-0.013694</td>\n    </tr>\n    <tr>\n      <th>race_o</th>\n      <td>0.027244</td>\n      <td>-0.026071</td>\n      <td>-0.091912</td>\n      <td>0.067121</td>\n      <td>-0.062549</td>\n      <td>-0.092831</td>\n      <td>-0.069159</td>\n      <td>-0.017122</td>\n      <td>-0.039588</td>\n      <td>0.060003</td>\n      <td>...</td>\n      <td>-0.000589</td>\n      <td>-0.019296</td>\n      <td>0.009734</td>\n      <td>0.019909</td>\n      <td>-0.022354</td>\n      <td>0.025867</td>\n      <td>-0.001924</td>\n      <td>0.025706</td>\n      <td>0.000540</td>\n      <td>0.016192</td>\n    </tr>\n    <tr>\n      <th>pf_o_att</th>\n      <td>-0.358661</td>\n      <td>-0.022933</td>\n      <td>-0.059716</td>\n      <td>0.064234</td>\n      <td>-0.022082</td>\n      <td>0.021470</td>\n      <td>0.028941</td>\n      <td>-0.001589</td>\n      <td>0.051090</td>\n      <td>0.093871</td>\n      <td>...</td>\n      <td>0.006564</td>\n      <td>-0.123352</td>\n      <td>-0.005803</td>\n      <td>-0.073788</td>\n      <td>0.060730</td>\n      <td>0.121022</td>\n      <td>-0.044838</td>\n      <td>0.034232</td>\n      <td>-0.057774</td>\n      <td>0.008319</td>\n    </tr>\n    <tr>\n      <th>pf_o_sin</th>\n      <td>0.137767</td>\n      <td>0.018207</td>\n      <td>0.056617</td>\n      <td>0.000619</td>\n      <td>0.034419</td>\n      <td>-0.046014</td>\n      <td>-0.075246</td>\n      <td>0.008352</td>\n      <td>-0.064742</td>\n      <td>-0.012650</td>\n      <td>...</td>\n      <td>-0.024739</td>\n      <td>0.024120</td>\n      <td>0.007306</td>\n      <td>0.021869</td>\n      <td>-0.034533</td>\n      <td>-0.078758</td>\n      <td>0.010946</td>\n      <td>-0.003366</td>\n      <td>0.019432</td>\n      <td>-0.014310</td>\n    </tr>\n    <tr>\n      <th>pf_o_int</th>\n      <td>0.119492</td>\n      <td>0.021344</td>\n      <td>0.039571</td>\n      <td>-0.059921</td>\n      <td>0.033372</td>\n      <td>0.075163</td>\n      <td>0.051622</td>\n      <td>0.001846</td>\n      <td>0.015007</td>\n      <td>-0.064292</td>\n      <td>...</td>\n      <td>-0.033627</td>\n      <td>-0.019020</td>\n      <td>-0.039907</td>\n      <td>-0.023281</td>\n      <td>-0.007365</td>\n      <td>-0.030232</td>\n      <td>0.027977</td>\n      <td>0.001947</td>\n      <td>0.020827</td>\n      <td>-0.001463</td>\n    </tr>\n    <tr>\n      <th>pf_o_fun</th>\n      <td>-0.054472</td>\n      <td>0.001556</td>\n      <td>-0.010274</td>\n      <td>0.020695</td>\n      <td>-0.016803</td>\n      <td>-0.058370</td>\n      <td>0.012950</td>\n      <td>-0.003176</td>\n      <td>-0.009921</td>\n      <td>0.022132</td>\n      <td>...</td>\n      <td>0.020859</td>\n      <td>-0.010636</td>\n      <td>0.010691</td>\n      <td>0.011182</td>\n      <td>-0.011298</td>\n      <td>0.039741</td>\n      <td>-0.014393</td>\n      <td>0.005304</td>\n      <td>0.030916</td>\n      <td>-0.003647</td>\n    </tr>\n    <tr>\n      <th>pf_o_amb</th>\n      <td>0.349116</td>\n      <td>-0.019420</td>\n      <td>-0.040077</td>\n      <td>-0.121859</td>\n      <td>-0.059631</td>\n      <td>-0.006482</td>\n      <td>-0.026111</td>\n      <td>-0.020790</td>\n      <td>-0.067648</td>\n      <td>-0.149614</td>\n      <td>...</td>\n      <td>0.001719</td>\n      <td>0.147931</td>\n      <td>0.002992</td>\n      <td>0.060453</td>\n      <td>-0.083804</td>\n      <td>-0.120149</td>\n      <td>0.047995</td>\n      <td>-0.061783</td>\n      <td>0.010931</td>\n      <td>-0.008735</td>\n    </tr>\n    <tr>\n      <th>pf_o_sha</th>\n      <td>0.131327</td>\n      <td>0.005954</td>\n      <td>0.012578</td>\n      <td>-0.003644</td>\n      <td>0.005248</td>\n      <td>-0.017178</td>\n      <td>-0.018036</td>\n      <td>0.002012</td>\n      <td>0.008362</td>\n      <td>-0.016944</td>\n      <td>...</td>\n      <td>0.029942</td>\n      <td>0.111504</td>\n      <td>0.039563</td>\n      <td>0.085489</td>\n      <td>0.002692</td>\n      <td>-0.060377</td>\n      <td>0.026938</td>\n      <td>-0.024538</td>\n      <td>0.037396</td>\n      <td>0.009074</td>\n    </tr>\n    <tr>\n      <th>attr_o</th>\n      <td>-0.131695</td>\n      <td>0.025992</td>\n      <td>0.000433</td>\n      <td>-0.027403</td>\n      <td>-0.048432</td>\n      <td>-0.011114</td>\n      <td>-0.027070</td>\n      <td>-0.022957</td>\n      <td>-0.043246</td>\n      <td>-0.019339</td>\n      <td>...</td>\n      <td>-0.042072</td>\n      <td>0.028249</td>\n      <td>0.127471</td>\n      <td>0.072266</td>\n      <td>0.229829</td>\n      <td>-0.044499</td>\n      <td>-0.024843</td>\n      <td>0.131459</td>\n      <td>-0.046281</td>\n      <td>-0.005112</td>\n    </tr>\n    <tr>\n      <th>sinc_o</th>\n      <td>-0.035660</td>\n      <td>-0.014766</td>\n      <td>-0.039144</td>\n      <td>-0.049451</td>\n      <td>-0.087160</td>\n      <td>-0.033502</td>\n      <td>-0.026049</td>\n      <td>-0.124237</td>\n      <td>-0.037962</td>\n      <td>-0.043689</td>\n      <td>...</td>\n      <td>0.005042</td>\n      <td>-0.034780</td>\n      <td>-0.009639</td>\n      <td>-0.050024</td>\n      <td>-0.056311</td>\n      <td>-0.006763</td>\n      <td>-0.082100</td>\n      <td>-0.033609</td>\n      <td>-0.081850</td>\n      <td>0.004987</td>\n    </tr>\n    <tr>\n      <th>intel_o</th>\n      <td>0.056205</td>\n      <td>-0.025672</td>\n      <td>-0.019193</td>\n      <td>-0.042894</td>\n      <td>-0.073468</td>\n      <td>-0.030418</td>\n      <td>-0.048187</td>\n      <td>-0.091123</td>\n      <td>-0.047603</td>\n      <td>-0.045093</td>\n      <td>...</td>\n      <td>0.003046</td>\n      <td>0.038570</td>\n      <td>-0.028981</td>\n      <td>-0.025375</td>\n      <td>-0.029575</td>\n      <td>-0.029827</td>\n      <td>0.013227</td>\n      <td>-0.058197</td>\n      <td>-0.018104</td>\n      <td>0.006735</td>\n    </tr>\n    <tr>\n      <th>fun_o</th>\n      <td>-0.058486</td>\n      <td>0.015192</td>\n      <td>-0.031480</td>\n      <td>-0.008809</td>\n      <td>-0.073642</td>\n      <td>-0.003423</td>\n      <td>-0.032715</td>\n      <td>-0.058559</td>\n      <td>-0.050419</td>\n      <td>-0.004145</td>\n      <td>...</td>\n      <td>-0.021349</td>\n      <td>0.047319</td>\n      <td>0.156654</td>\n      <td>0.060780</td>\n      <td>0.114706</td>\n      <td>0.000202</td>\n      <td>-0.022603</td>\n      <td>0.153961</td>\n      <td>0.008543</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <th>amb_o</th>\n      <td>0.094500</td>\n      <td>-0.003001</td>\n      <td>-0.022295</td>\n      <td>-0.030943</td>\n      <td>-0.074191</td>\n      <td>-0.010865</td>\n      <td>-0.026521</td>\n      <td>-0.079342</td>\n      <td>-0.051124</td>\n      <td>-0.034429</td>\n      <td>...</td>\n      <td>0.005720</td>\n      <td>0.080934</td>\n      <td>0.039626</td>\n      <td>0.059628</td>\n      <td>0.034623</td>\n      <td>-0.052729</td>\n      <td>0.022583</td>\n      <td>0.019996</td>\n      <td>0.030233</td>\n      <td>-0.000981</td>\n    </tr>\n    <tr>\n      <th>shar_o</th>\n      <td>-0.027496</td>\n      <td>-0.001070</td>\n      <td>-0.028479</td>\n      <td>-0.051531</td>\n      <td>-0.055836</td>\n      <td>-0.025627</td>\n      <td>-0.045861</td>\n      <td>-0.001224</td>\n      <td>-0.025884</td>\n      <td>-0.048017</td>\n      <td>...</td>\n      <td>-0.027548</td>\n      <td>0.065353</td>\n      <td>0.074508</td>\n      <td>0.042933</td>\n      <td>0.128149</td>\n      <td>-0.042126</td>\n      <td>0.006964</td>\n      <td>0.075896</td>\n      <td>0.012119</td>\n      <td>0.001961</td>\n    </tr>\n    <tr>\n      <th>like_o</th>\n      <td>-0.072048</td>\n      <td>0.000713</td>\n      <td>-0.035615</td>\n      <td>-0.046025</td>\n      <td>-0.072608</td>\n      <td>-0.027920</td>\n      <td>-0.060944</td>\n      <td>-0.061194</td>\n      <td>-0.044188</td>\n      <td>-0.040668</td>\n      <td>...</td>\n      <td>-0.029362</td>\n      <td>0.027297</td>\n      <td>0.092382</td>\n      <td>0.027875</td>\n      <td>0.113651</td>\n      <td>-0.027713</td>\n      <td>-0.037313</td>\n      <td>0.087366</td>\n      <td>-0.022044</td>\n      <td>-0.001085</td>\n    </tr>\n    <tr>\n      <th>prob_o</th>\n      <td>0.004045</td>\n      <td>-0.001639</td>\n      <td>-0.037464</td>\n      <td>0.017731</td>\n      <td>-0.073096</td>\n      <td>-0.047533</td>\n      <td>-0.032497</td>\n      <td>-0.096395</td>\n      <td>-0.004352</td>\n      <td>0.020179</td>\n      <td>...</td>\n      <td>-0.007677</td>\n      <td>0.030686</td>\n      <td>0.043200</td>\n      <td>0.014001</td>\n      <td>0.013743</td>\n      <td>-0.017628</td>\n      <td>-0.032689</td>\n      <td>0.028500</td>\n      <td>-0.035624</td>\n      <td>0.001815</td>\n    </tr>\n    <tr>\n      <th>met_o</th>\n      <td>0.008274</td>\n      <td>0.031788</td>\n      <td>0.020551</td>\n      <td>0.049918</td>\n      <td>0.022617</td>\n      <td>0.037015</td>\n      <td>0.018081</td>\n      <td>-0.001150</td>\n      <td>0.002497</td>\n      <td>0.050574</td>\n      <td>...</td>\n      <td>0.005969</td>\n      <td>-0.021632</td>\n      <td>0.001133</td>\n      <td>-0.034778</td>\n      <td>0.028479</td>\n      <td>0.058930</td>\n      <td>-0.006701</td>\n      <td>0.063289</td>\n      <td>-0.026634</td>\n      <td>-0.024330</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>0.069427</td>\n      <td>0.027894</td>\n      <td>0.091264</td>\n      <td>0.090236</td>\n      <td>0.004127</td>\n      <td>0.001442</td>\n      <td>-0.034133</td>\n      <td>0.000881</td>\n      <td>0.000008</td>\n      <td>0.085023</td>\n      <td>...</td>\n      <td>-0.023295</td>\n      <td>-0.001104</td>\n      <td>-0.091368</td>\n      <td>-0.134249</td>\n      <td>-0.096014</td>\n      <td>-0.020274</td>\n      <td>0.087549</td>\n      <td>-0.076022</td>\n      <td>0.051164</td>\n      <td>-0.012097</td>\n    </tr>\n    <tr>\n      <th>field_cd</th>\n      <td>-0.069071</td>\n      <td>0.069341</td>\n      <td>0.118383</td>\n      <td>0.123313</td>\n      <td>0.077153</td>\n      <td>0.010745</td>\n      <td>-0.004896</td>\n      <td>0.035063</td>\n      <td>0.041827</td>\n      <td>0.121785</td>\n      <td>...</td>\n      <td>-0.060940</td>\n      <td>-0.067377</td>\n      <td>-0.022162</td>\n      <td>-0.016643</td>\n      <td>-0.089767</td>\n      <td>-0.184357</td>\n      <td>-0.007130</td>\n      <td>-0.232345</td>\n      <td>-0.060974</td>\n      <td>0.003461</td>\n    </tr>\n    <tr>\n      <th>race</th>\n      <td>-0.011617</td>\n      <td>-0.038091</td>\n      <td>-0.105011</td>\n      <td>0.077669</td>\n      <td>-0.083882</td>\n      <td>-0.086422</td>\n      <td>-0.111482</td>\n      <td>-0.042934</td>\n      <td>-0.032099</td>\n      <td>0.075196</td>\n      <td>...</td>\n      <td>0.015723</td>\n      <td>-0.118720</td>\n      <td>-0.060218</td>\n      <td>-0.022075</td>\n      <td>-0.180510</td>\n      <td>0.036745</td>\n      <td>-0.108570</td>\n      <td>-0.190634</td>\n      <td>0.014533</td>\n      <td>-0.002041</td>\n    </tr>\n    <tr>\n      <th>imprace</th>\n      <td>-0.113948</td>\n      <td>0.016161</td>\n      <td>0.044369</td>\n      <td>-0.030351</td>\n      <td>0.078953</td>\n      <td>0.066652</td>\n      <td>0.130577</td>\n      <td>0.027145</td>\n      <td>0.022614</td>\n      <td>-0.027255</td>\n      <td>...</td>\n      <td>0.064441</td>\n      <td>0.135849</td>\n      <td>0.168694</td>\n      <td>0.199266</td>\n      <td>0.198056</td>\n      <td>0.221680</td>\n      <td>0.122016</td>\n      <td>0.273409</td>\n      <td>0.216074</td>\n      <td>0.018592</td>\n    </tr>\n    <tr>\n      <th>imprelig</th>\n      <td>-0.208852</td>\n      <td>0.056349</td>\n      <td>0.083515</td>\n      <td>-0.054266</td>\n      <td>0.066776</td>\n      <td>0.023342</td>\n      <td>0.053534</td>\n      <td>0.017375</td>\n      <td>0.019059</td>\n      <td>-0.040871</td>\n      <td>...</td>\n      <td>0.141049</td>\n      <td>0.184416</td>\n      <td>0.231129</td>\n      <td>0.126834</td>\n      <td>0.220660</td>\n      <td>0.314702</td>\n      <td>0.307723</td>\n      <td>0.259477</td>\n      <td>0.164523</td>\n      <td>-0.012569</td>\n    </tr>\n    <tr>\n      <th>goal</th>\n      <td>0.028397</td>\n      <td>-0.043024</td>\n      <td>-0.065811</td>\n      <td>-0.053324</td>\n      <td>-0.076306</td>\n      <td>-0.030210</td>\n      <td>0.003221</td>\n      <td>-0.023085</td>\n      <td>-0.040481</td>\n      <td>-0.049881</td>\n      <td>...</td>\n      <td>0.069918</td>\n      <td>0.027805</td>\n      <td>-0.102437</td>\n      <td>-0.091611</td>\n      <td>0.046964</td>\n      <td>0.156584</td>\n      <td>0.146097</td>\n      <td>-0.020968</td>\n      <td>-0.064361</td>\n      <td>-0.004055</td>\n    </tr>\n    <tr>\n      <th>date</th>\n      <td>-0.099738</td>\n      <td>-0.025829</td>\n      <td>0.059261</td>\n      <td>-0.000005</td>\n      <td>0.070118</td>\n      <td>0.037104</td>\n      <td>0.003945</td>\n      <td>0.024128</td>\n      <td>0.028116</td>\n      <td>0.003028</td>\n      <td>...</td>\n      <td>0.018388</td>\n      <td>-0.121439</td>\n      <td>-0.272808</td>\n      <td>-0.188325</td>\n      <td>-0.264036</td>\n      <td>0.033016</td>\n      <td>-0.138150</td>\n      <td>-0.202900</td>\n      <td>-0.004260</td>\n      <td>0.005109</td>\n    </tr>\n    <tr>\n      <th>go_out</th>\n      <td>0.023241</td>\n      <td>-0.086469</td>\n      <td>0.044050</td>\n      <td>0.024167</td>\n      <td>0.073584</td>\n      <td>0.055182</td>\n      <td>0.077921</td>\n      <td>0.024387</td>\n      <td>0.019726</td>\n      <td>0.019214</td>\n      <td>...</td>\n      <td>-0.001070</td>\n      <td>-0.161524</td>\n      <td>-0.336673</td>\n      <td>-0.225101</td>\n      <td>-0.375717</td>\n      <td>-0.002031</td>\n      <td>-0.149558</td>\n      <td>-0.265636</td>\n      <td>-0.086735</td>\n      <td>-0.008325</td>\n    </tr>\n    <tr>\n      <th>career_c</th>\n      <td>-0.007684</td>\n      <td>0.165261</td>\n      <td>0.015837</td>\n      <td>0.144097</td>\n      <td>0.030779</td>\n      <td>0.048137</td>\n      <td>0.080300</td>\n      <td>0.006180</td>\n      <td>0.002895</td>\n      <td>0.146303</td>\n      <td>...</td>\n      <td>-0.034842</td>\n      <td>-0.124857</td>\n      <td>-0.036115</td>\n      <td>0.034519</td>\n      <td>-0.014018</td>\n      <td>-0.020888</td>\n      <td>0.105055</td>\n      <td>-0.071122</td>\n      <td>0.022420</td>\n      <td>0.001839</td>\n    </tr>\n    <tr>\n      <th>sports</th>\n      <td>0.228759</td>\n      <td>0.077981</td>\n      <td>0.036334</td>\n      <td>-0.002509</td>\n      <td>0.023234</td>\n      <td>-0.036944</td>\n      <td>-0.078193</td>\n      <td>0.008790</td>\n      <td>0.001878</td>\n      <td>-0.014312</td>\n      <td>...</td>\n      <td>0.059039</td>\n      <td>0.064729</td>\n      <td>0.139470</td>\n      <td>0.179588</td>\n      <td>0.045596</td>\n      <td>-0.068817</td>\n      <td>0.049949</td>\n      <td>0.114218</td>\n      <td>0.048606</td>\n      <td>-0.013800</td>\n    </tr>\n    <tr>\n      <th>tvsports</th>\n      <td>0.145324</td>\n      <td>0.075633</td>\n      <td>0.013386</td>\n      <td>-0.023255</td>\n      <td>0.033805</td>\n      <td>-0.028955</td>\n      <td>-0.054733</td>\n      <td>0.005201</td>\n      <td>0.002481</td>\n      <td>-0.030770</td>\n      <td>...</td>\n      <td>0.108768</td>\n      <td>0.053405</td>\n      <td>0.097166</td>\n      <td>-0.014492</td>\n      <td>-0.025214</td>\n      <td>0.093377</td>\n      <td>-0.036258</td>\n      <td>0.088232</td>\n      <td>-0.101547</td>\n      <td>0.009894</td>\n    </tr>\n    <tr>\n      <th>exercise</th>\n      <td>-0.077061</td>\n      <td>-0.005694</td>\n      <td>-0.037484</td>\n      <td>-0.015931</td>\n      <td>-0.071055</td>\n      <td>-0.002636</td>\n      <td>-0.089496</td>\n      <td>-0.022582</td>\n      <td>-0.028422</td>\n      <td>-0.010085</td>\n      <td>...</td>\n      <td>0.039707</td>\n      <td>0.033784</td>\n      <td>0.180618</td>\n      <td>0.116326</td>\n      <td>0.135917</td>\n      <td>0.048008</td>\n      <td>-0.101250</td>\n      <td>0.003234</td>\n      <td>0.036933</td>\n      <td>0.008432</td>\n    </tr>\n    <tr>\n      <th>dining</th>\n      <td>-0.208608</td>\n      <td>0.021910</td>\n      <td>0.007183</td>\n      <td>0.105460</td>\n      <td>-0.018757</td>\n      <td>-0.006099</td>\n      <td>-0.073877</td>\n      <td>-0.007313</td>\n      <td>-0.006899</td>\n      <td>0.117794</td>\n      <td>...</td>\n      <td>0.110105</td>\n      <td>0.081312</td>\n      <td>0.177058</td>\n      <td>0.253417</td>\n      <td>0.194289</td>\n      <td>-0.032040</td>\n      <td>0.098567</td>\n      <td>-0.015668</td>\n      <td>0.053462</td>\n      <td>-0.005914</td>\n    </tr>\n    <tr>\n      <th>museums</th>\n      <td>-0.206215</td>\n      <td>-0.079038</td>\n      <td>0.029040</td>\n      <td>0.064333</td>\n      <td>0.017779</td>\n      <td>0.008001</td>\n      <td>-0.017217</td>\n      <td>0.002759</td>\n      <td>0.009895</td>\n      <td>0.071494</td>\n      <td>...</td>\n      <td>0.000583</td>\n      <td>-0.036061</td>\n      <td>-0.002971</td>\n      <td>-0.034990</td>\n      <td>0.045490</td>\n      <td>-0.103755</td>\n      <td>-0.039821</td>\n      <td>-0.082370</td>\n      <td>0.108616</td>\n      <td>-0.024631</td>\n    </tr>\n    <tr>\n      <th>art</th>\n      <td>-0.218061</td>\n      <td>-0.042987</td>\n      <td>0.039190</td>\n      <td>0.110946</td>\n      <td>0.037574</td>\n      <td>-0.010808</td>\n      <td>-0.047413</td>\n      <td>0.012456</td>\n      <td>0.020741</td>\n      <td>0.120467</td>\n      <td>...</td>\n      <td>0.025086</td>\n      <td>-0.051568</td>\n      <td>0.069272</td>\n      <td>-0.020453</td>\n      <td>-0.028968</td>\n      <td>-0.077500</td>\n      <td>-0.024287</td>\n      <td>-0.017419</td>\n      <td>0.136384</td>\n      <td>-0.021340</td>\n    </tr>\n    <tr>\n      <th>hiking</th>\n      <td>-0.065324</td>\n      <td>0.050733</td>\n      <td>0.008914</td>\n      <td>0.012606</td>\n      <td>-0.003676</td>\n      <td>-0.068490</td>\n      <td>-0.065454</td>\n      <td>-0.012845</td>\n      <td>-0.000274</td>\n      <td>0.016052</td>\n      <td>...</td>\n      <td>0.028004</td>\n      <td>-0.076248</td>\n      <td>0.030309</td>\n      <td>-0.024546</td>\n      <td>-0.093525</td>\n      <td>-0.075179</td>\n      <td>-0.094685</td>\n      <td>-0.106205</td>\n      <td>-0.106782</td>\n      <td>0.005695</td>\n    </tr>\n    <tr>\n      <th>gaming</th>\n      <td>0.217357</td>\n      <td>0.083485</td>\n      <td>0.005409</td>\n      <td>0.001732</td>\n      <td>0.075653</td>\n      <td>0.000536</td>\n      <td>-0.019060</td>\n      <td>0.020638</td>\n      <td>0.015907</td>\n      <td>-0.012166</td>\n      <td>...</td>\n      <td>0.065780</td>\n      <td>0.004322</td>\n      <td>-0.016737</td>\n      <td>0.070400</td>\n      <td>-0.153726</td>\n      <td>-0.010430</td>\n      <td>-0.136067</td>\n      <td>-0.103754</td>\n      <td>-0.090284</td>\n      <td>0.016828</td>\n    </tr>\n    <tr>\n      <th>clubbing</th>\n      <td>-0.054334</td>\n      <td>-0.027315</td>\n      <td>-0.054789</td>\n      <td>-0.038468</td>\n      <td>-0.010795</td>\n      <td>0.024627</td>\n      <td>0.047144</td>\n      <td>-0.004245</td>\n      <td>-0.007468</td>\n      <td>-0.040095</td>\n      <td>...</td>\n      <td>0.082506</td>\n      <td>-0.030430</td>\n      <td>0.150512</td>\n      <td>0.140721</td>\n      <td>-0.072702</td>\n      <td>-0.205654</td>\n      <td>-0.186982</td>\n      <td>-0.064888</td>\n      <td>-0.019706</td>\n      <td>-0.016390</td>\n    </tr>\n    <tr>\n      <th>reading</th>\n      <td>-0.105863</td>\n      <td>0.008332</td>\n      <td>0.006538</td>\n      <td>-0.018368</td>\n      <td>0.035405</td>\n      <td>-0.020462</td>\n      <td>0.011974</td>\n      <td>0.014931</td>\n      <td>-0.001645</td>\n      <td>-0.017688</td>\n      <td>...</td>\n      <td>0.096574</td>\n      <td>0.132744</td>\n      <td>-0.084584</td>\n      <td>0.000408</td>\n      <td>0.019147</td>\n      <td>-0.101042</td>\n      <td>0.069386</td>\n      <td>-0.173555</td>\n      <td>-0.015754</td>\n      <td>-0.014140</td>\n    </tr>\n    <tr>\n      <th>tv</th>\n      <td>-0.166900</td>\n      <td>-0.010950</td>\n      <td>-0.007640</td>\n      <td>0.018124</td>\n      <td>-0.013004</td>\n      <td>-0.019809</td>\n      <td>-0.067140</td>\n      <td>-0.008448</td>\n      <td>-0.010343</td>\n      <td>0.026783</td>\n      <td>...</td>\n      <td>0.042670</td>\n      <td>0.048799</td>\n      <td>0.046928</td>\n      <td>0.088496</td>\n      <td>0.064928</td>\n      <td>0.086154</td>\n      <td>-0.067903</td>\n      <td>0.091350</td>\n      <td>0.005325</td>\n      <td>0.006957</td>\n    </tr>\n    <tr>\n      <th>theater</th>\n      <td>-0.305193</td>\n      <td>0.008516</td>\n      <td>0.026644</td>\n      <td>0.032895</td>\n      <td>0.000845</td>\n      <td>-0.003645</td>\n      <td>-0.045004</td>\n      <td>0.003173</td>\n      <td>0.002555</td>\n      <td>0.047319</td>\n      <td>...</td>\n      <td>0.038191</td>\n      <td>-0.121900</td>\n      <td>0.041026</td>\n      <td>-0.055303</td>\n      <td>-0.114084</td>\n      <td>0.022848</td>\n      <td>-0.088945</td>\n      <td>-0.043596</td>\n      <td>0.143413</td>\n      <td>-0.014190</td>\n    </tr>\n    <tr>\n      <th>movies</th>\n      <td>-0.160059</td>\n      <td>-0.039631</td>\n      <td>0.031231</td>\n      <td>-0.009459</td>\n      <td>0.012650</td>\n      <td>0.002444</td>\n      <td>-0.034219</td>\n      <td>-0.002535</td>\n      <td>0.010053</td>\n      <td>-0.000266</td>\n      <td>...</td>\n      <td>0.045916</td>\n      <td>-0.076999</td>\n      <td>-0.023180</td>\n      <td>-0.039844</td>\n      <td>-0.055256</td>\n      <td>0.088386</td>\n      <td>0.029035</td>\n      <td>-0.075798</td>\n      <td>0.105382</td>\n      <td>-0.005111</td>\n    </tr>\n    <tr>\n      <th>concerts</th>\n      <td>-0.132015</td>\n      <td>-0.005953</td>\n      <td>-0.016695</td>\n      <td>0.051820</td>\n      <td>-0.019012</td>\n      <td>-0.023554</td>\n      <td>-0.036230</td>\n      <td>-0.009631</td>\n      <td>-0.017752</td>\n      <td>0.059339</td>\n      <td>...</td>\n      <td>0.020933</td>\n      <td>-0.151309</td>\n      <td>-0.013663</td>\n      <td>-0.075999</td>\n      <td>0.004560</td>\n      <td>0.028306</td>\n      <td>-0.003903</td>\n      <td>-0.037652</td>\n      <td>-0.074486</td>\n      <td>-0.012210</td>\n    </tr>\n    <tr>\n      <th>music</th>\n      <td>-0.077399</td>\n      <td>0.005671</td>\n      <td>-0.026277</td>\n      <td>0.049195</td>\n      <td>-0.033941</td>\n      <td>-0.008580</td>\n      <td>-0.046754</td>\n      <td>-0.018522</td>\n      <td>-0.017026</td>\n      <td>0.052336</td>\n      <td>...</td>\n      <td>0.067068</td>\n      <td>-0.027028</td>\n      <td>0.095359</td>\n      <td>0.024996</td>\n      <td>0.012711</td>\n      <td>0.001373</td>\n      <td>-0.009794</td>\n      <td>-0.042046</td>\n      <td>-0.085466</td>\n      <td>-0.024923</td>\n    </tr>\n    <tr>\n      <th>shopping</th>\n      <td>-0.327006</td>\n      <td>-0.034792</td>\n      <td>0.029079</td>\n      <td>0.071717</td>\n      <td>0.000919</td>\n      <td>0.019994</td>\n      <td>-0.009181</td>\n      <td>0.007340</td>\n      <td>-0.002754</td>\n      <td>0.089531</td>\n      <td>...</td>\n      <td>0.052722</td>\n      <td>0.067411</td>\n      <td>0.293958</td>\n      <td>0.224608</td>\n      <td>0.263795</td>\n      <td>-0.001206</td>\n      <td>-0.117283</td>\n      <td>0.127488</td>\n      <td>0.098574</td>\n      <td>-0.011323</td>\n    </tr>\n    <tr>\n      <th>yoga</th>\n      <td>-0.222094</td>\n      <td>-0.047489</td>\n      <td>-0.036536</td>\n      <td>0.026499</td>\n      <td>-0.071173</td>\n      <td>-0.066239</td>\n      <td>-0.133878</td>\n      <td>-0.035965</td>\n      <td>-0.037529</td>\n      <td>0.037607</td>\n      <td>...</td>\n      <td>0.021349</td>\n      <td>-0.054702</td>\n      <td>0.100792</td>\n      <td>0.030477</td>\n      <td>0.077607</td>\n      <td>-0.055472</td>\n      <td>-0.067602</td>\n      <td>-0.041259</td>\n      <td>-0.009995</td>\n      <td>0.000725</td>\n    </tr>\n    <tr>\n      <th>exphappy</th>\n      <td>0.212598</td>\n      <td>-0.010431</td>\n      <td>0.025067</td>\n      <td>0.059054</td>\n      <td>0.049766</td>\n      <td>-0.015050</td>\n      <td>-0.051197</td>\n      <td>0.006015</td>\n      <td>0.027450</td>\n      <td>0.047259</td>\n      <td>...</td>\n      <td>0.002897</td>\n      <td>0.047521</td>\n      <td>0.170751</td>\n      <td>0.055124</td>\n      <td>0.062260</td>\n      <td>-0.113926</td>\n      <td>0.084459</td>\n      <td>0.163823</td>\n      <td>0.135095</td>\n      <td>-0.002183</td>\n    </tr>\n    <tr>\n      <th>expnum</th>\n      <td>0.008436</td>\n      <td>-0.061917</td>\n      <td>-0.219622</td>\n      <td>0.010543</td>\n      <td>-0.206112</td>\n      <td>-0.087988</td>\n      <td>NaN</td>\n      <td>-0.067169</td>\n      <td>-0.077468</td>\n      <td>0.004025</td>\n      <td>...</td>\n      <td>0.238088</td>\n      <td>0.310448</td>\n      <td>0.403202</td>\n      <td>0.364171</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.026029</td>\n    </tr>\n    <tr>\n      <th>attr1_1</th>\n      <td>0.358708</td>\n      <td>0.064929</td>\n      <td>-0.058400</td>\n      <td>0.066159</td>\n      <td>-0.022033</td>\n      <td>0.029496</td>\n      <td>0.039966</td>\n      <td>-0.000206</td>\n      <td>-0.014164</td>\n      <td>0.058514</td>\n      <td>...</td>\n      <td>-0.165132</td>\n      <td>-0.053793</td>\n      <td>0.016634</td>\n      <td>0.017151</td>\n      <td>0.141781</td>\n      <td>-0.126329</td>\n      <td>0.028733</td>\n      <td>0.053737</td>\n      <td>0.083864</td>\n      <td>0.004569</td>\n    </tr>\n    <tr>\n      <th>sinc1_1</th>\n      <td>-0.128036</td>\n      <td>-0.054237</td>\n      <td>0.073861</td>\n      <td>-0.004444</td>\n      <td>0.054769</td>\n      <td>-0.034389</td>\n      <td>-0.108037</td>\n      <td>0.008055</td>\n      <td>0.035212</td>\n      <td>-0.000547</td>\n      <td>...</td>\n      <td>0.276773</td>\n      <td>-0.098424</td>\n      <td>-0.135019</td>\n      <td>-0.221646</td>\n      <td>-0.218225</td>\n      <td>0.266792</td>\n      <td>-0.037643</td>\n      <td>-0.030022</td>\n      <td>-0.167470</td>\n      <td>0.003760</td>\n    </tr>\n    <tr>\n      <th>intel1_1</th>\n      <td>-0.116111</td>\n      <td>0.001187</td>\n      <td>0.022598</td>\n      <td>-0.068378</td>\n      <td>0.018986</td>\n      <td>0.068122</td>\n      <td>0.113905</td>\n      <td>0.010694</td>\n      <td>0.011788</td>\n      <td>-0.061844</td>\n      <td>...</td>\n      <td>-0.139009</td>\n      <td>0.052604</td>\n      <td>-0.106786</td>\n      <td>-0.055398</td>\n      <td>0.145876</td>\n      <td>-0.002869</td>\n      <td>0.182763</td>\n      <td>-0.025429</td>\n      <td>0.134709</td>\n      <td>-0.005910</td>\n    </tr>\n    <tr>\n      <th>fun1_1</th>\n      <td>0.038880</td>\n      <td>-0.005397</td>\n      <td>-0.011343</td>\n      <td>0.018212</td>\n      <td>-0.002919</td>\n      <td>-0.058613</td>\n      <td>-0.015776</td>\n      <td>0.008166</td>\n      <td>0.005819</td>\n      <td>0.012844</td>\n      <td>...</td>\n      <td>-0.121795</td>\n      <td>-0.003887</td>\n      <td>0.208838</td>\n      <td>-0.022123</td>\n      <td>0.041759</td>\n      <td>-0.195344</td>\n      <td>-0.060989</td>\n      <td>0.080179</td>\n      <td>-0.031505</td>\n      <td>0.011890</td>\n    </tr>\n    <tr>\n      <th>amb1_1</th>\n      <td>-0.353727</td>\n      <td>-0.091886</td>\n      <td>-0.037925</td>\n      <td>-0.106278</td>\n      <td>-0.084007</td>\n      <td>-0.010809</td>\n      <td>-0.034438</td>\n      <td>-0.044883</td>\n      <td>-0.029271</td>\n      <td>-0.094237</td>\n      <td>...</td>\n      <td>0.176973</td>\n      <td>0.119080</td>\n      <td>0.143643</td>\n      <td>0.338457</td>\n      <td>0.019402</td>\n      <td>-0.065959</td>\n      <td>-0.218550</td>\n      <td>-0.110028</td>\n      <td>0.032544</td>\n      <td>-0.002390</td>\n    </tr>\n    <tr>\n      <th>shar1_1</th>\n      <td>-0.138141</td>\n      <td>0.013375</td>\n      <td>0.021193</td>\n      <td>0.005778</td>\n      <td>0.023091</td>\n      <td>-0.027411</td>\n      <td>-0.018629</td>\n      <td>0.002939</td>\n      <td>-0.007603</td>\n      <td>0.005116</td>\n      <td>...</td>\n      <td>0.073131</td>\n      <td>0.068290</td>\n      <td>-0.052838</td>\n      <td>0.010282</td>\n      <td>-0.156318</td>\n      <td>0.176820</td>\n      <td>0.060130</td>\n      <td>0.026773</td>\n      <td>-0.065422</td>\n      <td>-0.009196</td>\n    </tr>\n    <tr>\n      <th>attr4_1</th>\n      <td>0.109446</td>\n      <td>0.039394</td>\n      <td>0.032700</td>\n      <td>0.493314</td>\n      <td>0.034487</td>\n      <td>0.072010</td>\n      <td>0.065447</td>\n      <td>0.008602</td>\n      <td>0.007331</td>\n      <td>0.499765</td>\n      <td>...</td>\n      <td>-0.113153</td>\n      <td>-0.211445</td>\n      <td>-0.026292</td>\n      <td>-0.105653</td>\n      <td>0.072470</td>\n      <td>-0.058278</td>\n      <td>0.080330</td>\n      <td>0.060939</td>\n      <td>0.051275</td>\n      <td>-0.005220</td>\n    </tr>\n    <tr>\n      <th>sinc4_1</th>\n      <td>-0.051003</td>\n      <td>-0.015084</td>\n      <td>-0.014103</td>\n      <td>0.189814</td>\n      <td>0.026545</td>\n      <td>-0.042490</td>\n      <td>-0.069286</td>\n      <td>0.008469</td>\n      <td>0.006681</td>\n      <td>0.201632</td>\n      <td>...</td>\n      <td>0.064290</td>\n      <td>-0.211495</td>\n      <td>-0.135032</td>\n      <td>-0.147860</td>\n      <td>-0.153126</td>\n      <td>0.072240</td>\n      <td>-0.049013</td>\n      <td>-0.002854</td>\n      <td>-0.009661</td>\n      <td>0.020575</td>\n    </tr>\n    <tr>\n      <th>intel4_1</th>\n      <td>-0.082631</td>\n      <td>0.004993</td>\n      <td>0.043723</td>\n      <td>0.272818</td>\n      <td>0.008698</td>\n      <td>0.016263</td>\n      <td>0.041593</td>\n      <td>-0.007428</td>\n      <td>-0.005573</td>\n      <td>0.294825</td>\n      <td>...</td>\n      <td>-0.073756</td>\n      <td>-0.183882</td>\n      <td>-0.032640</td>\n      <td>-0.114885</td>\n      <td>0.015993</td>\n      <td>-0.081432</td>\n      <td>-0.047351</td>\n      <td>-0.088294</td>\n      <td>0.033842</td>\n      <td>0.012566</td>\n    </tr>\n    <tr>\n      <th>fun4_1</th>\n      <td>0.081553</td>\n      <td>-0.014381</td>\n      <td>0.009761</td>\n      <td>0.402315</td>\n      <td>0.002562</td>\n      <td>-0.033385</td>\n      <td>0.021970</td>\n      <td>-0.000185</td>\n      <td>-0.000733</td>\n      <td>0.408668</td>\n      <td>...</td>\n      <td>-0.083362</td>\n      <td>-0.192402</td>\n      <td>-0.023672</td>\n      <td>-0.100802</td>\n      <td>0.056719</td>\n      <td>-0.021706</td>\n      <td>-0.012436</td>\n      <td>0.105909</td>\n      <td>0.088294</td>\n      <td>0.002328</td>\n    </tr>\n    <tr>\n      <th>amb4_1</th>\n      <td>-0.306273</td>\n      <td>0.063270</td>\n      <td>0.010402</td>\n      <td>0.141852</td>\n      <td>-0.004121</td>\n      <td>-0.031143</td>\n      <td>-0.032264</td>\n      <td>0.001953</td>\n      <td>-0.013533</td>\n      <td>0.176704</td>\n      <td>...</td>\n      <td>0.092950</td>\n      <td>-0.165902</td>\n      <td>-0.072509</td>\n      <td>-0.126621</td>\n      <td>0.040663</td>\n      <td>0.139908</td>\n      <td>-0.079058</td>\n      <td>-0.018185</td>\n      <td>-0.130667</td>\n      <td>-0.018565</td>\n    </tr>\n    <tr>\n      <th>shar4_1</th>\n      <td>-0.030401</td>\n      <td>-0.044442</td>\n      <td>0.026313</td>\n      <td>0.235715</td>\n      <td>0.019805</td>\n      <td>-0.047731</td>\n      <td>-0.055558</td>\n      <td>0.008589</td>\n      <td>0.003407</td>\n      <td>0.247361</td>\n      <td>...</td>\n      <td>-0.091868</td>\n      <td>-0.183753</td>\n      <td>-0.229451</td>\n      <td>-0.122882</td>\n      <td>-0.045712</td>\n      <td>0.073514</td>\n      <td>0.048611</td>\n      <td>-0.123826</td>\n      <td>-0.052342</td>\n      <td>0.018717</td>\n    </tr>\n    <tr>\n      <th>attr2_1</th>\n      <td>-0.321041</td>\n      <td>-0.018334</td>\n      <td>0.000013</td>\n      <td>0.113596</td>\n      <td>0.000602</td>\n      <td>0.080404</td>\n      <td>0.087930</td>\n      <td>-0.001284</td>\n      <td>-0.003943</td>\n      <td>0.142768</td>\n      <td>...</td>\n      <td>-0.002899</td>\n      <td>-0.135342</td>\n      <td>0.010052</td>\n      <td>-0.049852</td>\n      <td>0.006362</td>\n      <td>0.192893</td>\n      <td>-0.100061</td>\n      <td>0.091315</td>\n      <td>0.008574</td>\n      <td>-0.007076</td>\n    </tr>\n    <tr>\n      <th>sinc2_1</th>\n      <td>0.272461</td>\n      <td>0.015002</td>\n      <td>0.006631</td>\n      <td>-0.079352</td>\n      <td>0.047320</td>\n      <td>-0.056115</td>\n      <td>-0.066518</td>\n      <td>0.012382</td>\n      <td>0.016386</td>\n      <td>-0.101223</td>\n      <td>...</td>\n      <td>-0.002310</td>\n      <td>0.008459</td>\n      <td>-0.046643</td>\n      <td>-0.032854</td>\n      <td>-0.072877</td>\n      <td>-0.163560</td>\n      <td>-0.056843</td>\n      <td>-0.043213</td>\n      <td>0.022905</td>\n      <td>0.002436</td>\n    </tr>\n    <tr>\n      <th>intel2_1</th>\n      <td>0.291767</td>\n      <td>-0.046757</td>\n      <td>-0.014672</td>\n      <td>-0.145004</td>\n      <td>-0.026564</td>\n      <td>-0.077221</td>\n      <td>-0.086568</td>\n      <td>-0.022672</td>\n      <td>-0.012907</td>\n      <td>-0.163611</td>\n      <td>...</td>\n      <td>-0.084576</td>\n      <td>0.043136</td>\n      <td>-0.004259</td>\n      <td>0.037097</td>\n      <td>-0.001693</td>\n      <td>-0.178927</td>\n      <td>0.058929</td>\n      <td>-0.022984</td>\n      <td>0.128769</td>\n      <td>-0.000717</td>\n    </tr>\n    <tr>\n      <th>fun2_1</th>\n      <td>-0.046865</td>\n      <td>-0.007714</td>\n      <td>-0.062844</td>\n      <td>-0.022612</td>\n      <td>-0.056252</td>\n      <td>-0.062787</td>\n      <td>0.005288</td>\n      <td>-0.017481</td>\n      <td>-0.014848</td>\n      <td>-0.023950</td>\n      <td>...</td>\n      <td>0.061218</td>\n      <td>0.084568</td>\n      <td>0.030122</td>\n      <td>0.069966</td>\n      <td>0.251204</td>\n      <td>0.089477</td>\n      <td>0.053217</td>\n      <td>0.053786</td>\n      <td>-0.022542</td>\n      <td>0.017907</td>\n    </tr>\n    <tr>\n      <th>amb2_1</th>\n      <td>0.360655</td>\n      <td>0.005122</td>\n      <td>0.037463</td>\n      <td>-0.030698</td>\n      <td>0.019585</td>\n      <td>0.029416</td>\n      <td>0.005615</td>\n      <td>0.021711</td>\n      <td>0.010997</td>\n      <td>-0.056469</td>\n      <td>...</td>\n      <td>0.007858</td>\n      <td>0.067564</td>\n      <td>0.067664</td>\n      <td>0.061583</td>\n      <td>-0.186672</td>\n      <td>-0.180578</td>\n      <td>-0.056502</td>\n      <td>0.005160</td>\n      <td>-0.057602</td>\n      <td>-0.000935</td>\n    </tr>\n    <tr>\n      <th>shar2_1</th>\n      <td>-0.129111</td>\n      <td>0.068580</td>\n      <td>0.025546</td>\n      <td>-0.004710</td>\n      <td>0.010532</td>\n      <td>-0.021853</td>\n      <td>-0.057630</td>\n      <td>0.004621</td>\n      <td>0.008613</td>\n      <td>-0.006804</td>\n      <td>...</td>\n      <td>0.044612</td>\n      <td>0.105042</td>\n      <td>-0.078481</td>\n      <td>-0.019268</td>\n      <td>0.046571</td>\n      <td>0.061499</td>\n      <td>0.223123</td>\n      <td>-0.204678</td>\n      <td>-0.086441</td>\n      <td>0.000652</td>\n    </tr>\n    <tr>\n      <th>attr3_1</th>\n      <td>-0.097964</td>\n      <td>-0.003279</td>\n      <td>-0.021499</td>\n      <td>0.031454</td>\n      <td>-0.008915</td>\n      <td>-0.000843</td>\n      <td>-0.056831</td>\n      <td>-0.001284</td>\n      <td>-0.006513</td>\n      <td>0.033409</td>\n      <td>...</td>\n      <td>0.241868</td>\n      <td>0.418011</td>\n      <td>0.453654</td>\n      <td>0.328365</td>\n      <td>0.728599</td>\n      <td>0.062970</td>\n      <td>0.253562</td>\n      <td>0.200068</td>\n      <td>0.065573</td>\n      <td>0.001370</td>\n    </tr>\n    <tr>\n      <th>sinc3_1</th>\n      <td>-0.112585</td>\n      <td>0.022840</td>\n      <td>0.039220</td>\n      <td>0.080159</td>\n      <td>0.041560</td>\n      <td>-0.037172</td>\n      <td>-0.124761</td>\n      <td>0.004317</td>\n      <td>0.013982</td>\n      <td>0.080224</td>\n      <td>...</td>\n      <td>0.595560</td>\n      <td>0.101004</td>\n      <td>0.151177</td>\n      <td>0.121634</td>\n      <td>-0.008482</td>\n      <td>0.346617</td>\n      <td>0.118511</td>\n      <td>-0.092052</td>\n      <td>0.043192</td>\n      <td>-0.021861</td>\n    </tr>\n    <tr>\n      <th>fun3_1</th>\n      <td>-0.124766</td>\n      <td>0.011894</td>\n      <td>0.006433</td>\n      <td>0.041330</td>\n      <td>0.004073</td>\n      <td>-0.056843</td>\n      <td>-0.125989</td>\n      <td>-0.012746</td>\n      <td>-0.008545</td>\n      <td>0.047207</td>\n      <td>...</td>\n      <td>0.243325</td>\n      <td>0.243808</td>\n      <td>0.780468</td>\n      <td>0.395295</td>\n      <td>0.238455</td>\n      <td>0.083827</td>\n      <td>0.063552</td>\n      <td>0.567068</td>\n      <td>0.206823</td>\n      <td>-0.012435</td>\n    </tr>\n    <tr>\n      <th>intel3_1</th>\n      <td>0.066972</td>\n      <td>-0.025905</td>\n      <td>0.036632</td>\n      <td>-0.020034</td>\n      <td>0.054837</td>\n      <td>0.027916</td>\n      <td>-0.046548</td>\n      <td>0.024283</td>\n      <td>0.021033</td>\n      <td>-0.028808</td>\n      <td>...</td>\n      <td>0.275452</td>\n      <td>0.648242</td>\n      <td>0.248223</td>\n      <td>0.374548</td>\n      <td>0.168583</td>\n      <td>0.126810</td>\n      <td>0.573897</td>\n      <td>0.043509</td>\n      <td>0.157084</td>\n      <td>-0.020278</td>\n    </tr>\n    <tr>\n      <th>amb3_1</th>\n      <td>-0.027999</td>\n      <td>0.013006</td>\n      <td>-0.026106</td>\n      <td>0.066982</td>\n      <td>0.009213</td>\n      <td>0.047543</td>\n      <td>-0.004873</td>\n      <td>0.006092</td>\n      <td>0.001432</td>\n      <td>0.068137</td>\n      <td>...</td>\n      <td>0.181898</td>\n      <td>0.236303</td>\n      <td>0.313178</td>\n      <td>0.717328</td>\n      <td>0.038822</td>\n      <td>-0.042573</td>\n      <td>0.113309</td>\n      <td>0.012905</td>\n      <td>0.465309</td>\n      <td>-0.005674</td>\n    </tr>\n    <tr>\n      <th>attr5_1</th>\n      <td>-0.054282</td>\n      <td>0.021721</td>\n      <td>-0.001471</td>\n      <td>0.040485</td>\n      <td>0.015923</td>\n      <td>0.001034</td>\n      <td>-0.000589</td>\n      <td>0.003969</td>\n      <td>0.005443</td>\n      <td>0.043895</td>\n      <td>...</td>\n      <td>0.144479</td>\n      <td>0.354587</td>\n      <td>0.453205</td>\n      <td>0.239123</td>\n      <td>0.783188</td>\n      <td>0.061684</td>\n      <td>0.266386</td>\n      <td>0.366830</td>\n      <td>0.154660</td>\n      <td>-0.012665</td>\n    </tr>\n    <tr>\n      <th>sinc5_1</th>\n      <td>-0.120055</td>\n      <td>-0.002354</td>\n      <td>-0.024573</td>\n      <td>0.052784</td>\n      <td>-0.029875</td>\n      <td>-0.090004</td>\n      <td>-0.113604</td>\n      <td>-0.012479</td>\n      <td>-0.015566</td>\n      <td>0.058008</td>\n      <td>...</td>\n      <td>0.467349</td>\n      <td>0.123232</td>\n      <td>0.024818</td>\n      <td>-0.112742</td>\n      <td>0.003268</td>\n      <td>0.418335</td>\n      <td>0.232349</td>\n      <td>-0.018190</td>\n      <td>0.046480</td>\n      <td>-0.010633</td>\n    </tr>\n    <tr>\n      <th>intel5_1</th>\n      <td>0.033399</td>\n      <td>0.059064</td>\n      <td>0.037056</td>\n      <td>0.004592</td>\n      <td>0.062049</td>\n      <td>-0.015384</td>\n      <td>-0.044831</td>\n      <td>0.023665</td>\n      <td>0.024851</td>\n      <td>-0.004256</td>\n      <td>...</td>\n      <td>0.219992</td>\n      <td>0.463971</td>\n      <td>0.151436</td>\n      <td>0.166192</td>\n      <td>0.194008</td>\n      <td>0.231658</td>\n      <td>0.600613</td>\n      <td>0.025814</td>\n      <td>0.234111</td>\n      <td>-0.026115</td>\n    </tr>\n    <tr>\n      <th>fun5_1</th>\n      <td>-0.046997</td>\n      <td>0.007296</td>\n      <td>0.071791</td>\n      <td>0.105681</td>\n      <td>0.026465</td>\n      <td>-0.054398</td>\n      <td>-0.084207</td>\n      <td>-0.002723</td>\n      <td>0.013201</td>\n      <td>0.116088</td>\n      <td>...</td>\n      <td>0.077312</td>\n      <td>0.124537</td>\n      <td>0.670564</td>\n      <td>0.228003</td>\n      <td>0.295534</td>\n      <td>0.096232</td>\n      <td>0.064198</td>\n      <td>0.587761</td>\n      <td>0.172046</td>\n      <td>-0.018259</td>\n    </tr>\n    <tr>\n      <th>amb5_1</th>\n      <td>-0.068505</td>\n      <td>-0.008596</td>\n      <td>-0.001089</td>\n      <td>-0.066055</td>\n      <td>0.001293</td>\n      <td>-0.011485</td>\n      <td>-0.056936</td>\n      <td>-0.007957</td>\n      <td>0.008483</td>\n      <td>-0.057144</td>\n      <td>...</td>\n      <td>0.170869</td>\n      <td>0.160568</td>\n      <td>0.366440</td>\n      <td>0.475958</td>\n      <td>0.166571</td>\n      <td>0.133860</td>\n      <td>0.253957</td>\n      <td>0.181020</td>\n      <td>0.537121</td>\n      <td>-0.002704</td>\n    </tr>\n    <tr>\n      <th>attr</th>\n      <td>0.150591</td>\n      <td>-0.029040</td>\n      <td>0.001257</td>\n      <td>-0.021996</td>\n      <td>-0.030136</td>\n      <td>-0.017458</td>\n      <td>-0.041890</td>\n      <td>-0.015684</td>\n      <td>0.044669</td>\n      <td>-0.026312</td>\n      <td>...</td>\n      <td>0.050352</td>\n      <td>0.054133</td>\n      <td>-0.003241</td>\n      <td>-0.023687</td>\n      <td>-0.094509</td>\n      <td>-0.089235</td>\n      <td>0.047135</td>\n      <td>-0.064726</td>\n      <td>-0.030551</td>\n      <td>0.004724</td>\n    </tr>\n    <tr>\n      <th>sinc</th>\n      <td>0.052213</td>\n      <td>-0.042309</td>\n      <td>-0.035046</td>\n      <td>-0.045152</td>\n      <td>-0.065544</td>\n      <td>-0.037133</td>\n      <td>-0.081904</td>\n      <td>-0.109045</td>\n      <td>-0.011619</td>\n      <td>-0.044979</td>\n      <td>...</td>\n      <td>0.117877</td>\n      <td>0.143153</td>\n      <td>0.096242</td>\n      <td>0.051357</td>\n      <td>-0.010054</td>\n      <td>0.002014</td>\n      <td>0.126807</td>\n      <td>0.029569</td>\n      <td>0.012667</td>\n      <td>-0.005235</td>\n    </tr>\n    <tr>\n      <th>intel</th>\n      <td>-0.056286</td>\n      <td>-0.055453</td>\n      <td>-0.013140</td>\n      <td>-0.042712</td>\n      <td>-0.049731</td>\n      <td>-0.026030</td>\n      <td>-0.094042</td>\n      <td>-0.070997</td>\n      <td>-0.030061</td>\n      <td>-0.039822</td>\n      <td>...</td>\n      <td>0.106134</td>\n      <td>0.120312</td>\n      <td>0.099107</td>\n      <td>0.063536</td>\n      <td>-0.036674</td>\n      <td>-0.074296</td>\n      <td>0.042609</td>\n      <td>-0.040037</td>\n      <td>-0.027882</td>\n      <td>-0.021023</td>\n    </tr>\n    <tr>\n      <th>fun</th>\n      <td>0.067724</td>\n      <td>-0.055276</td>\n      <td>-0.029161</td>\n      <td>-0.000692</td>\n      <td>-0.062719</td>\n      <td>-0.007740</td>\n      <td>-0.055955</td>\n      <td>-0.049840</td>\n      <td>0.031848</td>\n      <td>-0.001095</td>\n      <td>...</td>\n      <td>0.067943</td>\n      <td>0.066830</td>\n      <td>0.056486</td>\n      <td>0.018673</td>\n      <td>-0.062324</td>\n      <td>-0.046059</td>\n      <td>0.055066</td>\n      <td>-0.048143</td>\n      <td>-0.018573</td>\n      <td>0.006732</td>\n    </tr>\n    <tr>\n      <th>amb</th>\n      <td>-0.100807</td>\n      <td>-0.056884</td>\n      <td>-0.019165</td>\n      <td>-0.029314</td>\n      <td>-0.051643</td>\n      <td>-0.016013</td>\n      <td>-0.079649</td>\n      <td>-0.062281</td>\n      <td>-0.009791</td>\n      <td>-0.022139</td>\n      <td>...</td>\n      <td>0.104383</td>\n      <td>0.084675</td>\n      <td>0.111915</td>\n      <td>0.080133</td>\n      <td>-0.011721</td>\n      <td>-0.043762</td>\n      <td>0.030984</td>\n      <td>-0.031694</td>\n      <td>-0.000232</td>\n      <td>0.006436</td>\n    </tr>\n    <tr>\n      <th>shar</th>\n      <td>0.045450</td>\n      <td>-0.033752</td>\n      <td>-0.027448</td>\n      <td>-0.047732</td>\n      <td>-0.046708</td>\n      <td>-0.021186</td>\n      <td>-0.053804</td>\n      <td>0.004834</td>\n      <td>-0.002430</td>\n      <td>-0.047593</td>\n      <td>...</td>\n      <td>0.053673</td>\n      <td>0.099172</td>\n      <td>0.055434</td>\n      <td>0.052385</td>\n      <td>-0.053168</td>\n      <td>0.002786</td>\n      <td>0.072060</td>\n      <td>-0.071557</td>\n      <td>0.044264</td>\n      <td>-0.002772</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0.087993</td>\n      <td>-0.048893</td>\n      <td>-0.017051</td>\n      <td>-0.033305</td>\n      <td>-0.053121</td>\n      <td>-0.028364</td>\n      <td>-0.061637</td>\n      <td>-0.045581</td>\n      <td>0.009716</td>\n      <td>-0.035488</td>\n      <td>...</td>\n      <td>0.063154</td>\n      <td>0.067423</td>\n      <td>0.039788</td>\n      <td>0.020897</td>\n      <td>-0.038667</td>\n      <td>-0.072835</td>\n      <td>0.023475</td>\n      <td>-0.021692</td>\n      <td>-0.028307</td>\n      <td>-0.005198</td>\n    </tr>\n    <tr>\n      <th>prob</th>\n      <td>-0.002889</td>\n      <td>-0.002267</td>\n      <td>-0.013728</td>\n      <td>0.026464</td>\n      <td>-0.034377</td>\n      <td>-0.037648</td>\n      <td>-0.045606</td>\n      <td>-0.070418</td>\n      <td>0.003500</td>\n      <td>0.028654</td>\n      <td>...</td>\n      <td>0.122711</td>\n      <td>0.225129</td>\n      <td>0.198384</td>\n      <td>0.151261</td>\n      <td>0.193262</td>\n      <td>0.052151</td>\n      <td>0.115475</td>\n      <td>0.102856</td>\n      <td>-0.069938</td>\n      <td>-0.007546</td>\n    </tr>\n    <tr>\n      <th>met</th>\n      <td>-0.008150</td>\n      <td>-0.073981</td>\n      <td>-0.149189</td>\n      <td>-0.534940</td>\n      <td>-0.165709</td>\n      <td>-0.062723</td>\n      <td>-0.042405</td>\n      <td>-0.066052</td>\n      <td>-0.063532</td>\n      <td>-0.516313</td>\n      <td>...</td>\n      <td>-0.035228</td>\n      <td>0.095836</td>\n      <td>0.014692</td>\n      <td>0.065309</td>\n      <td>0.126795</td>\n      <td>-0.049179</td>\n      <td>0.008415</td>\n      <td>0.086748</td>\n      <td>0.055107</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <th>match_es</th>\n      <td>0.072425</td>\n      <td>-0.009075</td>\n      <td>0.160734</td>\n      <td>0.066307</td>\n      <td>0.176401</td>\n      <td>0.030625</td>\n      <td>0.062017</td>\n      <td>0.065416</td>\n      <td>0.066444</td>\n      <td>0.063662</td>\n      <td>...</td>\n      <td>0.021927</td>\n      <td>0.238484</td>\n      <td>0.220876</td>\n      <td>0.090001</td>\n      <td>0.162734</td>\n      <td>-0.038218</td>\n      <td>0.083571</td>\n      <td>0.192865</td>\n      <td>0.080877</td>\n      <td>-0.010435</td>\n    </tr>\n    <tr>\n      <th>attr1_s</th>\n      <td>0.227378</td>\n      <td>0.144504</td>\n      <td>0.125272</td>\n      <td>-0.036866</td>\n      <td>0.139061</td>\n      <td>0.099658</td>\n      <td>0.029689</td>\n      <td>0.056094</td>\n      <td>0.041694</td>\n      <td>-0.016376</td>\n      <td>...</td>\n      <td>-0.125570</td>\n      <td>-0.114423</td>\n      <td>-0.033959</td>\n      <td>-0.106962</td>\n      <td>-0.004976</td>\n      <td>-0.026713</td>\n      <td>-0.169411</td>\n      <td>0.140237</td>\n      <td>-0.017623</td>\n      <td>0.005297</td>\n    </tr>\n    <tr>\n      <th>sinc1_s</th>\n      <td>-0.148228</td>\n      <td>0.037631</td>\n      <td>0.004089</td>\n      <td>-0.370831</td>\n      <td>0.113735</td>\n      <td>-0.007399</td>\n      <td>-0.008551</td>\n      <td>0.034659</td>\n      <td>0.037056</td>\n      <td>-0.338712</td>\n      <td>...</td>\n      <td>0.141123</td>\n      <td>-0.072959</td>\n      <td>-0.030216</td>\n      <td>-0.124861</td>\n      <td>-0.050045</td>\n      <td>0.192493</td>\n      <td>-0.031094</td>\n      <td>-0.043958</td>\n      <td>-0.136601</td>\n      <td>-0.006537</td>\n    </tr>\n    <tr>\n      <th>intel1_s</th>\n      <td>-0.071305</td>\n      <td>0.071612</td>\n      <td>0.024626</td>\n      <td>-0.449692</td>\n      <td>0.216136</td>\n      <td>0.102865</td>\n      <td>0.103964</td>\n      <td>0.096128</td>\n      <td>0.099265</td>\n      <td>-0.420033</td>\n      <td>...</td>\n      <td>0.008616</td>\n      <td>0.128174</td>\n      <td>-0.026696</td>\n      <td>0.153747</td>\n      <td>0.252735</td>\n      <td>0.048295</td>\n      <td>-0.016186</td>\n      <td>0.023570</td>\n      <td>0.102055</td>\n      <td>-0.006084</td>\n    </tr>\n    <tr>\n      <th>fun1_s</th>\n      <td>0.045059</td>\n      <td>-0.007659</td>\n      <td>0.055672</td>\n      <td>-0.478134</td>\n      <td>0.190455</td>\n      <td>0.060828</td>\n      <td>0.057004</td>\n      <td>0.079385</td>\n      <td>0.079734</td>\n      <td>-0.459403</td>\n      <td>...</td>\n      <td>-0.004265</td>\n      <td>0.222721</td>\n      <td>0.380015</td>\n      <td>0.201385</td>\n      <td>0.001344</td>\n      <td>-0.002813</td>\n      <td>0.018332</td>\n      <td>0.284007</td>\n      <td>0.096638</td>\n      <td>-0.040421</td>\n    </tr>\n    <tr>\n      <th>amb1_s</th>\n      <td>-0.225662</td>\n      <td>0.033745</td>\n      <td>-0.012921</td>\n      <td>-0.499079</td>\n      <td>0.152895</td>\n      <td>0.107787</td>\n      <td>0.127354</td>\n      <td>0.064455</td>\n      <td>0.068826</td>\n      <td>-0.478852</td>\n      <td>...</td>\n      <td>0.081957</td>\n      <td>0.213335</td>\n      <td>0.233355</td>\n      <td>0.422025</td>\n      <td>0.113646</td>\n      <td>0.011880</td>\n      <td>0.009424</td>\n      <td>0.139955</td>\n      <td>0.091539</td>\n      <td>-0.005805</td>\n    </tr>\n    <tr>\n      <th>shar1_s</th>\n      <td>-0.163630</td>\n      <td>0.052607</td>\n      <td>0.057951</td>\n      <td>-0.392774</td>\n      <td>0.241270</td>\n      <td>0.090933</td>\n      <td>0.135785</td>\n      <td>0.102709</td>\n      <td>0.102039</td>\n      <td>-0.366181</td>\n      <td>...</td>\n      <td>0.095394</td>\n      <td>0.107575</td>\n      <td>-0.059192</td>\n      <td>0.028756</td>\n      <td>-0.074977</td>\n      <td>0.197201</td>\n      <td>0.223003</td>\n      <td>0.005782</td>\n      <td>-0.036773</td>\n      <td>0.001530</td>\n    </tr>\n    <tr>\n      <th>attr3_s</th>\n      <td>-0.170337</td>\n      <td>-0.042979</td>\n      <td>-0.002199</td>\n      <td>-0.048040</td>\n      <td>0.000260</td>\n      <td>-0.070755</td>\n      <td>-0.177439</td>\n      <td>-0.004447</td>\n      <td>-0.009027</td>\n      <td>-0.033833</td>\n      <td>...</td>\n      <td>0.183761</td>\n      <td>0.364096</td>\n      <td>0.411011</td>\n      <td>0.296898</td>\n      <td>0.645850</td>\n      <td>0.097216</td>\n      <td>0.207239</td>\n      <td>0.245253</td>\n      <td>0.166821</td>\n      <td>-0.026020</td>\n    </tr>\n    <tr>\n      <th>sinc3_s</th>\n      <td>-0.083114</td>\n      <td>-0.076855</td>\n      <td>-0.033920</td>\n      <td>-0.022823</td>\n      <td>-0.057385</td>\n      <td>-0.085861</td>\n      <td>-0.126159</td>\n      <td>-0.037781</td>\n      <td>-0.035741</td>\n      <td>-0.014486</td>\n      <td>...</td>\n      <td>0.505139</td>\n      <td>0.256875</td>\n      <td>0.107078</td>\n      <td>0.202500</td>\n      <td>-0.008320</td>\n      <td>0.536494</td>\n      <td>0.240621</td>\n      <td>0.223292</td>\n      <td>0.264584</td>\n      <td>-0.030234</td>\n    </tr>\n    <tr>\n      <th>intel3_s</th>\n      <td>0.079544</td>\n      <td>-0.110518</td>\n      <td>0.014676</td>\n      <td>-0.117192</td>\n      <td>0.052595</td>\n      <td>-0.010670</td>\n      <td>-0.077018</td>\n      <td>0.012341</td>\n      <td>0.019171</td>\n      <td>-0.122699</td>\n      <td>...</td>\n      <td>0.206168</td>\n      <td>0.721019</td>\n      <td>0.324110</td>\n      <td>0.366250</td>\n      <td>0.107097</td>\n      <td>0.023433</td>\n      <td>0.633459</td>\n      <td>0.052304</td>\n      <td>0.351795</td>\n      <td>-0.033225</td>\n    </tr>\n    <tr>\n      <th>fun3_s</th>\n      <td>-0.151719</td>\n      <td>0.004248</td>\n      <td>0.006073</td>\n      <td>0.042108</td>\n      <td>-0.044370</td>\n      <td>-0.110437</td>\n      <td>-0.141682</td>\n      <td>-0.035326</td>\n      <td>-0.031097</td>\n      <td>0.054940</td>\n      <td>...</td>\n      <td>0.264958</td>\n      <td>0.379005</td>\n      <td>0.743430</td>\n      <td>0.362475</td>\n      <td>0.443814</td>\n      <td>0.178383</td>\n      <td>0.205753</td>\n      <td>0.618454</td>\n      <td>0.176674</td>\n      <td>-0.043875</td>\n    </tr>\n    <tr>\n      <th>amb3_s</th>\n      <td>0.000678</td>\n      <td>0.049354</td>\n      <td>0.018129</td>\n      <td>-0.035084</td>\n      <td>0.021250</td>\n      <td>0.075103</td>\n      <td>-0.004171</td>\n      <td>0.002221</td>\n      <td>0.007901</td>\n      <td>-0.029416</td>\n      <td>...</td>\n      <td>0.168138</td>\n      <td>0.414846</td>\n      <td>0.339094</td>\n      <td>0.752821</td>\n      <td>0.066160</td>\n      <td>-0.094443</td>\n      <td>0.171887</td>\n      <td>0.046869</td>\n      <td>0.397169</td>\n      <td>-0.017615</td>\n    </tr>\n    <tr>\n      <th>satis_2</th>\n      <td>0.233018</td>\n      <td>-0.068285</td>\n      <td>0.089599</td>\n      <td>0.025666</td>\n      <td>0.033352</td>\n      <td>-0.015543</td>\n      <td>-0.073251</td>\n      <td>0.004668</td>\n      <td>0.025536</td>\n      <td>0.018324</td>\n      <td>...</td>\n      <td>0.070152</td>\n      <td>0.069002</td>\n      <td>0.001383</td>\n      <td>-0.059468</td>\n      <td>-0.131076</td>\n      <td>0.060918</td>\n      <td>0.138220</td>\n      <td>-0.021568</td>\n      <td>0.058399</td>\n      <td>-0.010571</td>\n    </tr>\n    <tr>\n      <th>length</th>\n      <td>-0.119169</td>\n      <td>-0.042588</td>\n      <td>-0.074486</td>\n      <td>-0.031396</td>\n      <td>-0.087297</td>\n      <td>-0.010583</td>\n      <td>-0.007105</td>\n      <td>-0.029560</td>\n      <td>-0.032727</td>\n      <td>-0.025512</td>\n      <td>...</td>\n      <td>0.053620</td>\n      <td>0.083446</td>\n      <td>0.087704</td>\n      <td>0.016259</td>\n      <td>0.005104</td>\n      <td>-0.040089</td>\n      <td>0.011840</td>\n      <td>-0.015701</td>\n      <td>0.016839</td>\n      <td>0.024098</td>\n    </tr>\n    <tr>\n      <th>numdat_2</th>\n      <td>0.009902</td>\n      <td>-0.014616</td>\n      <td>0.125513</td>\n      <td>0.052464</td>\n      <td>0.046609</td>\n      <td>0.029570</td>\n      <td>0.081855</td>\n      <td>0.015751</td>\n      <td>0.012016</td>\n      <td>0.054740</td>\n      <td>...</td>\n      <td>0.075689</td>\n      <td>0.143969</td>\n      <td>0.088561</td>\n      <td>0.084523</td>\n      <td>0.066042</td>\n      <td>0.013466</td>\n      <td>0.176579</td>\n      <td>0.117053</td>\n      <td>0.178603</td>\n      <td>-0.003891</td>\n    </tr>\n    <tr>\n      <th>attr7_2</th>\n      <td>0.240079</td>\n      <td>0.101113</td>\n      <td>0.094626</td>\n      <td>0.140518</td>\n      <td>0.161243</td>\n      <td>0.230463</td>\n      <td>0.196614</td>\n      <td>0.056029</td>\n      <td>0.068460</td>\n      <td>0.105500</td>\n      <td>...</td>\n      <td>-0.423815</td>\n      <td>0.030720</td>\n      <td>0.086469</td>\n      <td>-0.023553</td>\n      <td>0.240206</td>\n      <td>-0.259468</td>\n      <td>0.039075</td>\n      <td>0.078242</td>\n      <td>-0.001049</td>\n      <td>-0.031765</td>\n    </tr>\n    <tr>\n      <th>sinc7_2</th>\n      <td>-0.229532</td>\n      <td>-0.079521</td>\n      <td>-0.133667</td>\n      <td>-0.203478</td>\n      <td>-0.116902</td>\n      <td>-0.226966</td>\n      <td>-0.270938</td>\n      <td>-0.040826</td>\n      <td>-0.059975</td>\n      <td>-0.158737</td>\n      <td>...</td>\n      <td>0.442285</td>\n      <td>-0.125988</td>\n      <td>-0.133264</td>\n      <td>-0.271622</td>\n      <td>-0.222895</td>\n      <td>0.358466</td>\n      <td>-0.127123</td>\n      <td>-0.035996</td>\n      <td>-0.113881</td>\n      <td>0.016765</td>\n    </tr>\n    <tr>\n      <th>intel7_2</th>\n      <td>-0.035797</td>\n      <td>-0.125661</td>\n      <td>-0.157890</td>\n      <td>-0.049091</td>\n      <td>-0.172290</td>\n      <td>-0.115714</td>\n      <td>-0.011696</td>\n      <td>-0.046442</td>\n      <td>-0.084981</td>\n      <td>-0.053771</td>\n      <td>...</td>\n      <td>0.119599</td>\n      <td>0.114853</td>\n      <td>-0.371799</td>\n      <td>0.120080</td>\n      <td>-0.065080</td>\n      <td>0.085637</td>\n      <td>0.052095</td>\n      <td>-0.051027</td>\n      <td>0.186837</td>\n      <td>-0.013544</td>\n    </tr>\n    <tr>\n      <th>fun7_2</th>\n      <td>0.050539</td>\n      <td>0.116986</td>\n      <td>0.164666</td>\n      <td>0.040259</td>\n      <td>0.095115</td>\n      <td>0.032051</td>\n      <td>0.016590</td>\n      <td>0.019963</td>\n      <td>0.061224</td>\n      <td>0.037684</td>\n      <td>...</td>\n      <td>0.017275</td>\n      <td>0.088979</td>\n      <td>0.441174</td>\n      <td>-0.024716</td>\n      <td>0.083959</td>\n      <td>-0.039435</td>\n      <td>-0.054284</td>\n      <td>0.070718</td>\n      <td>-0.036297</td>\n      <td>0.001578</td>\n    </tr>\n    <tr>\n      <th>amb7_2</th>\n      <td>-0.118053</td>\n      <td>-0.098058</td>\n      <td>0.068928</td>\n      <td>-0.304713</td>\n      <td>-0.121248</td>\n      <td>-0.021613</td>\n      <td>-0.052506</td>\n      <td>-0.033037</td>\n      <td>-0.065445</td>\n      <td>-0.294113</td>\n      <td>...</td>\n      <td>0.173409</td>\n      <td>-0.064826</td>\n      <td>-0.352185</td>\n      <td>0.163962</td>\n      <td>-0.274135</td>\n      <td>-0.024126</td>\n      <td>-0.083176</td>\n      <td>-0.240096</td>\n      <td>0.188590</td>\n      <td>-0.003489</td>\n    </tr>\n    <tr>\n      <th>shar7_2</th>\n      <td>-0.187272</td>\n      <td>0.038767</td>\n      <td>0.017435</td>\n      <td>0.015800</td>\n      <td>0.017017</td>\n      <td>-0.121056</td>\n      <td>-0.045231</td>\n      <td>0.004111</td>\n      <td>-0.003744</td>\n      <td>0.047792</td>\n      <td>...</td>\n      <td>0.187403</td>\n      <td>-0.075326</td>\n      <td>0.105244</td>\n      <td>0.123534</td>\n      <td>-0.104483</td>\n      <td>0.282007</td>\n      <td>0.131060</td>\n      <td>0.152609</td>\n      <td>-0.071867</td>\n      <td>0.049266</td>\n    </tr>\n    <tr>\n      <th>attr1_2</th>\n      <td>0.286076</td>\n      <td>0.050717</td>\n      <td>0.025172</td>\n      <td>0.017277</td>\n      <td>0.030184</td>\n      <td>0.054764</td>\n      <td>0.017208</td>\n      <td>0.021672</td>\n      <td>0.013804</td>\n      <td>0.013149</td>\n      <td>...</td>\n      <td>-0.179189</td>\n      <td>-0.099979</td>\n      <td>-0.017683</td>\n      <td>0.005369</td>\n      <td>0.134024</td>\n      <td>-0.202915</td>\n      <td>-0.045205</td>\n      <td>0.038521</td>\n      <td>0.050290</td>\n      <td>-0.000348</td>\n    </tr>\n    <tr>\n      <th>sinc1_2</th>\n      <td>-0.119539</td>\n      <td>-0.081276</td>\n      <td>0.051594</td>\n      <td>0.005384</td>\n      <td>0.009784</td>\n      <td>-0.048538</td>\n      <td>-0.121358</td>\n      <td>-0.010012</td>\n      <td>0.009305</td>\n      <td>0.007889</td>\n      <td>...</td>\n      <td>0.305937</td>\n      <td>-0.035080</td>\n      <td>-0.120756</td>\n      <td>-0.172083</td>\n      <td>-0.115117</td>\n      <td>0.250008</td>\n      <td>-0.023996</td>\n      <td>-0.110787</td>\n      <td>-0.165929</td>\n      <td>0.000659</td>\n    </tr>\n    <tr>\n      <th>intel1_2</th>\n      <td>-0.181103</td>\n      <td>0.027341</td>\n      <td>-0.032168</td>\n      <td>0.044726</td>\n      <td>-0.046451</td>\n      <td>-0.013350</td>\n      <td>0.070433</td>\n      <td>-0.017717</td>\n      <td>-0.027963</td>\n      <td>0.051520</td>\n      <td>...</td>\n      <td>-0.056578</td>\n      <td>0.075657</td>\n      <td>-0.025681</td>\n      <td>0.001921</td>\n      <td>0.197443</td>\n      <td>0.045393</td>\n      <td>0.175110</td>\n      <td>-0.004640</td>\n      <td>0.132209</td>\n      <td>0.005489</td>\n    </tr>\n    <tr>\n      <th>fun1_2</th>\n      <td>0.039954</td>\n      <td>-0.024334</td>\n      <td>-0.009615</td>\n      <td>-0.117616</td>\n      <td>-0.011172</td>\n      <td>-0.056444</td>\n      <td>-0.025703</td>\n      <td>-0.007437</td>\n      <td>0.002146</td>\n      <td>-0.119528</td>\n      <td>...</td>\n      <td>-0.089612</td>\n      <td>-0.047710</td>\n      <td>0.156329</td>\n      <td>-0.099554</td>\n      <td>-0.077956</td>\n      <td>0.007568</td>\n      <td>-0.120197</td>\n      <td>0.181430</td>\n      <td>0.122049</td>\n      <td>-0.004338</td>\n    </tr>\n    <tr>\n      <th>amb1_2</th>\n      <td>-0.253489</td>\n      <td>-0.032963</td>\n      <td>-0.038700</td>\n      <td>0.005138</td>\n      <td>-0.022467</td>\n      <td>0.002718</td>\n      <td>0.022552</td>\n      <td>-0.014914</td>\n      <td>-0.013337</td>\n      <td>0.006846</td>\n      <td>...</td>\n      <td>0.178237</td>\n      <td>0.131004</td>\n      <td>0.094893</td>\n      <td>0.322076</td>\n      <td>-0.158519</td>\n      <td>-0.039641</td>\n      <td>-0.119619</td>\n      <td>-0.142052</td>\n      <td>0.097966</td>\n      <td>-0.003638</td>\n    </tr>\n    <tr>\n      <th>shar1_2</th>\n      <td>-0.180378</td>\n      <td>0.008411</td>\n      <td>-0.004014</td>\n      <td>0.019403</td>\n      <td>0.024939</td>\n      <td>-0.002842</td>\n      <td>0.031258</td>\n      <td>0.012869</td>\n      <td>0.006547</td>\n      <td>0.022413</td>\n      <td>...</td>\n      <td>0.026500</td>\n      <td>0.088735</td>\n      <td>-0.025856</td>\n      <td>-0.025786</td>\n      <td>-0.102186</td>\n      <td>0.117239</td>\n      <td>0.134727</td>\n      <td>0.060368</td>\n      <td>-0.172710</td>\n      <td>0.006281</td>\n    </tr>\n    <tr>\n      <th>attr4_2</th>\n      <td>0.134797</td>\n      <td>0.043706</td>\n      <td>0.037410</td>\n      <td>0.506276</td>\n      <td>0.030875</td>\n      <td>0.059540</td>\n      <td>0.022563</td>\n      <td>0.014388</td>\n      <td>0.001388</td>\n      <td>0.509596</td>\n      <td>...</td>\n      <td>-0.179959</td>\n      <td>-0.242490</td>\n      <td>-0.028967</td>\n      <td>-0.054571</td>\n      <td>-0.055860</td>\n      <td>-0.178872</td>\n      <td>0.039273</td>\n      <td>-0.086996</td>\n      <td>0.113210</td>\n      <td>-0.008282</td>\n    </tr>\n    <tr>\n      <th>sinc4_2</th>\n      <td>-0.134006</td>\n      <td>-0.039498</td>\n      <td>0.036923</td>\n      <td>0.317918</td>\n      <td>0.077659</td>\n      <td>0.015477</td>\n      <td>0.049633</td>\n      <td>0.018436</td>\n      <td>0.016394</td>\n      <td>0.336628</td>\n      <td>...</td>\n      <td>0.108105</td>\n      <td>-0.253635</td>\n      <td>-0.050362</td>\n      <td>-0.132106</td>\n      <td>0.058395</td>\n      <td>0.113777</td>\n      <td>-0.090325</td>\n      <td>0.071470</td>\n      <td>-0.096285</td>\n      <td>0.027833</td>\n    </tr>\n    <tr>\n      <th>intel4_2</th>\n      <td>-0.159543</td>\n      <td>0.010525</td>\n      <td>0.015670</td>\n      <td>0.300415</td>\n      <td>0.042230</td>\n      <td>0.053747</td>\n      <td>0.073411</td>\n      <td>0.011362</td>\n      <td>0.003559</td>\n      <td>0.325448</td>\n      <td>...</td>\n      <td>-0.011493</td>\n      <td>-0.285922</td>\n      <td>-0.062223</td>\n      <td>-0.143255</td>\n      <td>0.062297</td>\n      <td>0.023841</td>\n      <td>-0.128432</td>\n      <td>0.051341</td>\n      <td>-0.025929</td>\n      <td>0.025330</td>\n    </tr>\n    <tr>\n      <th>fun4_2</th>\n      <td>0.073694</td>\n      <td>0.020063</td>\n      <td>0.039591</td>\n      <td>0.317603</td>\n      <td>0.021948</td>\n      <td>-0.049850</td>\n      <td>-0.016466</td>\n      <td>0.016636</td>\n      <td>0.008282</td>\n      <td>0.329761</td>\n      <td>...</td>\n      <td>0.023175</td>\n      <td>-0.094106</td>\n      <td>-0.031029</td>\n      <td>-0.120842</td>\n      <td>0.030319</td>\n      <td>0.195694</td>\n      <td>0.129897</td>\n      <td>0.116196</td>\n      <td>-0.057922</td>\n      <td>0.014238</td>\n    </tr>\n    <tr>\n      <th>amb4_2</th>\n      <td>-0.213561</td>\n      <td>-0.007788</td>\n      <td>0.000135</td>\n      <td>0.070735</td>\n      <td>-0.057689</td>\n      <td>-0.050490</td>\n      <td>-0.062375</td>\n      <td>-0.039256</td>\n      <td>-0.029746</td>\n      <td>0.104471</td>\n      <td>...</td>\n      <td>0.077059</td>\n      <td>-0.071351</td>\n      <td>-0.121340</td>\n      <td>-0.095546</td>\n      <td>-0.046012</td>\n      <td>-0.047679</td>\n      <td>-0.020109</td>\n      <td>-0.062717</td>\n      <td>-0.074417</td>\n      <td>-0.005088</td>\n    </tr>\n    <tr>\n      <th>shar4_2</th>\n      <td>-0.062882</td>\n      <td>0.028333</td>\n      <td>0.004393</td>\n      <td>0.229321</td>\n      <td>0.035801</td>\n      <td>-0.012619</td>\n      <td>0.054523</td>\n      <td>0.012502</td>\n      <td>0.003183</td>\n      <td>0.240849</td>\n      <td>...</td>\n      <td>0.010682</td>\n      <td>-0.074188</td>\n      <td>-0.104147</td>\n      <td>-0.070030</td>\n      <td>0.003800</td>\n      <td>0.096599</td>\n      <td>0.039976</td>\n      <td>0.030900</td>\n      <td>-0.016677</td>\n      <td>0.000286</td>\n    </tr>\n    <tr>\n      <th>attr2_2</th>\n      <td>-0.249641</td>\n      <td>0.032125</td>\n      <td>0.021666</td>\n      <td>0.364277</td>\n      <td>0.081487</td>\n      <td>0.109483</td>\n      <td>0.057990</td>\n      <td>0.026639</td>\n      <td>0.033129</td>\n      <td>0.390561</td>\n      <td>...</td>\n      <td>0.014207</td>\n      <td>-0.264648</td>\n      <td>-0.018316</td>\n      <td>-0.129391</td>\n      <td>0.003390</td>\n      <td>0.199671</td>\n      <td>-0.050254</td>\n      <td>0.076758</td>\n      <td>-0.012195</td>\n      <td>0.008762</td>\n    </tr>\n    <tr>\n      <th>sinc2_2</th>\n      <td>0.282042</td>\n      <td>-0.036260</td>\n      <td>0.001673</td>\n      <td>-0.117161</td>\n      <td>-0.068704</td>\n      <td>-0.079203</td>\n      <td>-0.036794</td>\n      <td>-0.026157</td>\n      <td>-0.032839</td>\n      <td>-0.144230</td>\n      <td>...</td>\n      <td>-0.059786</td>\n      <td>0.057889</td>\n      <td>0.042891</td>\n      <td>-0.031762</td>\n      <td>-0.037131</td>\n      <td>-0.263691</td>\n      <td>-0.011343</td>\n      <td>-0.081519</td>\n      <td>-0.036465</td>\n      <td>-0.003652</td>\n    </tr>\n    <tr>\n      <th>intel2_2</th>\n      <td>0.262282</td>\n      <td>-0.014203</td>\n      <td>-0.023484</td>\n      <td>-0.272287</td>\n      <td>-0.079662</td>\n      <td>-0.046209</td>\n      <td>-0.057347</td>\n      <td>-0.034406</td>\n      <td>-0.028891</td>\n      <td>-0.294044</td>\n      <td>...</td>\n      <td>-0.068325</td>\n      <td>0.243352</td>\n      <td>0.045944</td>\n      <td>0.269488</td>\n      <td>0.041522</td>\n      <td>-0.173493</td>\n      <td>0.144448</td>\n      <td>-0.092637</td>\n      <td>0.230910</td>\n      <td>-0.022810</td>\n    </tr>\n    <tr>\n      <th>fun2_2</th>\n      <td>-0.125038</td>\n      <td>0.004210</td>\n      <td>-0.016848</td>\n      <td>0.023925</td>\n      <td>-0.004139</td>\n      <td>-0.105188</td>\n      <td>-0.056649</td>\n      <td>-0.002939</td>\n      <td>0.009590</td>\n      <td>0.034163</td>\n      <td>...</td>\n      <td>0.073385</td>\n      <td>0.038466</td>\n      <td>-0.028343</td>\n      <td>-0.077306</td>\n      <td>0.175831</td>\n      <td>0.146663</td>\n      <td>-0.077671</td>\n      <td>0.001042</td>\n      <td>-0.109821</td>\n      <td>0.006237</td>\n    </tr>\n    <tr>\n      <th>amb2_2</th>\n      <td>0.363901</td>\n      <td>-0.039150</td>\n      <td>-0.035504</td>\n      <td>-0.310607</td>\n      <td>-0.070813</td>\n      <td>0.009670</td>\n      <td>0.038334</td>\n      <td>-0.008177</td>\n      <td>-0.028579</td>\n      <td>-0.343063</td>\n      <td>...</td>\n      <td>0.012500</td>\n      <td>0.131631</td>\n      <td>-0.000921</td>\n      <td>0.088803</td>\n      <td>-0.231054</td>\n      <td>-0.161803</td>\n      <td>-0.017971</td>\n      <td>0.006171</td>\n      <td>0.032362</td>\n      <td>-0.007276</td>\n    </tr>\n    <tr>\n      <th>shar2_2</th>\n      <td>-0.169447</td>\n      <td>0.016351</td>\n      <td>0.032716</td>\n      <td>-0.231347</td>\n      <td>0.035671</td>\n      <td>-0.039712</td>\n      <td>-0.017535</td>\n      <td>0.009599</td>\n      <td>0.003964</td>\n      <td>-0.222360</td>\n      <td>...</td>\n      <td>0.018925</td>\n      <td>0.144862</td>\n      <td>-0.001040</td>\n      <td>0.079423</td>\n      <td>0.100580</td>\n      <td>0.048894</td>\n      <td>0.112721</td>\n      <td>0.019140</td>\n      <td>-0.061706</td>\n      <td>0.003793</td>\n    </tr>\n    <tr>\n      <th>attr3_2</th>\n      <td>-0.083599</td>\n      <td>-0.013440</td>\n      <td>0.011033</td>\n      <td>0.008712</td>\n      <td>0.036352</td>\n      <td>-0.001342</td>\n      <td>0.002049</td>\n      <td>0.018473</td>\n      <td>0.008956</td>\n      <td>0.009011</td>\n      <td>...</td>\n      <td>0.283241</td>\n      <td>0.449818</td>\n      <td>0.488424</td>\n      <td>0.397723</td>\n      <td>0.778450</td>\n      <td>0.176612</td>\n      <td>0.267179</td>\n      <td>0.360296</td>\n      <td>0.145390</td>\n      <td>-0.005457</td>\n    </tr>\n    <tr>\n      <th>sinc3_2</th>\n      <td>-0.140321</td>\n      <td>0.009154</td>\n      <td>0.043116</td>\n      <td>0.030243</td>\n      <td>0.012534</td>\n      <td>-0.028218</td>\n      <td>-0.134389</td>\n      <td>-0.014251</td>\n      <td>0.000096</td>\n      <td>0.037178</td>\n      <td>...</td>\n      <td>0.701755</td>\n      <td>0.245663</td>\n      <td>0.180129</td>\n      <td>0.191822</td>\n      <td>0.125471</td>\n      <td>0.615330</td>\n      <td>0.272623</td>\n      <td>0.189890</td>\n      <td>0.150855</td>\n      <td>-0.015575</td>\n    </tr>\n    <tr>\n      <th>intel3_2</th>\n      <td>0.095417</td>\n      <td>-0.005050</td>\n      <td>0.049328</td>\n      <td>-0.005593</td>\n      <td>0.057628</td>\n      <td>0.057750</td>\n      <td>0.026342</td>\n      <td>0.020453</td>\n      <td>0.023230</td>\n      <td>-0.014879</td>\n      <td>...</td>\n      <td>0.345728</td>\n      <td>0.721426</td>\n      <td>0.386880</td>\n      <td>0.422864</td>\n      <td>0.271990</td>\n      <td>0.212728</td>\n      <td>0.673059</td>\n      <td>0.189852</td>\n      <td>0.249855</td>\n      <td>0.006043</td>\n    </tr>\n    <tr>\n      <th>fun3_2</th>\n      <td>-0.116650</td>\n      <td>0.008124</td>\n      <td>0.008403</td>\n      <td>0.070527</td>\n      <td>0.003904</td>\n      <td>-0.050530</td>\n      <td>-0.076781</td>\n      <td>-0.022651</td>\n      <td>-0.001706</td>\n      <td>0.077710</td>\n      <td>...</td>\n      <td>0.227269</td>\n      <td>0.270928</td>\n      <td>0.806554</td>\n      <td>0.391272</td>\n      <td>0.308667</td>\n      <td>0.175207</td>\n      <td>0.134801</td>\n      <td>0.605386</td>\n      <td>0.235961</td>\n      <td>-0.016433</td>\n    </tr>\n    <tr>\n      <th>amb3_2</th>\n      <td>-0.007924</td>\n      <td>0.039601</td>\n      <td>0.040339</td>\n      <td>0.091290</td>\n      <td>0.054940</td>\n      <td>0.077231</td>\n      <td>-0.001756</td>\n      <td>0.016422</td>\n      <td>0.018675</td>\n      <td>0.090609</td>\n      <td>...</td>\n      <td>0.308090</td>\n      <td>0.329965</td>\n      <td>0.406224</td>\n      <td>0.810285</td>\n      <td>0.146067</td>\n      <td>0.077369</td>\n      <td>0.208075</td>\n      <td>0.081199</td>\n      <td>0.413743</td>\n      <td>-0.004165</td>\n    </tr>\n    <tr>\n      <th>attr5_2</th>\n      <td>-0.001287</td>\n      <td>-0.021981</td>\n      <td>-0.000160</td>\n      <td>-0.048536</td>\n      <td>0.017414</td>\n      <td>-0.039660</td>\n      <td>-0.037168</td>\n      <td>0.011537</td>\n      <td>-0.005290</td>\n      <td>-0.052292</td>\n      <td>...</td>\n      <td>0.144200</td>\n      <td>0.319702</td>\n      <td>0.366612</td>\n      <td>0.155311</td>\n      <td>0.832708</td>\n      <td>0.124502</td>\n      <td>0.231306</td>\n      <td>0.376704</td>\n      <td>0.068421</td>\n      <td>-0.018410</td>\n    </tr>\n    <tr>\n      <th>sinc5_2</th>\n      <td>-0.178150</td>\n      <td>-0.061233</td>\n      <td>-0.042388</td>\n      <td>-0.072139</td>\n      <td>-0.069566</td>\n      <td>-0.063697</td>\n      <td>-0.138315</td>\n      <td>-0.043010</td>\n      <td>-0.030502</td>\n      <td>-0.052526</td>\n      <td>...</td>\n      <td>0.517034</td>\n      <td>0.275302</td>\n      <td>0.188608</td>\n      <td>-0.038665</td>\n      <td>0.127302</td>\n      <td>0.637660</td>\n      <td>0.339848</td>\n      <td>0.187536</td>\n      <td>0.058967</td>\n      <td>0.004089</td>\n    </tr>\n    <tr>\n      <th>intel5_2</th>\n      <td>0.054945</td>\n      <td>-0.130879</td>\n      <td>-0.001262</td>\n      <td>-0.044116</td>\n      <td>0.025495</td>\n      <td>0.014985</td>\n      <td>-0.049397</td>\n      <td>0.000735</td>\n      <td>0.013367</td>\n      <td>-0.055936</td>\n      <td>...</td>\n      <td>0.217510</td>\n      <td>0.455876</td>\n      <td>0.225983</td>\n      <td>0.159733</td>\n      <td>0.274736</td>\n      <td>0.408256</td>\n      <td>0.650015</td>\n      <td>0.280329</td>\n      <td>0.252123</td>\n      <td>0.002299</td>\n    </tr>\n    <tr>\n      <th>fun5_2</th>\n      <td>-0.009537</td>\n      <td>0.025262</td>\n      <td>0.048333</td>\n      <td>0.005875</td>\n      <td>0.023722</td>\n      <td>-0.056221</td>\n      <td>-0.064562</td>\n      <td>-0.008307</td>\n      <td>0.006360</td>\n      <td>0.009758</td>\n      <td>...</td>\n      <td>0.159925</td>\n      <td>0.224323</td>\n      <td>0.678271</td>\n      <td>0.323849</td>\n      <td>0.264236</td>\n      <td>0.217260</td>\n      <td>0.144337</td>\n      <td>0.656616</td>\n      <td>0.242940</td>\n      <td>-0.012951</td>\n    </tr>\n    <tr>\n      <th>amb5_2</th>\n      <td>0.064466</td>\n      <td>-0.057714</td>\n      <td>0.017924</td>\n      <td>-0.000992</td>\n      <td>0.013475</td>\n      <td>0.044112</td>\n      <td>-0.058697</td>\n      <td>-0.003514</td>\n      <td>0.004763</td>\n      <td>-0.012435</td>\n      <td>...</td>\n      <td>0.179030</td>\n      <td>0.211334</td>\n      <td>0.407815</td>\n      <td>0.567315</td>\n      <td>0.139705</td>\n      <td>0.207738</td>\n      <td>0.263271</td>\n      <td>0.279301</td>\n      <td>0.567598</td>\n      <td>0.003009</td>\n    </tr>\n    <tr>\n      <th>you_call</th>\n      <td>0.321151</td>\n      <td>-0.028005</td>\n      <td>0.028137</td>\n      <td>0.069609</td>\n      <td>0.029863</td>\n      <td>0.021048</td>\n      <td>0.004019</td>\n      <td>0.007800</td>\n      <td>0.031776</td>\n      <td>0.053962</td>\n      <td>...</td>\n      <td>0.028763</td>\n      <td>0.026624</td>\n      <td>0.038245</td>\n      <td>0.076199</td>\n      <td>-0.100722</td>\n      <td>0.070509</td>\n      <td>0.006323</td>\n      <td>-0.033755</td>\n      <td>0.177991</td>\n      <td>0.004126</td>\n    </tr>\n    <tr>\n      <th>them_cal</th>\n      <td>-0.267411</td>\n      <td>0.090237</td>\n      <td>0.166501</td>\n      <td>0.095790</td>\n      <td>0.163257</td>\n      <td>0.014472</td>\n      <td>0.037554</td>\n      <td>0.061944</td>\n      <td>0.065983</td>\n      <td>0.114239</td>\n      <td>...</td>\n      <td>0.010946</td>\n      <td>-0.021943</td>\n      <td>0.108908</td>\n      <td>0.014056</td>\n      <td>0.176633</td>\n      <td>-0.097311</td>\n      <td>-0.177227</td>\n      <td>0.102825</td>\n      <td>-0.089334</td>\n      <td>-0.013923</td>\n    </tr>\n    <tr>\n      <th>date_3</th>\n      <td>0.007755</td>\n      <td>0.057249</td>\n      <td>0.120132</td>\n      <td>0.114185</td>\n      <td>0.093302</td>\n      <td>0.034538</td>\n      <td>-0.003920</td>\n      <td>0.045446</td>\n      <td>0.044604</td>\n      <td>0.114708</td>\n      <td>...</td>\n      <td>-0.051228</td>\n      <td>-0.080365</td>\n      <td>-0.063376</td>\n      <td>-0.041458</td>\n      <td>0.038475</td>\n      <td>-0.034999</td>\n      <td>-0.072381</td>\n      <td>0.018261</td>\n      <td>0.034839</td>\n      <td>-0.045999</td>\n    </tr>\n    <tr>\n      <th>numdat_3</th>\n      <td>0.003680</td>\n      <td>0.196831</td>\n      <td>0.061342</td>\n      <td>-0.183618</td>\n      <td>0.135128</td>\n      <td>-0.006652</td>\n      <td>0.004385</td>\n      <td>0.033122</td>\n      <td>0.069489</td>\n      <td>-0.181726</td>\n      <td>...</td>\n      <td>-0.005622</td>\n      <td>0.048642</td>\n      <td>-0.033306</td>\n      <td>0.090593</td>\n      <td>0.018597</td>\n      <td>-0.058040</td>\n      <td>-0.080342</td>\n      <td>0.076046</td>\n      <td>-0.059301</td>\n      <td>0.002815</td>\n    </tr>\n    <tr>\n      <th>num_in_3</th>\n      <td>0.251143</td>\n      <td>0.016424</td>\n      <td>0.257876</td>\n      <td>0.173421</td>\n      <td>0.245137</td>\n      <td>-0.076935</td>\n      <td>0.033308</td>\n      <td>0.075358</td>\n      <td>0.091934</td>\n      <td>0.157377</td>\n      <td>...</td>\n      <td>0.010967</td>\n      <td>0.113989</td>\n      <td>0.158014</td>\n      <td>-0.002839</td>\n      <td>0.083457</td>\n      <td>-0.006671</td>\n      <td>-0.039418</td>\n      <td>-0.021460</td>\n      <td>-0.133550</td>\n      <td>-0.000119</td>\n    </tr>\n    <tr>\n      <th>attr1_3</th>\n      <td>0.291946</td>\n      <td>0.052652</td>\n      <td>-0.003305</td>\n      <td>0.091458</td>\n      <td>-0.005465</td>\n      <td>0.021833</td>\n      <td>0.025084</td>\n      <td>0.000844</td>\n      <td>-0.006415</td>\n      <td>0.088425</td>\n      <td>...</td>\n      <td>-0.194855</td>\n      <td>-0.115881</td>\n      <td>-0.013441</td>\n      <td>-0.020330</td>\n      <td>0.151984</td>\n      <td>-0.183062</td>\n      <td>-0.002282</td>\n      <td>0.071728</td>\n      <td>0.070902</td>\n      <td>0.001324</td>\n    </tr>\n    <tr>\n      <th>sinc1_3</th>\n      <td>-0.095523</td>\n      <td>-0.068116</td>\n      <td>-0.030393</td>\n      <td>0.033867</td>\n      <td>-0.000397</td>\n      <td>-0.067908</td>\n      <td>-0.095138</td>\n      <td>-0.013001</td>\n      <td>0.006763</td>\n      <td>0.036280</td>\n      <td>...</td>\n      <td>0.266495</td>\n      <td>-0.027765</td>\n      <td>-0.109881</td>\n      <td>-0.151111</td>\n      <td>-0.143629</td>\n      <td>0.172087</td>\n      <td>-0.072747</td>\n      <td>-0.043329</td>\n      <td>-0.132296</td>\n      <td>-0.013299</td>\n    </tr>\n    <tr>\n      <th>intel1_3</th>\n      <td>-0.043765</td>\n      <td>0.062598</td>\n      <td>-0.014441</td>\n      <td>-0.133639</td>\n      <td>-0.040050</td>\n      <td>0.035103</td>\n      <td>0.019810</td>\n      <td>-0.008996</td>\n      <td>-0.023158</td>\n      <td>-0.129687</td>\n      <td>...</td>\n      <td>-0.083965</td>\n      <td>0.060835</td>\n      <td>-0.179574</td>\n      <td>-0.117099</td>\n      <td>0.063085</td>\n      <td>-0.001316</td>\n      <td>0.181427</td>\n      <td>-0.185431</td>\n      <td>-0.057076</td>\n      <td>0.011455</td>\n    </tr>\n    <tr>\n      <th>fun1_3</th>\n      <td>0.073421</td>\n      <td>-0.029183</td>\n      <td>0.061891</td>\n      <td>-0.054681</td>\n      <td>0.067762</td>\n      <td>0.058265</td>\n      <td>0.178941</td>\n      <td>0.036429</td>\n      <td>0.044068</td>\n      <td>-0.064903</td>\n      <td>...</td>\n      <td>0.005032</td>\n      <td>0.094166</td>\n      <td>0.332448</td>\n      <td>0.097415</td>\n      <td>-0.011183</td>\n      <td>0.004676</td>\n      <td>0.010281</td>\n      <td>0.222417</td>\n      <td>0.087981</td>\n      <td>0.000125</td>\n    </tr>\n    <tr>\n      <th>amb1_3</th>\n      <td>-0.279596</td>\n      <td>-0.036053</td>\n      <td>0.003579</td>\n      <td>-0.121360</td>\n      <td>0.010185</td>\n      <td>-0.024862</td>\n      <td>-0.106862</td>\n      <td>0.002551</td>\n      <td>0.001750</td>\n      <td>-0.118295</td>\n      <td>...</td>\n      <td>0.136103</td>\n      <td>0.139973</td>\n      <td>0.106014</td>\n      <td>0.361924</td>\n      <td>-0.163332</td>\n      <td>-0.094961</td>\n      <td>-0.144967</td>\n      <td>-0.193320</td>\n      <td>0.074328</td>\n      <td>0.004359</td>\n    </tr>\n    <tr>\n      <th>shar1_3</th>\n      <td>-0.240175</td>\n      <td>0.011663</td>\n      <td>0.025050</td>\n      <td>0.021159</td>\n      <td>0.024254</td>\n      <td>-0.016526</td>\n      <td>0.008345</td>\n      <td>0.006491</td>\n      <td>-0.001156</td>\n      <td>0.024263</td>\n      <td>...</td>\n      <td>0.051439</td>\n      <td>0.002686</td>\n      <td>-0.008022</td>\n      <td>-0.063536</td>\n      <td>-0.051587</td>\n      <td>0.128463</td>\n      <td>0.033021</td>\n      <td>0.019191</td>\n      <td>-0.083669</td>\n      <td>-0.007135</td>\n    </tr>\n    <tr>\n      <th>attr7_3</th>\n      <td>0.265643</td>\n      <td>0.050812</td>\n      <td>0.044714</td>\n      <td>0.118919</td>\n      <td>0.008609</td>\n      <td>0.084423</td>\n      <td>0.062109</td>\n      <td>-0.004360</td>\n      <td>0.008913</td>\n      <td>0.106994</td>\n      <td>...</td>\n      <td>-0.269991</td>\n      <td>-0.037334</td>\n      <td>0.091908</td>\n      <td>0.001703</td>\n      <td>0.120429</td>\n      <td>-0.101053</td>\n      <td>0.049591</td>\n      <td>0.073153</td>\n      <td>0.115004</td>\n      <td>-0.023678</td>\n    </tr>\n    <tr>\n      <th>sinc7_3</th>\n      <td>-0.299939</td>\n      <td>-0.114605</td>\n      <td>-0.101834</td>\n      <td>-0.087408</td>\n      <td>-0.087175</td>\n      <td>-0.144410</td>\n      <td>-0.142299</td>\n      <td>-0.040043</td>\n      <td>-0.037115</td>\n      <td>-0.064473</td>\n      <td>...</td>\n      <td>0.376003</td>\n      <td>0.028562</td>\n      <td>-0.049370</td>\n      <td>-0.133591</td>\n      <td>-0.070949</td>\n      <td>0.194890</td>\n      <td>-0.081343</td>\n      <td>0.010862</td>\n      <td>-0.161255</td>\n      <td>-0.014658</td>\n    </tr>\n    <tr>\n      <th>intel7_3</th>\n      <td>0.065740</td>\n      <td>0.062704</td>\n      <td>-0.042053</td>\n      <td>-0.035181</td>\n      <td>-0.042486</td>\n      <td>0.022261</td>\n      <td>-0.066124</td>\n      <td>0.006698</td>\n      <td>-0.021499</td>\n      <td>-0.046402</td>\n      <td>...</td>\n      <td>0.005997</td>\n      <td>0.054931</td>\n      <td>-0.149483</td>\n      <td>0.105161</td>\n      <td>-0.059777</td>\n      <td>-0.081319</td>\n      <td>0.054450</td>\n      <td>-0.260065</td>\n      <td>0.027736</td>\n      <td>-0.006156</td>\n    </tr>\n    <tr>\n      <th>fun7_3</th>\n      <td>0.163306</td>\n      <td>0.089223</td>\n      <td>0.036073</td>\n      <td>-0.030592</td>\n      <td>0.166911</td>\n      <td>0.033813</td>\n      <td>0.126571</td>\n      <td>0.069327</td>\n      <td>0.070966</td>\n      <td>-0.059019</td>\n      <td>...</td>\n      <td>0.002913</td>\n      <td>0.105665</td>\n      <td>0.190674</td>\n      <td>-0.055382</td>\n      <td>0.093876</td>\n      <td>0.054805</td>\n      <td>-0.055669</td>\n      <td>0.255162</td>\n      <td>-0.059321</td>\n      <td>0.035256</td>\n    </tr>\n    <tr>\n      <th>amb7_3</th>\n      <td>-0.274540</td>\n      <td>-0.031118</td>\n      <td>-0.061534</td>\n      <td>-0.181025</td>\n      <td>-0.038347</td>\n      <td>-0.004403</td>\n      <td>-0.015219</td>\n      <td>-0.005659</td>\n      <td>-0.019524</td>\n      <td>-0.158031</td>\n      <td>...</td>\n      <td>0.056967</td>\n      <td>-0.131288</td>\n      <td>-0.200213</td>\n      <td>0.104107</td>\n      <td>-0.184484</td>\n      <td>-0.060530</td>\n      <td>-0.020241</td>\n      <td>-0.202434</td>\n      <td>-0.012873</td>\n      <td>0.011289</td>\n    </tr>\n    <tr>\n      <th>shar7_3</th>\n      <td>-0.186516</td>\n      <td>-0.072238</td>\n      <td>0.048955</td>\n      <td>-0.044943</td>\n      <td>0.018906</td>\n      <td>-0.064925</td>\n      <td>-0.015383</td>\n      <td>-0.002368</td>\n      <td>-0.000267</td>\n      <td>-0.033759</td>\n      <td>...</td>\n      <td>0.044720</td>\n      <td>-0.063683</td>\n      <td>-0.049209</td>\n      <td>-0.060220</td>\n      <td>-0.059918</td>\n      <td>0.005215</td>\n      <td>-0.030004</td>\n      <td>-0.023135</td>\n      <td>-0.083565</td>\n      <td>0.008386</td>\n    </tr>\n    <tr>\n      <th>attr4_3</th>\n      <td>0.094793</td>\n      <td>0.174953</td>\n      <td>0.072068</td>\n      <td>0.584029</td>\n      <td>0.073596</td>\n      <td>0.031114</td>\n      <td>0.014891</td>\n      <td>0.009074</td>\n      <td>0.015907</td>\n      <td>0.590255</td>\n      <td>...</td>\n      <td>-0.110277</td>\n      <td>-0.237046</td>\n      <td>-0.042730</td>\n      <td>-0.093987</td>\n      <td>0.124561</td>\n      <td>-0.026355</td>\n      <td>0.043986</td>\n      <td>0.088047</td>\n      <td>0.081528</td>\n      <td>-0.020716</td>\n    </tr>\n    <tr>\n      <th>sinc4_3</th>\n      <td>-0.239556</td>\n      <td>-0.085686</td>\n      <td>-0.048438</td>\n      <td>0.199146</td>\n      <td>-0.056157</td>\n      <td>-0.142264</td>\n      <td>-0.101457</td>\n      <td>-0.048693</td>\n      <td>-0.034430</td>\n      <td>0.229317</td>\n      <td>...</td>\n      <td>0.070964</td>\n      <td>-0.166425</td>\n      <td>-0.094704</td>\n      <td>-0.116336</td>\n      <td>-0.112648</td>\n      <td>0.009514</td>\n      <td>-0.078527</td>\n      <td>-0.158964</td>\n      <td>-0.024962</td>\n      <td>-0.005087</td>\n    </tr>\n    <tr>\n      <th>intel4_3</th>\n      <td>0.039176</td>\n      <td>-0.002741</td>\n      <td>-0.037550</td>\n      <td>0.307739</td>\n      <td>-0.039640</td>\n      <td>-0.060468</td>\n      <td>-0.004987</td>\n      <td>-0.011341</td>\n      <td>-0.032447</td>\n      <td>0.318122</td>\n      <td>...</td>\n      <td>-0.102617</td>\n      <td>-0.160289</td>\n      <td>-0.061133</td>\n      <td>-0.106454</td>\n      <td>0.017918</td>\n      <td>-0.255931</td>\n      <td>-0.029460</td>\n      <td>-0.165309</td>\n      <td>0.011712</td>\n      <td>-0.002979</td>\n    </tr>\n    <tr>\n      <th>fun4_3</th>\n      <td>0.078254</td>\n      <td>-0.078506</td>\n      <td>0.020939</td>\n      <td>0.372885</td>\n      <td>0.082495</td>\n      <td>0.055664</td>\n      <td>0.125276</td>\n      <td>0.028365</td>\n      <td>0.025944</td>\n      <td>0.378940</td>\n      <td>...</td>\n      <td>0.011046</td>\n      <td>-0.125946</td>\n      <td>-0.028086</td>\n      <td>-0.143835</td>\n      <td>-0.073608</td>\n      <td>0.215811</td>\n      <td>0.062933</td>\n      <td>0.167107</td>\n      <td>0.006093</td>\n      <td>0.033108</td>\n    </tr>\n    <tr>\n      <th>amb4_3</th>\n      <td>-0.213160</td>\n      <td>-0.000157</td>\n      <td>-0.009982</td>\n      <td>0.116946</td>\n      <td>-0.101453</td>\n      <td>-0.004656</td>\n      <td>0.025779</td>\n      <td>-0.057178</td>\n      <td>-0.041156</td>\n      <td>0.148365</td>\n      <td>...</td>\n      <td>0.021287</td>\n      <td>-0.046057</td>\n      <td>-0.062640</td>\n      <td>-0.017990</td>\n      <td>-0.020759</td>\n      <td>0.032481</td>\n      <td>-0.002663</td>\n      <td>-0.009322</td>\n      <td>-0.077136</td>\n      <td>-0.021670</td>\n    </tr>\n    <tr>\n      <th>shar4_3</th>\n      <td>-0.056307</td>\n      <td>-0.056162</td>\n      <td>0.093551</td>\n      <td>0.271893</td>\n      <td>0.102718</td>\n      <td>0.015256</td>\n      <td>0.094471</td>\n      <td>0.038042</td>\n      <td>0.039546</td>\n      <td>0.280248</td>\n      <td>...</td>\n      <td>-0.059340</td>\n      <td>-0.134786</td>\n      <td>-0.035968</td>\n      <td>-0.101789</td>\n      <td>-0.026038</td>\n      <td>0.044714</td>\n      <td>-0.023516</td>\n      <td>0.005492</td>\n      <td>-0.057441</td>\n      <td>0.003276</td>\n    </tr>\n    <tr>\n      <th>attr2_3</th>\n      <td>-0.181189</td>\n      <td>0.122896</td>\n      <td>0.057393</td>\n      <td>0.576042</td>\n      <td>0.038028</td>\n      <td>-0.007567</td>\n      <td>-0.010926</td>\n      <td>-0.015732</td>\n      <td>0.005954</td>\n      <td>0.607683</td>\n      <td>...</td>\n      <td>-0.016681</td>\n      <td>-0.257133</td>\n      <td>-0.035511</td>\n      <td>-0.139484</td>\n      <td>0.161679</td>\n      <td>0.147940</td>\n      <td>-0.058050</td>\n      <td>0.139143</td>\n      <td>0.021694</td>\n      <td>-0.014799</td>\n    </tr>\n    <tr>\n      <th>sinc2_3</th>\n      <td>0.154823</td>\n      <td>-0.030499</td>\n      <td>-0.060318</td>\n      <td>0.246075</td>\n      <td>-0.049210</td>\n      <td>-0.048717</td>\n      <td>-0.026804</td>\n      <td>-0.035766</td>\n      <td>-0.036746</td>\n      <td>0.241499</td>\n      <td>...</td>\n      <td>-0.064288</td>\n      <td>-0.111796</td>\n      <td>-0.060138</td>\n      <td>-0.116035</td>\n      <td>-0.116175</td>\n      <td>-0.152146</td>\n      <td>-0.013163</td>\n      <td>-0.056954</td>\n      <td>-0.121687</td>\n      <td>0.004638</td>\n    </tr>\n    <tr>\n      <th>intel2_3</th>\n      <td>0.237649</td>\n      <td>0.059922</td>\n      <td>-0.010001</td>\n      <td>0.324646</td>\n      <td>0.016105</td>\n      <td>-0.002853</td>\n      <td>0.030367</td>\n      <td>0.011661</td>\n      <td>-0.008189</td>\n      <td>0.309253</td>\n      <td>...</td>\n      <td>-0.153843</td>\n      <td>-0.125591</td>\n      <td>0.014203</td>\n      <td>-0.027926</td>\n      <td>0.017131</td>\n      <td>-0.257159</td>\n      <td>0.035891</td>\n      <td>-0.108908</td>\n      <td>0.131485</td>\n      <td>-0.011456</td>\n    </tr>\n    <tr>\n      <th>fun2_3</th>\n      <td>-0.149604</td>\n      <td>-0.092149</td>\n      <td>0.010686</td>\n      <td>0.396995</td>\n      <td>0.022080</td>\n      <td>-0.012413</td>\n      <td>0.058210</td>\n      <td>-0.000422</td>\n      <td>0.008930</td>\n      <td>0.427383</td>\n      <td>...</td>\n      <td>-0.013369</td>\n      <td>-0.172522</td>\n      <td>-0.139070</td>\n      <td>-0.127763</td>\n      <td>0.076953</td>\n      <td>0.138838</td>\n      <td>0.031338</td>\n      <td>-0.019293</td>\n      <td>-0.002714</td>\n      <td>0.006694</td>\n    </tr>\n    <tr>\n      <th>amb2_3</th>\n      <td>0.362593</td>\n      <td>-0.084710</td>\n      <td>0.017754</td>\n      <td>-0.003061</td>\n      <td>0.017948</td>\n      <td>0.040691</td>\n      <td>0.045706</td>\n      <td>0.022774</td>\n      <td>0.013225</td>\n      <td>-0.021860</td>\n      <td>...</td>\n      <td>-0.047040</td>\n      <td>-0.047872</td>\n      <td>-0.103486</td>\n      <td>-0.009560</td>\n      <td>-0.272513</td>\n      <td>-0.023040</td>\n      <td>0.063912</td>\n      <td>-0.049271</td>\n      <td>0.080480</td>\n      <td>-0.007382</td>\n    </tr>\n    <tr>\n      <th>shar2_3</th>\n      <td>-0.252469</td>\n      <td>0.017551</td>\n      <td>0.055328</td>\n      <td>-0.095389</td>\n      <td>0.027282</td>\n      <td>0.009081</td>\n      <td>-0.000387</td>\n      <td>0.016610</td>\n      <td>-0.009307</td>\n      <td>-0.075240</td>\n      <td>...</td>\n      <td>0.033459</td>\n      <td>-0.071692</td>\n      <td>-0.077305</td>\n      <td>-0.032421</td>\n      <td>-0.027785</td>\n      <td>0.005579</td>\n      <td>-0.033531</td>\n      <td>-0.044611</td>\n      <td>-0.136852</td>\n      <td>0.022419</td>\n    </tr>\n    <tr>\n      <th>attr3_3</th>\n      <td>-0.150992</td>\n      <td>0.011633</td>\n      <td>0.087388</td>\n      <td>-0.001678</td>\n      <td>0.094818</td>\n      <td>0.060514</td>\n      <td>0.104672</td>\n      <td>0.039429</td>\n      <td>0.028673</td>\n      <td>-0.001053</td>\n      <td>...</td>\n      <td>0.400349</td>\n      <td>0.516139</td>\n      <td>0.577210</td>\n      <td>0.438404</td>\n      <td>0.858673</td>\n      <td>0.224796</td>\n      <td>0.335831</td>\n      <td>0.394979</td>\n      <td>0.194495</td>\n      <td>-0.005782</td>\n    </tr>\n    <tr>\n      <th>sinc3_3</th>\n      <td>-0.169387</td>\n      <td>-0.050350</td>\n      <td>0.044158</td>\n      <td>0.036948</td>\n      <td>0.036600</td>\n      <td>0.041865</td>\n      <td>-0.031083</td>\n      <td>0.000877</td>\n      <td>0.016141</td>\n      <td>0.040356</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.407683</td>\n      <td>0.398775</td>\n      <td>0.363356</td>\n      <td>0.174329</td>\n      <td>0.608213</td>\n      <td>0.288062</td>\n      <td>0.160204</td>\n      <td>0.135499</td>\n      <td>0.020346</td>\n    </tr>\n    <tr>\n      <th>intel3_3</th>\n      <td>0.011476</td>\n      <td>-0.060674</td>\n      <td>0.061796</td>\n      <td>-0.104562</td>\n      <td>0.022696</td>\n      <td>0.066377</td>\n      <td>0.040163</td>\n      <td>0.002350</td>\n      <td>0.012612</td>\n      <td>-0.113803</td>\n      <td>...</td>\n      <td>0.407683</td>\n      <td>1.000000</td>\n      <td>0.440359</td>\n      <td>0.527568</td>\n      <td>0.421117</td>\n      <td>0.315857</td>\n      <td>0.683871</td>\n      <td>0.295997</td>\n      <td>0.353093</td>\n      <td>0.004193</td>\n    </tr>\n    <tr>\n      <th>fun3_3</th>\n      <td>-0.153701</td>\n      <td>-0.041080</td>\n      <td>0.069298</td>\n      <td>0.053971</td>\n      <td>0.059530</td>\n      <td>0.043142</td>\n      <td>0.073800</td>\n      <td>0.006350</td>\n      <td>0.021345</td>\n      <td>0.056667</td>\n      <td>...</td>\n      <td>0.398775</td>\n      <td>0.440359</td>\n      <td>1.000000</td>\n      <td>0.533005</td>\n      <td>0.410789</td>\n      <td>0.180240</td>\n      <td>0.240322</td>\n      <td>0.771447</td>\n      <td>0.429332</td>\n      <td>0.000966</td>\n    </tr>\n    <tr>\n      <th>amb3_3</th>\n      <td>-0.066626</td>\n      <td>-0.005502</td>\n      <td>0.027447</td>\n      <td>0.000520</td>\n      <td>0.013107</td>\n      <td>0.054227</td>\n      <td>-0.024003</td>\n      <td>0.002645</td>\n      <td>-0.000157</td>\n      <td>-0.001845</td>\n      <td>...</td>\n      <td>0.363356</td>\n      <td>0.527568</td>\n      <td>0.533005</td>\n      <td>1.000000</td>\n      <td>0.235485</td>\n      <td>0.122856</td>\n      <td>0.268646</td>\n      <td>0.331533</td>\n      <td>0.619087</td>\n      <td>-0.003579</td>\n    </tr>\n    <tr>\n      <th>attr5_3</th>\n      <td>-0.133302</td>\n      <td>0.001764</td>\n      <td>0.077571</td>\n      <td>0.092556</td>\n      <td>0.034647</td>\n      <td>0.066300</td>\n      <td>0.201598</td>\n      <td>0.018610</td>\n      <td>0.002868</td>\n      <td>0.108400</td>\n      <td>...</td>\n      <td>0.174329</td>\n      <td>0.421117</td>\n      <td>0.410789</td>\n      <td>0.235485</td>\n      <td>1.000000</td>\n      <td>0.190198</td>\n      <td>0.355876</td>\n      <td>0.412403</td>\n      <td>0.194078</td>\n      <td>-0.017013</td>\n    </tr>\n    <tr>\n      <th>sinc5_3</th>\n      <td>-0.277085</td>\n      <td>-0.018915</td>\n      <td>-0.087502</td>\n      <td>-0.001751</td>\n      <td>-0.035242</td>\n      <td>0.111991</td>\n      <td>0.125594</td>\n      <td>-0.035897</td>\n      <td>-0.002601</td>\n      <td>0.021372</td>\n      <td>...</td>\n      <td>0.608213</td>\n      <td>0.315857</td>\n      <td>0.180240</td>\n      <td>0.122856</td>\n      <td>0.190198</td>\n      <td>1.000000</td>\n      <td>0.498313</td>\n      <td>0.335823</td>\n      <td>0.261307</td>\n      <td>0.051102</td>\n    </tr>\n    <tr>\n      <th>intel5_3</th>\n      <td>0.080227</td>\n      <td>-0.093206</td>\n      <td>0.051449</td>\n      <td>0.014611</td>\n      <td>-0.012896</td>\n      <td>0.089283</td>\n      <td>0.106791</td>\n      <td>-0.004632</td>\n      <td>0.003345</td>\n      <td>0.009585</td>\n      <td>...</td>\n      <td>0.288062</td>\n      <td>0.683871</td>\n      <td>0.240322</td>\n      <td>0.268646</td>\n      <td>0.355876</td>\n      <td>0.498313</td>\n      <td>1.000000</td>\n      <td>0.262987</td>\n      <td>0.422743</td>\n      <td>0.003512</td>\n    </tr>\n    <tr>\n      <th>fun5_3</th>\n      <td>-0.065562</td>\n      <td>-0.061079</td>\n      <td>0.106486</td>\n      <td>0.088716</td>\n      <td>0.104829</td>\n      <td>0.135484</td>\n      <td>0.232412</td>\n      <td>0.016419</td>\n      <td>0.031795</td>\n      <td>0.096788</td>\n      <td>...</td>\n      <td>0.160204</td>\n      <td>0.295997</td>\n      <td>0.771447</td>\n      <td>0.331533</td>\n      <td>0.412403</td>\n      <td>0.335823</td>\n      <td>0.262987</td>\n      <td>1.000000</td>\n      <td>0.379768</td>\n      <td>0.003349</td>\n    </tr>\n    <tr>\n      <th>amb5_3</th>\n      <td>0.069091</td>\n      <td>-0.145645</td>\n      <td>0.123314</td>\n      <td>0.047841</td>\n      <td>0.092951</td>\n      <td>0.058825</td>\n      <td>0.030543</td>\n      <td>0.029174</td>\n      <td>0.050511</td>\n      <td>0.043115</td>\n      <td>...</td>\n      <td>0.135499</td>\n      <td>0.353093</td>\n      <td>0.429332</td>\n      <td>0.619087</td>\n      <td>0.194078</td>\n      <td>0.261307</td>\n      <td>0.422743</td>\n      <td>0.379768</td>\n      <td>1.000000</td>\n      <td>-0.007695</td>\n    </tr>\n    <tr>\n      <th>id</th>\n      <td>0.006340</td>\n      <td>-0.036693</td>\n      <td>-0.044675</td>\n      <td>-0.001863</td>\n      <td>-0.029700</td>\n      <td>-0.026828</td>\n      <td>-0.011816</td>\n      <td>-0.016321</td>\n      <td>0.006595</td>\n      <td>-0.002027</td>\n      <td>...</td>\n      <td>0.020346</td>\n      <td>0.004193</td>\n      <td>0.000966</td>\n      <td>-0.003579</td>\n      <td>-0.017013</td>\n      <td>0.051102</td>\n      <td>0.003512</td>\n      <td>0.003349</td>\n      <td>-0.007695</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>184 rows × 184 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrM.sort_values(by=['match'], ascending=False)['match']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.799696Z",
          "iopub.execute_input": "2023-03-29T18:55:51.800473Z",
          "iopub.status.idle": "2023-03-29T18:55:51.817133Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.800419Z",
          "shell.execute_reply": "2023-03-29T18:55:51.816188Z"
        },
        "trusted": true,
        "id": "c-A0Xg4n126U",
        "outputId": "95699d17-8320-49ac-c4a0-95b4aeea218b"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 65,
          "output_type": "execute_result",
          "data": {
            "text/plain": "match       1.000000\nlike_o      0.309646\nlike        0.303105\nfun         0.273294\nfun_o       0.272843\nattr        0.269583\nshar        0.265907\nshar_o      0.263044\nattr_o      0.261901\nprob        0.259563\nprob_o      0.249655\nthem_cal    0.220125\nnum_in_3    0.218522\nintel_o     0.177166\nintel       0.174643\nsinc        0.171329\nsinc_o      0.162143\namb_o       0.144842\namb         0.144386\ndate_3      0.140299\nmatch_es    0.135905\nexpnum      0.128741\nnumdat_3    0.112133\nyou_call    0.097424\nfun7_2      0.084025\nintel2_3    0.068444\nsatis_2     0.059951\nfun3_2      0.057146\nfun1_2      0.052199\nfun1_s      0.048188\nintel2_2    0.047783\nattr1_s     0.046976\nfun3_1      0.046222\nfun7_3      0.044944\nclubbing    0.043935\nattr5_3     0.042603\nfun1_3      0.041720\nmet         0.040648\nyoga        0.040470\ndining      0.039973\nattr3_1     0.039295\nintel1_s    0.039283\nfun3_s      0.038288\nreading     0.038097\nfun1_1      0.037434\nfun4_2      0.037005\nlength      0.035683\npf_o_fun    0.035364\nhiking      0.035160\nintel1_1    0.034846\nattr4_3     0.033567\nfun5_2      0.033365\nint_corr    0.032807\nintel7_2    0.032582\nintel4_3    0.031911\nintel7_3    0.031062\nintel1_2    0.030831\nattr4_1     0.029483\nsinc3_s     0.029421\nfun5_1      0.028768\nattr3_s     0.028540\nattr4_2     0.028289\nexphappy    0.027274\nart         0.025120\nfun3_3      0.024759\nconcerts    0.024729\nattr2_1     0.024108\nintel2_1    0.023666\nnumdat_2    0.022119\nmusic       0.022085\npf_o_att    0.020987\namb3_s      0.020871\nintel3_3    0.019756\nfun2_3      0.019113\nsports      0.017300\nattr3_2     0.017007\nattr1_1     0.016976\nfun5_3      0.015977\nattr1_3     0.014889\namb3_3      0.014811\namb2_2      0.014246\nmuseums     0.013899\namb5_1      0.013548\nfun2_2      0.013000\namb2_3      0.010906\nintel3_s    0.010219\npartner     0.009451\nattr2_3     0.008827\nintel3_2    0.007652\nattr5_1     0.007505\nexercise    0.007446\nintel4_1    0.006939\nidg         0.006630\namb3_1      0.006363\nintel1_3    0.005666\nattr5_2     0.004704\ngaming      0.004696\nshar7_3     0.004524\nsinc5_1     0.004498\nattr3_3     0.004452\nsinc5_2     0.004434\ngender      0.004293\nfun4_1      0.004210\nsamerace    0.004090\nfun4_3      0.003942\npf_o_int    0.003421\namb3_2      0.001069\nintel3_1    0.000935\nshar4_3    -0.000108\nintel5_1   -0.001774\nattr1_2    -0.002508\namb1_3     -0.002617\nattr7_2    -0.003485\nattr2_2    -0.003728\nsinc3_1    -0.004965\ntvsports   -0.005471\namb5_2     -0.005643\nshopping   -0.006235\nfun2_1     -0.006490\ngoal       -0.006510\nattr7_3    -0.006544\ntheater    -0.006642\nrace_o     -0.007264\namb2_1     -0.008597\npf_o_amb   -0.009584\nposition   -0.010135\nintel5_2   -0.010216\namb4_3     -0.010610\nintel5_3   -0.011494\npositin1   -0.012012\nsinc3_3    -0.013565\nsinc1_s    -0.014918\nsinc3_2    -0.016089\namb1_1     -0.016315\nsinc2_3    -0.017550\nshar4_1    -0.017938\nrace       -0.018013\namb4_1     -0.019179\nimprelig   -0.019250\namb5_3     -0.019651\ntv         -0.021074\nsinc2_1    -0.021082\nsinc2_2    -0.021763\namb7_2     -0.022714\nshar1_2    -0.023827\npid        -0.024160\nid         -0.024600\nage_o      -0.025561\ncareer_c   -0.025623\nwave       -0.025673\nmovies     -0.027374\nsinc7_3    -0.027681\npf_o_sin   -0.028204\namb1_2     -0.028574\namb1_s     -0.029482\nsinc1_2    -0.029965\nage        -0.030142\nsinc7_2    -0.030480\nshar1_3    -0.030952\nround      -0.031313\npf_o_sha   -0.033300\nfield_cd   -0.033304\nintel4_2   -0.034066\nshar4_2    -0.035253\namb4_2     -0.038539\nshar1_s    -0.038868\nimprace    -0.040100\nsinc1_1    -0.040464\nsinc5_3    -0.040531\nshar2_1    -0.040762\nshar2_2    -0.041763\nsinc4_3    -0.041936\ncondtn     -0.042888\nsinc1_3    -0.046103\norder      -0.046686\nshar1_1    -0.048632\namb7_3     -0.050712\nsinc4_2    -0.061676\nshar2_3    -0.071049\ndate       -0.071510\ngo_out     -0.071895\nshar7_2    -0.073592\nsinc4_1    -0.074499\nmet_o      -0.093879\nName: match, dtype: float64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing The Data"
      ],
      "metadata": {
        "id": "t_Onzhrp126U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainData = train_data.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.818288Z",
          "iopub.execute_input": "2023-03-29T18:55:51.818936Z",
          "iopub.status.idle": "2023-03-29T18:55:51.826240Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.818891Z",
          "shell.execute_reply": "2023-03-29T18:55:51.824856Z"
        },
        "trusted": true,
        "id": "VTBymOec126U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change Data types"
      ],
      "metadata": {
        "id": "duO56sm1126U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainData[['mn_sat', 'tuition','zipcode', 'income']] = train_data[['mn_sat', 'tuition','zipcode', 'income']].replace(\",\", \"\", regex=True)\n",
        "#For Testing data\n",
        "test_data[['mn_sat', 'tuition','zipcode', 'income']] = train_data[['mn_sat', 'tuition','zipcode', 'income']].replace(\",\", \"\", regex=True)\n",
        "trainData[['mn_sat', 'tuition','zipcode', 'income']].head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.827738Z",
          "iopub.execute_input": "2023-03-29T18:55:51.828067Z",
          "iopub.status.idle": "2023-03-29T18:55:51.923747Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.828035Z",
          "shell.execute_reply": "2023-03-29T18:55:51.922282Z"
        },
        "trusted": true,
        "id": "i_5noQf5126U",
        "outputId": "0e372c64-bc4f-49dd-9e9a-c364264520d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 67,
          "output_type": "execute_result",
          "data": {
            "text/plain": "    mn_sat   tuition zipcode    income\n0  1290.00  21645.00     NaN       NaN\n1      NaN       NaN    2021       NaN\n2      NaN       NaN     NaN       NaN\n3      NaN       NaN   10471  45300.00\n4  1400.00  26019.00   66208  46138.00",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mn_sat</th>\n      <th>tuition</th>\n      <th>zipcode</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1290.00</td>\n      <td>21645.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2021</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10471</td>\n      <td>45300.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1400.00</td>\n      <td>26019.00</td>\n      <td>66208</td>\n      <td>46138.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above cells we convert the datatype of four columns (mn_sat, tuition,zipcode,income) into float "
      ],
      "metadata": {
        "id": "-IwENcql126U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainData[['mn_sat', 'tuition','zipcode', 'income']] = trainData[['mn_sat', 'tuition','zipcode', 'income']].astype(float)\n",
        "#For testing data\n",
        "test_data[['mn_sat', 'tuition','zipcode', 'income']] = test_data[['mn_sat', 'tuition','zipcode', 'income']].astype(float)\n",
        "trainData.info()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.925993Z",
          "iopub.execute_input": "2023-03-29T18:55:51.926491Z",
          "iopub.status.idle": "2023-03-29T18:55:51.962280Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.926440Z",
          "shell.execute_reply": "2023-03-29T18:55:51.960940Z"
        },
        "trusted": true,
        "id": "yLyd17nS126U",
        "outputId": "e3d6a7ae-58db-4d51-89a8-b54b09026edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5909 entries, 0 to 5908\nColumns: 192 entries, gender to id\ndtypes: float64(177), int64(11), object(4)\nmemory usage: 8.7+ MB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to display the categorical feature(s)."
      ],
      "metadata": {
        "id": "OcQqF6CS11B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical features can be selected by: (based on the train_data.info() output )\n",
        "features_categorical = list(trainData.select_dtypes(include=['object']))\n",
        "\n",
        "print('categorical features:', features_categorical)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.963801Z",
          "iopub.execute_input": "2023-03-29T18:55:51.964133Z",
          "iopub.status.idle": "2023-03-29T18:55:51.978769Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.964101Z",
          "shell.execute_reply": "2023-03-29T18:55:51.977146Z"
        },
        "trusted": true,
        "id": "mVDIdOHR126V",
        "outputId": "cb90b3d3-3cf8-487e-8fd4-f03827b064e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "categorical features: ['field', 'undergra', 'from', 'career']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The count of filed types:\", trainData['field'].value_counts().count())\n",
        "print(\"The count of school for undegraduates:\", trainData['undergra'].value_counts().count())\n",
        "print(\"The count of places:\", trainData['from'].value_counts().count())\n",
        "print(\"The count of career paths:\", trainData['career'].value_counts().count())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.980837Z",
          "iopub.execute_input": "2023-03-29T18:55:51.981301Z",
          "iopub.status.idle": "2023-03-29T18:55:51.997635Z",
          "shell.execute_reply.started": "2023-03-29T18:55:51.981255Z",
          "shell.execute_reply": "2023-03-29T18:55:51.996080Z"
        },
        "trusted": true,
        "id": "KHK2PkBJ126V",
        "outputId": "7b0adc07-d5dd-4684-a47a-51c42e10fcef"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "The count of filed types: 259\nThe count of school for undegraduates: 241\nThe count of places: 269\nThe count of career paths: 367\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical encoding for training data\n",
        "trainData[features_categorical] = trainData[features_categorical].astype(\"category\")\n",
        "# categorical encoding for testing data \n",
        "test_data[features_categorical] = test_data[features_categorical].astype(\"category\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:51.999733Z",
          "iopub.execute_input": "2023-03-29T18:55:52.000208Z",
          "iopub.status.idle": "2023-03-29T18:55:52.029051Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.000160Z",
          "shell.execute_reply": "2023-03-29T18:55:52.027813Z"
        },
        "trusted": true,
        "id": "1DZPP_QV126V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There 4 categorical columns (field, undergra, from, career) we will need to encode those columns before starting with the model."
      ],
      "metadata": {
        "id": "5riPfa99126V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode Categorical Data"
      ],
      "metadata": {
        "id": "uMort40Z126V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode field column\n",
        "trainData.drop(columns=['field'],inplace = True)\n",
        "label_encoder = preprocessing.LabelEncoder().fit(train_data['field'])\n",
        "labels = label_encoder.transform(train_data['field'])\n",
        "trainData['field'] = labels\n",
        "\n",
        "\n",
        "#Encode undergra column\n",
        "trainData.drop(columns=['undergra'],inplace = True)\n",
        "label_encoder = preprocessing.LabelEncoder().fit(train_data['undergra'])\n",
        "labels = label_encoder.transform(train_data['undergra'])\n",
        "trainData['undergra'] = labels\n",
        "\n",
        "#Encode from column\n",
        "trainData.drop(columns=['from'],inplace = True)\n",
        "label_encoder = preprocessing.LabelEncoder().fit(train_data['from'])\n",
        "labels = label_encoder.transform(train_data[ 'from'])\n",
        "trainData['from'] = labels\n",
        "\n",
        "#Encode career column\n",
        "trainData.drop(columns=['career'],inplace = True)\n",
        "label_encoder = preprocessing.LabelEncoder().fit(train_data['career'])\n",
        "labels = label_encoder.transform(train_data['career'])\n",
        "trainData['career'] = labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.030962Z",
          "iopub.execute_input": "2023-03-29T18:55:52.031428Z",
          "iopub.status.idle": "2023-03-29T18:55:52.038003Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.031377Z",
          "shell.execute_reply": "2023-03-29T18:55:52.036601Z"
        },
        "trusted": true,
        "id": "Q_W9vGnZ126V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We attempted to encode the four categorical features before training the model. However, we observed that the accuracy of the model decreased when tested on the test dataset."
      ],
      "metadata": {
        "id": "u1SBnNMy3GUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop Unwanted nulls"
      ],
      "metadata": {
        "id": "ihvplAyF126V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain features in the dataset have low significance, and therefore, we will remove them before training the model."
      ],
      "metadata": {
        "id": "3AutMjAy3uiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unwantedData = ['undergra' ,'from' ,'career','mn_sat', 'tuition','zipcode']\n",
        "trainData.drop(columns = unwantedData,inplace = True)\n",
        "#For Test Data\n",
        "test_data.drop(columns = unwantedData,inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.039499Z",
          "iopub.execute_input": "2023-03-29T18:55:52.039836Z",
          "iopub.status.idle": "2023-03-29T18:55:52.058547Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.039805Z",
          "shell.execute_reply": "2023-03-29T18:55:52.057466Z"
        },
        "trusted": true,
        "id": "ItCDPTEN126W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solve Nulls  "
      ],
      "metadata": {
        "id": "LxjrUAlt126W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to drope Columns with high percentage of nulls (more than 65%) "
      ],
      "metadata": {
        "id": "ksU6H0hJ126W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_count = trainData.isna().sum().sort_values(ascending = False)\n",
        "total_count = trainData.shape[0]\n",
        "null_percentage = (null_count / total_count)*100\n",
        "Droped_columns = pd.DataFrame(null_percentage[null_percentage >=65]).index\n",
        "print(\"Columns with high percentage of null values \\n\\n\",Droped_columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.059698Z",
          "iopub.execute_input": "2023-03-29T18:55:52.060721Z",
          "iopub.status.idle": "2023-03-29T18:55:52.076358Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.060685Z",
          "shell.execute_reply": "2023-03-29T18:55:52.075390Z"
        },
        "trusted": true,
        "id": "1PrZoMjW126W",
        "outputId": "8b48dad5-d0b1-49f1-9d40-87da69ef252d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Columns with high percentage of null values \n\n Index(['num_in_3', 'numdat_3', 'expnum', 'sinc7_2', 'amb7_2', 'shar7_2',\n       'attr7_2', 'intel7_2', 'fun7_2', 'shar7_3', 'attr7_3', 'sinc7_3',\n       'intel7_3', 'fun7_3', 'amb7_3', 'shar2_3', 'fun5_3', 'amb5_3',\n       'attr5_3', 'sinc5_3', 'intel5_3'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainData.drop(columns = Droped_columns, inplace=True)\n",
        "#For Testing Data\n",
        "test_data.drop(columns = Droped_columns, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.082227Z",
          "iopub.execute_input": "2023-03-29T18:55:52.082815Z",
          "iopub.status.idle": "2023-03-29T18:55:52.092859Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.082777Z",
          "shell.execute_reply": "2023-03-29T18:55:52.091584Z"
        },
        "trusted": true,
        "id": "aDBkggH-126W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainData.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.094138Z",
          "iopub.execute_input": "2023-03-29T18:55:52.094819Z",
          "iopub.status.idle": "2023-03-29T18:55:52.122105Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.094782Z",
          "shell.execute_reply": "2023-03-29T18:55:52.120629Z"
        },
        "trusted": true,
        "id": "ZttPZGMU126W",
        "outputId": "559c0b30-af34-403e-b294-184b3749ffca"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 76,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n0       0    3       2    14     18         2       2.0     14       12   \n1       1   14       1     3     10         2       NaN      8        8   \n2       1   14       1    13     10         8       8.0     10       10   \n3       1   38       2     9     20        18      13.0      6        7   \n4       1   24       2    14     20         6       6.0     20       17   \n\n     pid  ...  sinc2_3  intel2_3  fun2_3  amb2_3  attr3_3  sinc3_3  intel3_3  \\\n0  372.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n1   63.0  ...      NaN       NaN     NaN     NaN      6.0      8.0       8.0   \n2  331.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n3  200.0  ...      8.0       8.0     8.0     8.0      8.0      9.0       8.0   \n4  357.0  ...      NaN       NaN     NaN     NaN      NaN      NaN       NaN   \n\n   fun3_3  amb3_3    id  \n0     NaN     NaN  2583  \n1     7.0     8.0  6830  \n2     NaN     NaN  4840  \n3     8.0     6.0  5508  \n4     NaN     NaN  4828  \n\n[5 rows x 165 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>idg</th>\n      <th>condtn</th>\n      <th>wave</th>\n      <th>round</th>\n      <th>position</th>\n      <th>positin1</th>\n      <th>order</th>\n      <th>partner</th>\n      <th>pid</th>\n      <th>...</th>\n      <th>sinc2_3</th>\n      <th>intel2_3</th>\n      <th>fun2_3</th>\n      <th>amb2_3</th>\n      <th>attr3_3</th>\n      <th>sinc3_3</th>\n      <th>intel3_3</th>\n      <th>fun3_3</th>\n      <th>amb3_3</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>14</td>\n      <td>18</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>14</td>\n      <td>12</td>\n      <td>372.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2583</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>14</td>\n      <td>1</td>\n      <td>3</td>\n      <td>10</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>8</td>\n      <td>63.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>6830</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>14</td>\n      <td>1</td>\n      <td>13</td>\n      <td>10</td>\n      <td>8</td>\n      <td>8.0</td>\n      <td>10</td>\n      <td>10</td>\n      <td>331.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4840</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>38</td>\n      <td>2</td>\n      <td>9</td>\n      <td>20</td>\n      <td>18</td>\n      <td>13.0</td>\n      <td>6</td>\n      <td>7</td>\n      <td>200.0</td>\n      <td>...</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>6.0</td>\n      <td>5508</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>24</td>\n      <td>2</td>\n      <td>14</td>\n      <td>20</td>\n      <td>6</td>\n      <td>6.0</td>\n      <td>20</td>\n      <td>17</td>\n      <td>357.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4828</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 165 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After drop columns with high percentage of null values we need to solve the missing values in the rest column we will try different ways for that"
      ],
      "metadata": {
        "id": "NactHp71126W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Imputer"
      ],
      "metadata": {
        "id": "tFrXb2Cu126X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SimpleImputer is a machine learning technique used for imputing missing values in a dataset. Here are some benefits of using SimpleImputer:**</br></br>\n",
        "\n",
        "**Retains the structure of the data:** SimpleImputer replaces missing values with estimated values based on the mean, median or most frequent value of the respective feature. This means that the imputed values are likely to retain the structure and characteristics of the original data.</br>\n",
        "\n",
        "**No loss of information:** SimpleImputer imputes missing values by using information from other data points in the same feature. This means that there is no loss of information in the imputation process, as all available information is used to estimate the missing values.</br>\n",
        "\n",
        "**Handles both continuous and categorical data:** SimpleImputer can handle both continuous and categorical data, which makes it versatile and useful for a wide range of datasets.</br>\n",
        "\n",
        "**Easy to use:** SimpleImputer is easy to use and implement, as it is available in many machine learning libraries and frameworks such as scikit-learn. It can be used as a drop-in replacement for other imputation techniques such as KNNImputer or IterativeImputer.</br>\n",
        "\n",
        "**Can be tuned for optimal performance:** SimpleImputer allows the user to specify the strategy for imputing missing values, such as mean, median, most frequent or constant value. This parameter can be tuned for optimal performance on a given dataset.</br>\n",
        "\n",
        "**Can improve model performance:** Imputing missing values using SimpleImputer can improve the performance of machine learning models that are trained on the dataset. This is because missing values can negatively impact the accuracy and generalizability of machine learning models.\n",
        "\n",
        "**Overall,** SimpleImputer is a useful technique for imputing missing values in a dataset. Its ability to retain the structure of the data, handle both continuous and categorical data, and improve model performance makes it a popular choice for many machine learning tasks.</br>"
      ],
      "metadata": {
        "id": "JYxzwi6g4HOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First,We will try to fill nulls using mean value**"
      ],
      "metadata": {
        "id": "luTDwYCi126X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(missing_values = np.nan,\n",
        "                        strategy ='mean')\n",
        "# Fitting the data to the imputer object\n",
        "imputer = imputer.fit(trainData)\n",
        " \n",
        "# Imputing the data    \n",
        "trainData = imputer.transform(trainData)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.124044Z",
          "iopub.execute_input": "2023-03-29T18:55:52.124762Z",
          "iopub.status.idle": "2023-03-29T18:55:52.131992Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.124719Z",
          "shell.execute_reply": "2023-03-29T18:55:52.130809Z"
        },
        "trusted": true,
        "id": "WTd_Yy6M126X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainData.isna().sum().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.134672Z",
          "iopub.execute_input": "2023-03-29T18:55:52.135541Z",
          "iopub.status.idle": "2023-03-29T18:55:52.145426Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.135502Z",
          "shell.execute_reply": "2023-03-29T18:55:52.144230Z"
        },
        "trusted": true,
        "id": "gJXU7WP4126X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN Imputer"
      ],
      "metadata": {
        "id": "IKlNx1-J126X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNNImputer is a machine learning technique used for imputing missing values in a dataset. Here are some benefits of using KNNImputer:**</br></br>\n",
        "\n",
        "1. **Retains the structure of the data:** KNNImputer replaces missing  alues with estimated values that are based on the values of other similar data points. This means that the imputed values are likely to retain the structure and characteristics of the original data.</br>\n",
        "\n",
        "2. **No loss of information:** KNNImputer imputes missing values by using information from other data points in the same dataset. This means that there is no loss of information in the imputation process, as all available information is used to estimate the missing values.</br>\n",
        "\n",
        "3. **Handles both continuous and categorical data:** KNNImputer can handle both continuous and categorical data, which makes it versatile and useful for a wide range of datasets.</br>\n",
        "\n",
        "4. **Easy to use:** KNNImputer is easy to use and implement, as it is available in many machine learning libraries and frameworks such as scikit-learn. It can be used as a drop-in replacement for other imputation techniques such as mean imputation and median imputation.</br>\n",
        "\n",
        "5. **Can be tuned for optimal performance:** KNNImputer allows the user to specify the number of nearest neighbors to consider when imputing missing values. This parameter can be tuned for optimal performance on a given dataset.</br>\n",
        "\n",
        "6. **Can improve model performance:** Imputing missing values using KNNImputer can improve the performance of machine learning models that are trained on the dataset. This is because missing values can negatively impact the accuracy and generalizability of machine learning models.</br>\n",
        "\n",
        "**Overall,** KNNImputer is a useful technique for imputing missing values in a dataset. Its ability to retain the structure of the data, handle both continuous and categorical data, and improve model performance makes it a popular choice for many machine learning tasks."
      ],
      "metadata": {
        "id": "cbB7lXgmDlaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# use KNN model to predict missing value (works for both numeric & categorical)\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# replace with constant\n",
        "imputer = KNNImputer(n_neighbors=10) \n",
        "\n",
        "\n",
        "# here select only categorical features\n",
        "train_features = imputer.fit_transform(\n",
        "    df_converted_train)\n",
        "\n",
        "\n",
        "test_features = imputer.transform(\n",
        "    df_converted_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.147066Z",
          "iopub.execute_input": "2023-03-29T18:55:52.147463Z",
          "iopub.status.idle": "2023-03-29T18:55:52.157685Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.147425Z",
          "shell.execute_reply": "2023-03-29T18:55:52.156655Z"
        },
        "trusted": true,
        "id": "cX0qaStB126X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Imbalancing In Dataset"
      ],
      "metadata": {
        "id": "HyH-JO7a126X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling imbalanced datasets is important because it can negatively impact the performance of machine learning models. In an imbalanced dataset, one class (the minority class) may have significantly fewer examples than another class (the majority class). This can cause the machine learning model to be biased towards the majority class and perform poorly on the minority class."
      ],
      "metadata": {
        "id": "eE10ym_g5Ufj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In these step we will try to solve imbalancing in the dataset by using over sampling method, which will add random samples for low  "
      ],
      "metadata": {
        "id": "pwbp--wM126X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SMOTE (Synthetic Minority Over-sampling Technique) is a machine learning technique used for handling imbalanced data. Here are some benefits of using SMOTE:**</br></br>\n",
        "\n",
        "2. **Improves model performance:** Imbalanced data can negatively impact the performance of machine learning models, as the model may be biased towards the majority class. SMOTE can be used to generate synthetic samples of the minority class, which can improve the performance of the model.</br>\n",
        "\n",
        "3. **Retains the structure of the data:** SMOTE generates synthetic samples of the minority class by interpolating between existing samples. This means that the synthetic samples are likely to retain the structure and characteristics of the original data.</br>\n",
        "\n",
        "4. **Easy to use:** SMOTE is easy to use and can be implemented using many machine learning libraries and frameworks such as scikit-learn. It can be used as a drop-in replacement for other techniques such as random oversampling or undersampling.</br>\n",
        "\n",
        "5. **Versatile:** SMOTE can be used for a wide range of classification problems, including binary and multi-class classification.\n",
        "\n",
        "6. **Can be tuned for optimal performance:** SMOTE allows the user to specify the number of synthetic samples to generate and the distance metric used for interpolation. These parameters can be tuned for optimal performance on a given dataset.</br>\n",
        "\n",
        "7. **Reduces the risk of overfitting:** SMOTE generates synthetic samples that are similar to the minority class, which can reduce the risk of overfitting that can occur when using oversampling techniques that simply replicate existing samples.</br>\n"
      ],
      "metadata": {
        "id": "U32MsYN54pDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = trainData.copy()\n",
        "X_train .drop(['match'],axis='columns', inplace=True)\n",
        "y_train = trainData['match']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.159437Z",
          "iopub.execute_input": "2023-03-29T18:55:52.160683Z",
          "iopub.status.idle": "2023-03-29T18:55:52.177424Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.160642Z",
          "shell.execute_reply": "2023-03-29T18:55:52.176314Z"
        },
        "trusted": true,
        "id": "GcImGpIF126Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "# # transform the dataset\n",
        "# oversample = SMOTE()\n",
        "# X_train, y_train = oversample.fit_resample(X, y)\n",
        "# # summarize the new class distribution"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.178803Z",
          "iopub.execute_input": "2023-03-29T18:55:52.179921Z",
          "iopub.status.idle": "2023-03-29T18:55:52.188058Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.179883Z",
          "shell.execute_reply": "2023-03-29T18:55:52.186573Z"
        },
        "trusted": true,
        "id": "McI5EWrE126Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Splite"
      ],
      "metadata": {
        "id": "QR-XoQzh126Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=23 , stratify=y) #, stratify=y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.190051Z",
          "iopub.execute_input": "2023-03-29T18:55:52.190655Z",
          "iopub.status.idle": "2023-03-29T18:55:52.196135Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.190602Z",
          "shell.execute_reply": "2023-03-29T18:55:52.194842Z"
        },
        "trusted": true,
        "id": "K50beHvI126Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots(figsize=(15,7)) \n",
        "# sns.heatmap(data=train_data.isna(),yticklabels=False,cbar=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.197940Z",
          "iopub.execute_input": "2023-03-29T18:55:52.198745Z",
          "iopub.status.idle": "2023-03-29T18:55:52.208387Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.198690Z",
          "shell.execute_reply": "2023-03-29T18:55:52.207026Z"
        },
        "trusted": true,
        "id": "FrpOLyxd126Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Tunable Pipeline\n",
        "\n",
        "In fact, any preprocessing steps can be considered as part of model and different configurations for these steps can be tunable. We can combine preprocessing steps and model as a single tunable pipeline with hyper-parameters."
      ],
      "metadata": {
        "id": "TqCQqfk3126Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why we use pipeline?**  </br></br>\n",
        "The various stages in the machine learning process can be organised and automated conveniently using pipelines. In managing the process, this can save time and effort.</br>\n",
        "\n",
        "By streamlining the process and lowering the possibility of mistakes or inconsistencies, pipelines can enhance the performance of machine learning models.</br>\n",
        "\n",
        "By encapsulating the entire process in a single object, pipelines can facilitate the deployment of machine learning models in real-world settings.\n",
        "\n"
      ],
      "metadata": {
        "id": "kCggbUndCeKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we extract numeric features and categorical features names\n",
        "# for later use\n",
        "\n",
        "# numeric features can be selected by: (based on the df2.info() output )\n",
        "features_numeric = list(X_train.select_dtypes(include=['float64', 'int64']))\n",
        "\n",
        "# categorical features can be selected by: (based on the df2.info() output )\n",
        "features_categorical = list(X_train.select_dtypes(include=['category']))\n",
        "\n",
        "print('numeric features:', features_numeric)\n",
        "print('categorical features:', features_categorical)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.209620Z",
          "iopub.execute_input": "2023-03-29T18:55:52.210689Z",
          "iopub.status.idle": "2023-03-29T18:55:52.228116Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.210636Z",
          "shell.execute_reply": "2023-03-29T18:55:52.226260Z"
        },
        "trusted": true,
        "id": "YO5nxhJ-126Y",
        "outputId": "931301a4-d9e8-43f4-ab28-223b98230532"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "numeric features: ['gender', 'idg', 'condtn', 'wave', 'round', 'position', 'positin1', 'order', 'partner', 'pid', 'int_corr', 'samerace', 'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o', 'age', 'field_cd', 'race', 'imprace', 'imprelig', 'income', 'goal', 'date', 'go_out', 'career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1', 'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1', 'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob', 'met', 'match_es', 'attr1_s', 'sinc1_s', 'intel1_s', 'fun1_s', 'amb1_s', 'shar1_s', 'attr3_s', 'sinc3_s', 'intel3_s', 'fun3_s', 'amb3_s', 'satis_2', 'length', 'numdat_2', 'attr1_2', 'sinc1_2', 'intel1_2', 'fun1_2', 'amb1_2', 'shar1_2', 'attr4_2', 'sinc4_2', 'intel4_2', 'fun4_2', 'amb4_2', 'shar4_2', 'attr2_2', 'sinc2_2', 'intel2_2', 'fun2_2', 'amb2_2', 'shar2_2', 'attr3_2', 'sinc3_2', 'intel3_2', 'fun3_2', 'amb3_2', 'attr5_2', 'sinc5_2', 'intel5_2', 'fun5_2', 'amb5_2', 'you_call', 'them_cal', 'date_3', 'attr1_3', 'sinc1_3', 'intel1_3', 'fun1_3', 'amb1_3', 'shar1_3', 'attr4_3', 'sinc4_3', 'intel4_3', 'fun4_3', 'amb4_3', 'shar4_3', 'attr2_3', 'sinc2_3', 'intel2_3', 'fun2_3', 'amb2_3', 'attr3_3', 'sinc3_3', 'intel3_3', 'fun3_3', 'amb3_3', 'id']\ncategorical features: ['field']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # df = pd.DataFrame(X_train)\n",
        "# features_categorical = list(X_train.select_dtypes(include=['category']))\n",
        "# print(features_categorical)\n",
        "# for i in features_categorical:\n",
        "#     X_train.drop(columns=[i],inplace = True)\n",
        "#     print(X_train.head())\n",
        "#     label_encoder = preprocessing.LabelEncoder().fit(train_data[i])\n",
        "#     labels = label_encoder.transform(X_train[i])\n",
        "#     X_train[i] = labels\n",
        "# #     print(df[i].head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.230223Z",
          "iopub.execute_input": "2023-03-29T18:55:52.230821Z",
          "iopub.status.idle": "2023-03-29T18:55:52.236030Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.230771Z",
          "shell.execute_reply": "2023-03-29T18:55:52.234782Z"
        },
        "trusted": true,
        "id": "dKv9MHkq126Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding_func(df):\n",
        "    df = pd.DataFrame(df)\n",
        "    features_categorical = list(df.select_dtypes(include=['category']))\n",
        "    print(features_categorical)\n",
        "    for i in features_categorical:\n",
        "        df.drop(columns=[i],inplace = True)\n",
        "        label_encoder = preprocessing.LabelEncoder().fit(train_data[i])\n",
        "        labels = label_encoder.transform(df[i])\n",
        "        df[i] = labels\n",
        "    return df\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "transformer = FunctionTransformer(encoding_func)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.237583Z",
          "iopub.execute_input": "2023-03-29T18:55:52.237928Z",
          "iopub.status.idle": "2023-03-29T18:55:52.247296Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.237894Z",
          "shell.execute_reply": "2023-03-29T18:55:52.246086Z"
        },
        "trusted": true,
        "id": "9j18jZjy126Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we attempted to develop a function that can encode categorical features to use it in the pipeline "
      ],
      "metadata": {
        "id": "ghyb-CH5Bxbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df5 = encoding_func(X_train)\n",
        "# df5[['field', 'undergra', 'from', 'career']].head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.248918Z",
          "iopub.execute_input": "2023-03-29T18:55:52.249297Z",
          "iopub.status.idle": "2023-03-29T18:55:52.267336Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.249259Z",
          "shell.execute_reply": "2023-03-29T18:55:52.266094Z"
        },
        "trusted": true,
        "id": "BGVeLfKa126Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# define a pipe line for numeric feature preprocessing\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer()),\n",
        "        ('scaler', StandardScaler())]\n",
        ")\n",
        "\n",
        "# define a pipe line for categorical feature preprocessing\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant'))\n",
        "        ,('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "#         ('encoding', transformer)\n",
        "    ]\n",
        ")\n",
        "# define the preprocessor \n",
        "# we gave them a name so we can set their hyperparameters\n",
        "# we also specify what are the categorical \n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric),\n",
        "        ('cat', transformer_categorical, features_categorical)\n",
        "#         \n",
        "    ]\n",
        ")\n",
        "# PReProcessingStage = Pipeline(\n",
        "#     steps=[ ('Transformers', preprocessor),\n",
        "#         ('smote', SMOTE(random_state=11))])\n",
        "           \n",
        "# combine the preprocessor with the model as a full tunable pipeline\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "#         ('smote', SMOTE()),\n",
        "        ('my_classifier', \n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "full_pipline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.268984Z",
          "iopub.execute_input": "2023-03-29T18:55:52.269772Z",
          "iopub.status.idle": "2023-03-29T18:55:52.313984Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.269721Z",
          "shell.execute_reply": "2023-03-29T18:55:52.312652Z"
        },
        "trusted": true,
        "id": "79cMGW5P126Z",
        "outputId": "70e62209-49cb-42c6-a908-d5c0dd75417c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 88,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['gender', 'idg', 'condtn',\n                                                   'wave', 'round', 'position',\n                                                   'positin1', 'order',\n                                                   'partner', 'pid', 'int_corr',\n                                                   'samerace', 'age_o',\n                                                   'race_o', 'pf_o_att',\n                                                   'pf_o_sin', 'pf_o_int',\n                                                   'pf_o_fun', 'pf_o_amb',\n                                                   'pf_o_sha', 'attr_o',\n                                                   'sinc_o', 'intel_o', 'fun_o',\n                                                   'amb_o', 'shar_o', 'like_o',\n                                                   'prob_o', 'met_o', 'age', ...]),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['field'])])),\n                ('my_classifier', RandomForestClassifier())])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search with Cross-validation\n",
        "\n",
        "We can tune the hyperparameters (including different preprocessing configurations using cross-validation and grid-search).</br></br>\n",
        "\n",
        "\n",
        "**Grid search**</br></br>\n",
        "The grid search is the most common hyperparameter tuning approach given its simple and straightforward procedure. It is an uninformed search method, which means that it does not learn from its previous iterations.</br>\n",
        "\n",
        "Using this method entails testing every unique combination of hyperparameters in the search space to determine the combination that yields the best performance.</br>\n",
        "\n",
        "It’s easy to see the benefits of such a brute-force method; what better way to find the best solution than to try all of them out?</br>\n",
        "\n",
        "Unfortunately, this approach does not scale well; an increase in the size of the hyperparameter search space will result in an exponential rise in run time and computation."
      ],
      "metadata": {
        "id": "6y46U3_K126Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# here we specify the search space\n",
        "# `__` denotes an attribute of the preceeding name\n",
        "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`)\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['most_frequent'],\n",
        "    # preprocessor__num__imputer__strategy points to preprocessor->num (a Pipeline)-> imputer -> strategy\n",
        "    'my_classifier__n_estimators': [50,55,60,70,80,90],  \n",
        "     # my_classifier__n_estimators points to my_classifier->n_estimators \n",
        "    'my_classifier__max_depth':[4,5,6,10, 20, 30]       \n",
        "}\n",
        "\n",
        "# cv=2 means two-fold cross-validation\n",
        "# n_jobs means the cucurrent number of jobs\n",
        "# (on colab since we only have two cpu cores, we set it to 2)\n",
        "RandomForestGS = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=3, verbose=1, n_jobs=2, \n",
        "    scoring='roc_auc')\n",
        "\n",
        "RandomForestGS.fit(X_train, y_train)\n",
        "\n",
        "print('best score {}'.format(RandomForestGS.best_score_))\n",
        "print('best score {}'.format(RandomForestGS.best_params_))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:55:52.316201Z",
          "iopub.execute_input": "2023-03-29T18:55:52.316706Z",
          "iopub.status.idle": "2023-03-29T18:56:38.025950Z",
          "shell.execute_reply.started": "2023-03-29T18:55:52.316657Z",
          "shell.execute_reply": "2023-03-29T18:56:38.024260Z"
        },
        "trusted": true,
        "id": "Z8W_zbtP126Z",
        "outputId": "75a80036-50b0-439e-c894-bacb355f97a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 3 folds for each of 36 candidates, totalling 108 fits\nbest score 0.8550619669409283\nbest score {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 70, 'preprocessor__num__imputer__strategy': 'most_frequent'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # the resulting model is the same pipeline with the best hyperparameters\n",
        "# # trained on the full training set. we can use it directly\n",
        "# grid_search.predict(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:56:38.027513Z",
          "iopub.execute_input": "2023-03-29T18:56:38.027862Z",
          "iopub.status.idle": "2023-03-29T18:56:38.032898Z",
          "shell.execute_reply.started": "2023-03-29T18:56:38.027829Z",
          "shell.execute_reply": "2023-03-29T18:56:38.031595Z"
        },
        "trusted": true,
        "id": "iTvXHkcU126a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search with validation set\n",
        "\n",
        "Cross-validation is expensive. We can supply our own validation set"
      ],
      "metadata": {
        "id": "ln1gHZwB126a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "\n",
        "# Further split the original training set to a train and a validation set\n",
        "X_train2, X_val, y_train2, y_val = train_test_split(\n",
        "    X_train, y_train, train_size = 0.8, stratify = y_train, random_state = 2022)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train2.index else 0 for x in X_train.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "\n",
        "RandomForestGS_with_Val = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
        "    scoring='roc_auc')\n",
        "\n",
        "# here we still use X_train; but the grid search model\n",
        "# will use our predefined split internally to determine \n",
        "# which sample belongs to the validation set\n",
        "RandomForestGS_with_Val.fit(X_train, y_train)\n",
        "\n",
        "print('best score {}'.format(RandomForestGS_with_Val.best_score_))\n",
        "print('best score {}'.format(RandomForestGS_with_Val.best_params_))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:56:38.034254Z",
          "iopub.execute_input": "2023-03-29T18:56:38.034629Z",
          "iopub.status.idle": "2023-03-29T18:56:57.029803Z",
          "shell.execute_reply.started": "2023-03-29T18:56:38.034594Z",
          "shell.execute_reply": "2023-03-29T18:56:57.028303Z"
        },
        "trusted": true,
        "id": "J6ynG_yW126a",
        "outputId": "9563e264-4784-425c-a106-9f9cd1784a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 1 folds for each of 36 candidates, totalling 36 fits\nbest score 0.8465344501929867\nbest score {'my_classifier__max_depth': 6, 'my_classifier__n_estimators': 70, 'preprocessor__num__imputer__strategy': 'most_frequent'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy for these attempet is **86.662%**</br>\n",
        "But we need to try more models to find the best model for our problem"
      ],
      "metadata": {
        "id": "B1mqEDqqe4LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Search \n",
        "\n",
        "Grid search (global optimal) is expensive when you specify a large search space. Alternatively, random search CV give you local optimal (may be good enough and even more generalizable)</br>\n",
        "\n",
        "**Random search**</br></br>\n",
        "The random search is also an uninformed search method that treats iterations independently.</br>\n",
        "\n",
        "However, instead of searching for all hyperparameter sets in the search space, it evaluates a specific number of hyperparameter sets at random. This number is determined by the user.</br>\n",
        "\n",
        "Since it performs fewer trials in hyperparameter tuning, the method requires less computation and run time than the grid search.</br>\n",
        "\n",
        "Unfortunately, since the random search tests hyperparameter sets at random, it runs the risk of missing the ideal set of hyperparameters and forgoing peak model performance."
      ],
      "metadata": {
        "id": "mOPLLEe-126a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "\n",
        "RandomForestRS = RandomizedSearchCV(\n",
        "    full_pipline, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
        "    # number of random trials\n",
        "    n_iter=20,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "RandomForestRS.fit(X_train, y_train)\n",
        "\n",
        "print('best score {}'.format(RandomForestRS.best_score_))\n",
        "print('best score {}'.format(RandomForestRS.best_params_))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:56:57.031617Z",
          "iopub.execute_input": "2023-03-29T18:56:57.031989Z",
          "iopub.status.idle": "2023-03-29T18:57:07.758330Z",
          "shell.execute_reply.started": "2023-03-29T18:56:57.031954Z",
          "shell.execute_reply": "2023-03-29T18:57:07.756753Z"
        },
        "trusted": true,
        "id": "dc75zrnk126a",
        "outputId": "bc7549ed-a0da-43a8-c7ea-c5bc2079230f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 1 folds for each of 20 candidates, totalling 20 fits\nbest score 0.8469861213763653\nbest score {'preprocessor__num__imputer__strategy': 'most_frequent', 'my_classifier__n_estimators': 60, 'my_classifier__max_depth': 10}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Search\n",
        "\n",
        "We can interpret the hyperparameter search problem as a non-differentiable optimization problem, and use bayesian learning to predict what is the next hyperparamter values we should try given the current trials. \n",
        "\n",
        "The flowchart of the algorithm is roughly:\n",
        "\n",
        "![Flowchart](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/19_flowchart_bayesian_optimization.png?raw=1)\n",
        "\n",
        "[Image source: https://github.com/Hvass-Labs/TensorFlow-Tutorials]"
      ],
      "metadata": {
        "id": "nWBZ2f3-126a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:57:07.759952Z",
          "iopub.execute_input": "2023-03-29T18:57:07.760321Z",
          "iopub.status.idle": "2023-03-29T18:57:19.290783Z",
          "shell.execute_reply.started": "2023-03-29T18:57:07.760286Z",
          "shell.execute_reply": "2023-03-29T18:57:19.289187Z"
        },
        "trusted": true,
        "id": "2Qjp4n9a126a",
        "outputId": "0f32c257-bba5-4bac-aeab-0ede2f167161"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: scikit-optimize in /opt/conda/lib/python3.7/site-packages (0.9.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-optimize) (1.21.6)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-optimize) (1.2.0)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-optimize) (1.7.3)\nRequirement already satisfied: pyaml>=16.9 in /opt/conda/lib/python3.7/site-packages (from scikit-optimize) (21.10.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-optimize) (1.0.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try this with SVM model\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "# define ranges for bayes search\n",
        "RandomForestBS = BayesSearchCV(\n",
        "    full_pipline, param_grid,\n",
        "    # number of trials \n",
        "    n_iter=25,\n",
        "    random_state=0,\n",
        "    verbose=1,\n",
        "    # we still use \n",
        "    cv=pds,\n",
        ")\n",
        "\n",
        "RandomForestBS.fit(X_train, y_train)\n",
        "\n",
        "print('best score {}'.format(RandomForestBS.best_score_))\n",
        "print('best score {}'.format(RandomForestBS.best_params_))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:57:19.292964Z",
          "iopub.execute_input": "2023-03-29T18:57:19.293361Z",
          "iopub.status.idle": "2023-03-29T18:58:15.105654Z",
          "shell.execute_reply.started": "2023-03-29T18:57:19.293322Z",
          "shell.execute_reply": "2023-03-29T18:58:15.104371Z"
        },
        "trusted": true,
        "id": "0xvz3L3i126b",
        "outputId": "1a202c23-016d-4e12-8754-dd9b2181c6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nbest score 0.8544839255499154\nbest score OrderedDict([('my_classifier__max_depth', 30), ('my_classifier__n_estimators', 60), ('preprocessor__num__imputer__strategy', 'most_frequent')])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy for these attempet is **85.515%**</br>\n",
        "Which is less than the grid search and these is because the grid search is more powerfull as it try all solutions"
      ],
      "metadata": {
        "id": "KQ8s139vfaKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "xK86xhuP126b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try this with SVM model\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "SVC_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_svc', SVC(class_weight='balanced'))\n",
        "    ]\n",
        ")\n",
        "# SVC has a class_weight attribute for unbalanced data\n",
        "\n",
        "\n",
        "# define ranges for bayes search\n",
        "SVCbayes_search = BayesSearchCV(\n",
        "    SVC_pipline,\n",
        "    {\n",
        "        'my_svc__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
        "        'my_svc__gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
        "        'my_svc__degree': Integer(1,8),\n",
        "        'my_svc__kernel': Categorical(['linear', 'poly', 'rbf']),\n",
        "    },\n",
        "    # number of trials \n",
        "    n_iter=3,\n",
        "    random_state=0,\n",
        "    verbose=1,\n",
        "    # we still use \n",
        "    cv=pds,\n",
        ")\n",
        "\n",
        "SVCbayes_search.fit(X_train, y_train)\n",
        "\n",
        "print('best score {}'.format(SVCbayes_search.best_score_))\n",
        "print('best score {}'.format(SVCbayes_search.best_params_))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T18:58:15.107179Z",
          "iopub.execute_input": "2023-03-29T18:58:15.107526Z",
          "iopub.status.idle": "2023-03-29T18:58:55.521026Z",
          "shell.execute_reply.started": "2023-03-29T18:58:15.107493Z",
          "shell.execute_reply": "2023-03-29T18:58:55.519766Z"
        },
        "trusted": true,
        "id": "Rv3Y86ij126b",
        "outputId": "b0e96a19-09c6-46fd-b5df-43a661dba390"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nFitting 1 folds for each of 1 candidates, totalling 1 fits\nbest score 0.8248730964467005\nbest score OrderedDict([('my_svc__C', 0.0012602593949011189), ('my_svc__degree', 8), ('my_svc__gamma', 2.285959941576884), ('my_svc__kernel', 'poly')])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training accuracy for these attempet is less than other attempets "
      ],
      "metadata": {
        "id": "4YTZbVR_fzcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBost"
      ],
      "metadata": {
        "id": "0yDtHwrh126b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are several benefits of using XGBoost (Extreme Gradient Boosting) as a machine learning model:**</br></br>\n",
        "\n",
        "1. **High performance:** XGBoost is known for its high performance and scalability. It uses a gradient boosting algorithm that combines multiple weak models to make accurate predictions. It is particularly effective in handling large datasets and high-dimensional feature spaces.</br>\n",
        "\n",
        "2. **Flexibility:** XGBoost can be used for both classification and regression problems. It supports a wide range of objective functions, including binary logistic regression, multi-class classification, and ranking problems.</br>\n",
        "\n",
        "3. **Regularization:** XGBoost provides several regularization techniques, such as L1 and L2 regularization, to prevent overfitting and improve generalization performance.</br>\n",
        "\n",
        "4. **Feature importance:** XGBoost provides a feature importance score that tells us how important each feature is in making predictions. This can be useful in feature selection and feature engineering.</br>\n",
        "\n",
        "5. **Interpretable:** XGBoost provides a detailed summary of the model's performance and feature importance, making it easier to interpret and understand the model.</br>\n",
        "\n",
        "6. **Easy to use:** XGBoost is easy to implement and can be integrated with many machine learning libraries and frameworks, such as scikit-learn and TensorFlow.</br>\n"
      ],
      "metadata": {
        "id": "a_tRO4BXy7zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the search space for hyperparameters\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "\n",
        "\n",
        "full_piplineXGB = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier', XGBClassifier(objective='binary:logistic'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "search_spaces = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean'],\n",
        "    'my_classifier__learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
        "    'my_classifier__n_estimators': Integer(50, 200),\n",
        "    'my_classifier__max_depth': Integer(2, 10),\n",
        "    'my_classifier__min_child_weight': Integer(1, 10),\n",
        "    'my_classifier__subsample': Real(0.5, 1.0, prior='uniform'),\n",
        "    'my_classifier__gamma': Real(0, 1, prior='uniform'),\n",
        "    'my_classifier__colsample_bytree': Real(0.5, 1.0, prior='uniform'),\n",
        "    'my_classifier__reg_alpha': Real(1e-5, 1e-1, prior='log-uniform'),\n",
        "    'my_classifier__reg_lambda': Real(1e-5, 1e-1, prior='log-uniform')\n",
        "}\n",
        "\n",
        "\n",
        "# Define the Bayesian search with cross-validation\n",
        "bayes_search_cv = BayesSearchCV(\n",
        "    full_piplineXGB,\n",
        "    search_spaces,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    # n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform the search\n",
        "bayes_search_cv.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and score\n",
        "print(\"Best hyperparameters: \", bayes_search_cv.best_params_)\n",
        "print(\"Best score: \", bayes_search_cv.best_score_)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T19:20:39.926329Z",
          "iopub.execute_input": "2023-03-29T19:20:39.926776Z",
          "iopub.status.idle": "2023-03-29T19:50:15.953134Z",
          "shell.execute_reply.started": "2023-03-29T19:20:39.926736Z",
          "shell.execute_reply": "2023-03-29T19:50:15.951946Z"
        },
        "trusted": true,
        "id": "NsWriowG126b",
        "outputId": "d71f0778-cfa2-42d1-f334-9405620512ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nBest hyperparameters:  OrderedDict([('my_classifier__colsample_bytree', 0.9727994779431233), ('my_classifier__gamma', 0.0162600962380232), ('my_classifier__learning_rate', 0.043123800141422766), ('my_classifier__max_depth', 10), ('my_classifier__min_child_weight', 6), ('my_classifier__n_estimators', 200), ('my_classifier__reg_alpha', 0.09080673275828878), ('my_classifier__reg_lambda', 1.6129443148180106e-05), ('my_classifier__subsample', 0.5322806574027446), ('preprocessor__num__imputer__strategy', 'mean')])\nBest score:  0.8712131306315019\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy for these attempet is **88.44%**</br>\n",
        "Which is the highest accuracy until now "
      ],
      "metadata": {
        "id": "4dp5g5lEeVOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "Ir5SLNIY126b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_piplineLR = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier', LogisticRegression())\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "param_gridLR = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean'],\n",
        "    'my_classifier__C': np.logspace(-4, 4, 50),\n",
        "    'my_classifier__penalty' : ['l1', 'l2'],\n",
        "    'my_classifier__solver': [ 'liblinear'],\n",
        "}\n",
        "\n",
        "\n",
        "LogisticRegressionGS = GridSearchCV(\n",
        "    full_piplineLR, param_gridLR, cv=5, verbose=1, n_jobs=2, \n",
        "    scoring='roc_auc')\n",
        "\n",
        "LogisticRegressionGS.fit(X_train, y_train)\n",
        "\n",
        "print('best score {}'.format(LogisticRegressionGS.best_score_))\n",
        "print('best score {}'.format(LogisticRegressionGS.best_params_))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T20:32:37.873340Z",
          "iopub.execute_input": "2023-03-29T20:32:37.873743Z",
          "iopub.status.idle": "2023-03-29T21:16:21.189454Z",
          "shell.execute_reply.started": "2023-03-29T20:32:37.873706Z",
          "shell.execute_reply": "2023-03-29T21:16:21.187914Z"
        },
        "trusted": true,
        "id": "Eco66WaA126c",
        "outputId": "a25a2382-29b3-4ce8-a14c-65f84357165c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "best score 0.865647905341255\nbest score {'my_classifier__C': 0.0062505519252739694, 'my_classifier__penalty': 'l2', 'my_classifier__solver': 'liblinear', 'preprocessor__num__imputer__strategy': 'mean'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testing accurase for these attempet is **85.761%**</br>\n",
        "Which is less than the accurace of XGBoost the reason for that is:</br></br>\n",
        "XGBoost uses a decision tree-based approach for classification, which can capture complex interactions and non-linear relationships between the features and the target variable. In contrast, Logistic Regression assumes a linear relationship between the features and the target variable, which may not be suitable for complex problems."
      ],
      "metadata": {
        "id": "E6h6sr37dbhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost with KNN imputer\n",
        "\n"
      ],
      "metadata": {
        "id": "_YYNm1Gq126c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For these attempt, we will use:\n",
        "1.  XGBoost with cross validation.\n",
        "2. KNN Imputer to fill nulls \n",
        "3. Bayse search"
      ],
      "metadata": {
        "id": "Gvp8yhu-6x2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# # replace with constant\n",
        "# imputer = KNNImputer(n_neighbors=10) \n",
        "np.random.seed(0)\n",
        "\n",
        "# define a pipe line for numeric feature preprocessing\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', KNNImputer()),\n",
        "        ('scaler', StandardScaler())]\n",
        ")\n",
        "\n",
        "# define a pipe line for categorical feature preprocessing\n",
        "# we gave them a name so we can set their hyperparameters\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer',SimpleImputer(strategy='constant'))\n",
        "        ,('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "#         ('encoding', transformer)\n",
        "    ]\n",
        ")\n",
        "# define the preprocessor \n",
        "# we gave them a name so we can set their hyperparameters\n",
        "# we also specify what are the categorical \n",
        "preprocessor2 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric),\n",
        "        ('cat', transformer_categorical, features_categorical)\n",
        "#         \n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T22:06:21.669764Z",
          "iopub.execute_input": "2023-03-29T22:06:21.670188Z",
          "iopub.status.idle": "2023-03-29T22:06:21.678537Z",
          "shell.execute_reply.started": "2023-03-29T22:06:21.670152Z",
          "shell.execute_reply": "2023-03-29T22:06:21.677148Z"
        },
        "trusted": true,
        "id": "Z45d9tVC126c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the search space for hyperparameters\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "\n",
        "\n",
        "full_piplineXGB = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor2),\n",
        "        ('my_classifier', XGBClassifier(objective='binary:logistic'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "    'preprocessor__num__imputer__n_neighbors': [10,20,25],\n",
        "    'my_classifier__learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
        "    'my_classifier__n_estimators': Integer(50, 200),\n",
        "    'my_classifier__max_depth': Integer(2, 10),\n",
        "    'my_classifier__min_child_weight': Integer(1, 10),\n",
        "    'my_classifier__subsample': Real(0.5, 1.0, prior='uniform'),\n",
        "    'my_classifier__gamma': Real(0, 1, prior='uniform'),\n",
        "    'my_classifier__colsample_bytree': Real(0.5, 1.0, prior='uniform'),\n",
        "    'my_classifier__reg_alpha': Real(1e-5, 1e-1, prior='log-uniform'),\n",
        "    'my_classifier__reg_lambda': Real(1e-5, 1e-1, prior='log-uniform')\n",
        "}\n",
        "\n",
        "\n",
        "# Define the Bayesian search with cross-validation\n",
        "bayes_search_cv = BayesSearchCV(\n",
        "    full_piplineXGB,\n",
        "    search_spaces,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    # n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform the search\n",
        "bayes_search_cv.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and score\n",
        "print(\"Best hyperparameters: \", bayes_search_cv.best_params_)\n",
        "print(\"Best score: \", bayes_search_cv.best_score_)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-29T22:06:22.301589Z",
          "iopub.execute_input": "2023-03-29T22:06:22.302005Z",
          "iopub.status.idle": "2023-03-30T00:06:19.887530Z",
          "shell.execute_reply.started": "2023-03-29T22:06:22.301966Z",
          "shell.execute_reply": "2023-03-30T00:06:19.886462Z"
        },
        "trusted": true,
        "id": "nEXmreJb126c",
        "outputId": "02366906-46f0-4a8f-b7ca-a3d3d4e43d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nBest hyperparameters:  OrderedDict([('my_classifier__colsample_bytree', 0.9190831912766004), ('my_classifier__gamma', 0.9912240538928986), ('my_classifier__learning_rate', 0.12115201170479983), ('my_classifier__max_depth', 6), ('my_classifier__min_child_weight', 4), ('my_classifier__n_estimators', 92), ('my_classifier__reg_alpha', 0.02507466933248837), ('my_classifier__reg_lambda', 0.01174130027396949), ('my_classifier__subsample', 0.5953269597477608), ('preprocessor__num__imputer__n_neighbors', 25)])\nBest score:  0.869013898858262\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we used KNN Imputer to fill nulls it dditn't improve the accurace for XGBoost model as the accurace on testing dataset is **88.672%**</br></br>\n",
        "One Possible reason for that is KNN Imputer may not be able to effectively impute missing values when missing values are clustered or have a specific pattern, which leading to lower accuracy."
      ],
      "metadata": {
        "id": "qMLHQ6BzaFNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using test dataset"
      ],
      "metadata": {
        "id": "vL28lowU126c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data['id']\n",
        "\n",
        "submission['match'] = bayes_search_cv.predict_proba(test_data)[:,1]\n",
        "\n",
        "submission.to_csv('Xgboost_withBSV2.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-30T00:06:19.892149Z",
          "iopub.execute_input": "2023-03-30T00:06:19.893210Z",
          "iopub.status.idle": "2023-03-30T00:06:35.555022Z",
          "shell.execute_reply.started": "2023-03-30T00:06:19.893162Z",
          "shell.execute_reply": "2023-03-30T00:06:35.554007Z"
        },
        "trusted": true,
        "id": "vQjN7tAI126c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Result"
      ],
      "metadata": {
        "id": "nymPJ9xY9UuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this notebook, we experimented with various models and different preprocessing techniques:**</br></br>\n",
        "1. During the preprocessing stage, we utilized SimpleImputer to replace missing values in both categorical and numerical variables. We also employed one-hot encoding to encode categorical variables and utilized XGBoost as the machine learning model. The hyperparameters of the model were optimized using Bayesian search, which resulted in the highest accuracy of 88.672% on the test set.</br>\n",
        "\n",
        "2. In the preprocessing stage, we employed SimpleImputer to replace missing values in both categorical and numerical variables. We also utilized one-hot encoding to encode categorical variables and used Random Forest as the machine learning model. The hyperparameters of the model were optimized using Grid search, which resulted in an accuracy of 86.744% on the test set.</br>\n",
        "\n",
        "3. During the preprocessing stage, we utilized SimpleImputer to replace missing values in both categorical and numerical variables. We also employed one-hot encoding to encode categorical variables and employed Random Forest as the machine learning model. The hyperparameters of the model were optimized using Bayesian search, which resulted in an accuracy of 85.323% on the test set.</br>\n",
        "\n",
        "\n",
        "4. During the preprocessing stage, we employed SimpleImputer to replace missing values in both categorical and numerical variables. We also utilized one-hot encoding to encode categorical variables and used Random Forest as the machine learning model. The hyperparameters of the model were optimized using Grid search, which resulted in an accuracy of 84.974% on the test set.</br>\n",
        "\n",
        "5. In the preprocessing stage, we utilized SimpleImputer to fill null values for categorical features and KNNImputer for numerical features. We also used one-hot encoding to encode categorical features and XGBoost as the machine learning model. The hyperparameters of the model were tuned using Bayesian search, which resulted in the highest accuracy of 87.392% on the test set.\n"
      ],
      "metadata": {
        "id": "3wt5QWOcGAVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **The highst accuracy** reached using:\n",
        " 1. Xgboost model\n",
        " 2. Bayes Search \n",
        " 3. Simple Imputer to fill nulls \n",
        " \n",
        "The finall accuracy on testing dataset is : **88.66**"
      ],
      "metadata": {
        "id": "_xkF-XE-9MQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion"
      ],
      "metadata": {
        "id": "9nJGSDorz7ZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🌈 Why a simple linear regression model (without any activation function) is not good for classification task, compared to Perceptron/Logistic regression?**</br></br>\n",
        "There are two things that explain why Linear Regression is not suitable for classification.</br> **1.** The first one is that Linear Regression deals with continuous values whereas classification problems mandate discrete values.</br>\n",
        "**2.** The second problem is regarding the shift in threshold value when new data points are added.</br>\n",
        "**3.** It lacks non-linearity. Most real-world classification problems are non-linear. A simple linear model will not be able to capture these non-linear relationships correctly. Models like Perceptrons (with a step function) and Logisitc Regression (with a sigmoid) can induce non-linearity andMODEL more complex decision boundaries.</br>\n",
        "**4.** It lacks probabilistic output (for Logistic Regression). Logistic Regression produces probabilistic outputs (values between 0-1) that can be directly interpreted as class probabilities. A simple linear regression model just produces an uncalibrated continuous value.</br></br>\n",
        "\n",
        "**So in summary,** a classification model needs discrete class outputs, a way to threshold into classes, non-linearity, and potentially probabilistic outputs. A simple linear regression lacks all of these key aspects, hence it is not suitable for most classification problems"
      ],
      "metadata": {
        "id": "wWqjhC0Inn1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🌈What's a decision tree and how it is different to a logistic regression model?**</br></br>\n",
        "A decision tree and logistic regression are both popular classification models, but they differ in some key ways:</br>\n",
        "\n",
        "**1.** **Non-linearity**. Decision trees can model complex non-linear relationships, while logistic regression assumes linearity. A decision tree can partition the feature space into irregular regions, using thresholds and splits.</br>\n",
        "\n",
        "**2.** **Codes interaction terms**. A decision tree explicitly codes interaction terms between features by selecting features to split on at each node. Logistic regression requires adding manual interaction terms to model feature interactions.</br>\n",
        "\n",
        "**3.** **Interpretability**. Decision trees tend to be more interpretable, as the rules are easy to understand and visualize. The logistic regression coefficients are harder for humans to interpret meaningfully.</br>\n",
        "\n",
        "**4.** **Probabilistic output**. Logistic regression produces probabilistic class probabilities, while a decision tree provides a hard class prediction.</br>\n",
        "\n",
        "**5.** **Training algorithm**. Decision tree learning uses recursive partitioning and Gini/Entropy impurity minimization. Logistic regression uses iterative maximum likelihood estimation.</br>\n",
        "\n",
        "**6.** **Scales to large data.** Both models can handle large datasets, but decision trees may be slightly more scalable due to smaller memory requirements.</br>\n",
        "\n",
        "**7.** **Robustness to outliers.** Decision trees tend to be more robust to outliers in the training data. Logisitc regression can be sensitive to outliers which influence the maximum likelihood estimates.\n"
      ],
      "metadata": {
        "id": "HXB3MUrBr9Rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🌈What's the difference between grid search and random search?**</br></br>\n",
        "**In Grid Search**, we try every combination of a preset list of values of the hyper-parameters and choose the best combination based on the cross validation score.</br>\n",
        "\n",
        "**Random search** tries random combinations of a range of values (we have to define the number iterations). It is good in testing a wide range of values and normally it reaches a very good combination very fast, but the problem that it doesn’t guarantee to give the best parameters combination.</br>\n",
        "\n",
        "On the other hand, Grid search will give the best combination but it can take a lot of time."
      ],
      "metadata": {
        "id": "N_hNDWPcvlrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🌈What's the difference between bayesian search and random search?**</br></br>\n",
        "\n",
        "\n",
        "\n",
        "Both Bayesian search and random search are hyperparameter optimization techniques used to find the best combination of hyperparameters for a machine learning model. However, they differ in their approach to exploring the hyperparameter space.</br>\n",
        "\n",
        "**Random search** involves sampling hyperparameters randomly from a predefined search space. The idea is to explore a wide range of hyperparameters in the hopes of finding a good combination. Random search is simple to implement and can work well when the search space is small or when the impact of individual hyperparameters on the performance of the model is not well understood.</br>\n",
        "\n",
        "**Bayesian search,** on the other hand, uses probabilistic models to predict the performance of different hyperparameter combinations based on the results of previous evaluations. It builds a probabilistic model of the objective function (e.g., accuracy or F1 score) and uses this model to guide the search. Bayesian search is more efficient than random search as it uses the results of previous evaluations to guide the search towards promising regions of the hyperparameter space. It is particularly useful when the search space is large and the relationship between hyperparameters and model performance is complex.</br>\n",
        "\n",
        "**In summary,** while random search is simple and easy to implement, Bayesian search is more efficient and can be more effective at finding the best combination of hyperparameters, especially when the search space is large and complex."
      ],
      "metadata": {
        "id": "iUQX5P2Nw7ll"
      }
    }
  ]
}